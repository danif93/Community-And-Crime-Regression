{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cross validation purposes: create the cartesian product between the chosen values sets\n",
    "from itertools import product \n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    # for each column that contain at least 1 NaN value...\n",
    "    for nanCol in np.unique(np.where(pred_features.isna())[1]):\n",
    "        nanRows = np.where(pred_features.iloc[:,nanCol].isna())[0] #find NaN rows for the current column\n",
    "        available = df.iloc[~nanRows, nanCol]\n",
    "        value = value_withStrategy(available, strategy) #compute the filling value\n",
    "        df.iloc[nanRows, nanCol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- TBD <br>\n",
    "A thourough study from scratch of outliers detection is needed here, but for now it feels like it exceeds the course final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values rows on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_naSample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = \"robbPerPop\"\n",
    "data,values = drop_naSample(pred_features, regr_values[dep_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTest_split(in_matrix, out_vect, train_amount=0.7):\n",
    "    n,_ = in_matrix.shape\n",
    "\n",
    "    trVl_Amount = int(n*train_amount) #training-validation amount\n",
    "    indexes = np.random.permutation(n)\n",
    "    idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "    idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "    return in_matrix[idxTrVl], in_matrix[idxTs], out_vect[idxTrVl], out_vect[idxTs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal_data, test_data, trainVal_values, test_values = trainTest_split(data, values, train_amount=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_evaluationMetric:\n",
    "    def __init__(self, true, predicted):\n",
    "        self.true = true.flatten()\n",
    "        self.predicted = predicted.flatten()\n",
    "        self.residuals = self.true-self.predicted\n",
    "    \n",
    "    def meanSquareError(self):\n",
    "        return np.mean(np.square(self.residuals))\n",
    "    \n",
    "    def rootMeanSquareError(self):\n",
    "        return np.sqrt(np.mean(np.square(self.residuals)))\n",
    "    \n",
    "    def meanAbsoluteError(self):\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def rSquared(self):\n",
    "        ss_residual = np.sum(np.square(self.residuals))\n",
    "        ss_total = np.sum(np.square(self.true-np.mean(self.true)))        \n",
    "        return 1 - ss_residual/ss_total\n",
    "    \n",
    "    def adjusted_rSquared(self, p):\n",
    "        n = self.true.shape[0]\n",
    "        return 1-(1-self.rSquared)*((n-1)/(n-p-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variable Selection - Models Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def kFold_crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor, verbose=False):\n",
    "    nVal,_ = train_data.shape\n",
    "    \n",
    "    # Validation indexes adjustment -------------------------------\n",
    "    elemPerFold, remainder = np.divmod(nVal,k) #the remainder will be distributed across the firsts folds\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    # in each fold put as many samples as the division quotient +1 if the remainder is still positive\n",
    "    # then decrease the division remainder by 1\n",
    "    for i in range(k): \n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation --------------------------------------------\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values())) # build all the hyp-par combination\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList): # for each fold\n",
    "        if verbose: print(\"#{} fold:\".format(row+1))\n",
    "        for col, params in enumerate(params_product):\n",
    "            \n",
    "            if verbose:\n",
    "                update = col*100/len(params_product) # just print completion rate\n",
    "                print(\"\\t[\"+\"#\"*(int(update/5))+\" \"*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)} # {argument_name:argument_value, ... }\n",
    "            \n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            rem = Regression_evaluationMetric(trainVal_values[valIdx], pred)\n",
    "            #val_results[row,col] = rem.rSquared()\n",
    "            val_results[row,col] = rem.rootMeanSquareError()\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit - Not Working (use the sklearn model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            # project each column on the current residuals\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            # find the most correlated variable: the one that in norm maximise the projections\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "            \n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance # update the solution vector: canonical basis over the found column\n",
    "            residual -= np.matmul(data_matrix, self.weights) # update the residual\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)\n",
    "    \n",
    "    \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.weights)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(zip(range(d),self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]\n",
    "\n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "        return sorted(zip(columns, self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NumStreet', array([-4.63668685e+15])),\n",
       " ('NumKidsBornNeverMar', array([33.83809382])),\n",
       " ('population', array([0.])),\n",
       " ('householdsize', array([0.])),\n",
       " ('racepctblack', array([0.])),\n",
       " ('racePctWhite', array([0.])),\n",
       " ('racePctAsian', array([0.])),\n",
       " ('racePctHisp', array([0.])),\n",
       " ('agePct12t21', array([0.])),\n",
       " ('agePct12t29', array([0.])),\n",
       " ('agePct16t24', array([0.])),\n",
       " ('agePct65up', array([0.])),\n",
       " ('numbUrban', array([0.])),\n",
       " ('pctUrban', array([0.])),\n",
       " ('medIncome', array([0.])),\n",
       " ('pctWWage', array([0.])),\n",
       " ('pctWFarmSelf', array([0.])),\n",
       " ('pctWInvInc', array([0.])),\n",
       " ('pctWSocSec', array([0.])),\n",
       " ('pctWPubAsst', array([0.])),\n",
       " ('pctWRetire', array([0.])),\n",
       " ('medFamInc', array([0.])),\n",
       " ('perCapInc', array([0.])),\n",
       " ('whitePerCap', array([0.])),\n",
       " ('blackPerCap', array([0.])),\n",
       " ('indianPerCap', array([0.])),\n",
       " ('AsianPerCap', array([0.])),\n",
       " ('OtherPerCap', array([0.])),\n",
       " ('HispPerCap', array([0.])),\n",
       " ('NumUnderPov', array([0.])),\n",
       " ('PctPopUnderPov', array([0.])),\n",
       " ('PctLess9thGrade', array([0.])),\n",
       " ('PctNotHSGrad', array([0.])),\n",
       " ('PctBSorMore', array([0.])),\n",
       " ('PctUnemployed', array([0.])),\n",
       " ('PctEmploy', array([0.])),\n",
       " ('PctEmplManu', array([0.])),\n",
       " ('PctEmplProfServ', array([0.])),\n",
       " ('PctOccupManu', array([0.])),\n",
       " ('PctOccupMgmtProf', array([0.])),\n",
       " ('MalePctDivorce', array([0.])),\n",
       " ('MalePctNevMarr', array([0.])),\n",
       " ('FemalePctDiv', array([0.])),\n",
       " ('TotalPctDiv', array([0.])),\n",
       " ('PersPerFam', array([0.])),\n",
       " ('PctFam2Par', array([0.])),\n",
       " ('PctKids2Par', array([0.])),\n",
       " ('PctYoungKids2Par', array([0.])),\n",
       " ('PctTeen2Par', array([0.])),\n",
       " ('PctWorkMomYoungKids', array([0.])),\n",
       " ('PctWorkMom', array([0.])),\n",
       " ('PctKidsBornNeverMar', array([0.])),\n",
       " ('NumImmig', array([0.])),\n",
       " ('PctImmigRecent', array([0.])),\n",
       " ('PctImmigRec5', array([0.])),\n",
       " ('PctImmigRec8', array([0.])),\n",
       " ('PctImmigRec10', array([0.])),\n",
       " ('PctRecentImmig', array([0.])),\n",
       " ('PctRecImmig5', array([0.])),\n",
       " ('PctRecImmig8', array([0.])),\n",
       " ('PctRecImmig10', array([0.])),\n",
       " ('PctSpeakEnglOnly', array([0.])),\n",
       " ('PctNotSpeakEnglWell', array([0.])),\n",
       " ('PctLargHouseFam', array([0.])),\n",
       " ('PctLargHouseOccup', array([0.])),\n",
       " ('PersPerOccupHous', array([0.])),\n",
       " ('PersPerOwnOccHous', array([0.])),\n",
       " ('PersPerRentOccHous', array([0.])),\n",
       " ('PctPersOwnOccup', array([0.])),\n",
       " ('PctPersDenseHous', array([0.])),\n",
       " ('PctHousLess3BR', array([0.])),\n",
       " ('MedNumBR', array([0.])),\n",
       " ('HousVacant', array([0.])),\n",
       " ('PctHousOccup', array([0.])),\n",
       " ('PctHousOwnOcc', array([0.])),\n",
       " ('PctVacantBoarded', array([0.])),\n",
       " ('PctVacMore6Mos', array([0.])),\n",
       " ('MedYrHousBuilt', array([0.])),\n",
       " ('PctHousNoPhone', array([0.])),\n",
       " ('PctWOFullPlumb', array([0.])),\n",
       " ('OwnOccLowQuart', array([0.])),\n",
       " ('OwnOccMedVal', array([0.])),\n",
       " ('OwnOccHiQuart', array([0.])),\n",
       " ('OwnOccQrange', array([0.])),\n",
       " ('RentLowQ', array([0.])),\n",
       " ('RentMedian', array([0.])),\n",
       " ('RentHighQ', array([0.])),\n",
       " ('RentQrange', array([0.])),\n",
       " ('MedRent', array([0.])),\n",
       " ('MedRentPctHousInc', array([0.])),\n",
       " ('MedOwnCostPctInc', array([0.])),\n",
       " ('MedOwnCostPctIncNoMtg', array([0.])),\n",
       " ('NumInShelters', array([0.])),\n",
       " ('PctForeignBorn', array([0.])),\n",
       " ('PctBornSameState', array([0.])),\n",
       " ('PctSameHouse85', array([0.])),\n",
       " ('PctSameCity85', array([0.])),\n",
       " ('PctSameState85', array([0.])),\n",
       " ('LandArea', array([0.])),\n",
       " ('PopDens', array([0.])),\n",
       " ('PctUsePubTrans', array([0.])),\n",
       " ('LemasPctOfficDrugUn', array([0.]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "mp.sort_featureImportances(columns=pred_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 1.3270881403795465e+29\n",
      "Root Mean Square Error: 4627754785715472.0\n",
      "R^2 score: -5.041673456511588e+32\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 34, 38, 50, 76, 77, 92, 93, 94])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07716713048922287\n",
      "Root Mean Square Error: 0.11125529597117739\n",
      "R^2 score: 0.7086094768869478\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso) with Proximal Gradient - Not Working (use the sklearn model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression: # Iterative Soft Thresholding Algorithm\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        n,d = data_matrix.shape\n",
    "        self.weights = np.zeros((d,1))\n",
    "        \n",
    "        # convergence step-size: n/(2*||X^t*X||_2)\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # gradient step of the mse formulation\n",
    "            resid = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            grad_descent = (step/n)*np.matmul(data_matrix.T, resid)\n",
    "            self.weights -= 2*grad_descent\n",
    "\n",
    "            # proximal operator\n",
    "            upper = self.weights > softTresh  # elem to be reduced\n",
    "            lower = self.weights < -softTresh # elem to be increased\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)\n",
    "    \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.weights)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(zip(range(d),self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]\n",
    "\n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "        return sorted(zip(columns, self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NumKidsBornNeverMar', array([0.10104377])),\n",
       " ('NumInShelters', array([0.0860739])),\n",
       " ('NumStreet', array([0.08195012])),\n",
       " ('NumUnderPov', array([0.08004374])),\n",
       " ('NumImmig', array([0.07533265])),\n",
       " ('population', array([0.06335219])),\n",
       " ('numbUrban', array([0.0583407])),\n",
       " ('HousVacant', array([0.05659798])),\n",
       " ('LemasPctOfficDrugUn', array([0.00699548])),\n",
       " ('LandArea', array([0.00181962])),\n",
       " ('householdsize', array([0.])),\n",
       " ('racepctblack', array([0.])),\n",
       " ('racePctWhite', array([0.])),\n",
       " ('racePctAsian', array([0.])),\n",
       " ('racePctHisp', array([0.])),\n",
       " ('agePct12t21', array([0.])),\n",
       " ('agePct12t29', array([0.])),\n",
       " ('agePct16t24', array([0.])),\n",
       " ('agePct65up', array([0.])),\n",
       " ('pctUrban', array([0.])),\n",
       " ('medIncome', array([0.])),\n",
       " ('pctWWage', array([0.])),\n",
       " ('pctWFarmSelf', array([0.])),\n",
       " ('pctWInvInc', array([0.])),\n",
       " ('pctWSocSec', array([0.])),\n",
       " ('pctWPubAsst', array([0.])),\n",
       " ('pctWRetire', array([0.])),\n",
       " ('medFamInc', array([0.])),\n",
       " ('perCapInc', array([0.])),\n",
       " ('whitePerCap', array([0.])),\n",
       " ('blackPerCap', array([0.])),\n",
       " ('indianPerCap', array([0.])),\n",
       " ('AsianPerCap', array([0.])),\n",
       " ('OtherPerCap', array([0.])),\n",
       " ('HispPerCap', array([0.])),\n",
       " ('PctPopUnderPov', array([0.])),\n",
       " ('PctLess9thGrade', array([0.])),\n",
       " ('PctNotHSGrad', array([0.])),\n",
       " ('PctBSorMore', array([0.])),\n",
       " ('PctUnemployed', array([0.])),\n",
       " ('PctEmploy', array([0.])),\n",
       " ('PctEmplManu', array([0.])),\n",
       " ('PctEmplProfServ', array([0.])),\n",
       " ('PctOccupManu', array([0.])),\n",
       " ('PctOccupMgmtProf', array([0.])),\n",
       " ('MalePctDivorce', array([0.])),\n",
       " ('MalePctNevMarr', array([0.])),\n",
       " ('FemalePctDiv', array([0.])),\n",
       " ('TotalPctDiv', array([0.])),\n",
       " ('PersPerFam', array([0.])),\n",
       " ('PctFam2Par', array([0.])),\n",
       " ('PctKids2Par', array([0.])),\n",
       " ('PctYoungKids2Par', array([0.])),\n",
       " ('PctTeen2Par', array([0.])),\n",
       " ('PctWorkMomYoungKids', array([0.])),\n",
       " ('PctWorkMom', array([0.])),\n",
       " ('PctKidsBornNeverMar', array([0.])),\n",
       " ('PctImmigRecent', array([0.])),\n",
       " ('PctImmigRec5', array([0.])),\n",
       " ('PctImmigRec8', array([0.])),\n",
       " ('PctImmigRec10', array([0.])),\n",
       " ('PctRecentImmig', array([0.])),\n",
       " ('PctRecImmig5', array([0.])),\n",
       " ('PctRecImmig8', array([0.])),\n",
       " ('PctRecImmig10', array([0.])),\n",
       " ('PctSpeakEnglOnly', array([0.])),\n",
       " ('PctNotSpeakEnglWell', array([0.])),\n",
       " ('PctLargHouseFam', array([0.])),\n",
       " ('PctLargHouseOccup', array([0.])),\n",
       " ('PersPerOccupHous', array([0.])),\n",
       " ('PersPerOwnOccHous', array([0.])),\n",
       " ('PersPerRentOccHous', array([0.])),\n",
       " ('PctPersOwnOccup', array([0.])),\n",
       " ('PctPersDenseHous', array([0.])),\n",
       " ('PctHousLess3BR', array([0.])),\n",
       " ('MedNumBR', array([0.])),\n",
       " ('PctHousOccup', array([0.])),\n",
       " ('PctHousOwnOcc', array([0.])),\n",
       " ('PctVacantBoarded', array([0.])),\n",
       " ('PctVacMore6Mos', array([0.])),\n",
       " ('MedYrHousBuilt', array([0.])),\n",
       " ('PctHousNoPhone', array([0.])),\n",
       " ('PctWOFullPlumb', array([0.])),\n",
       " ('OwnOccLowQuart', array([0.])),\n",
       " ('OwnOccMedVal', array([0.])),\n",
       " ('OwnOccHiQuart', array([0.])),\n",
       " ('OwnOccQrange', array([0.])),\n",
       " ('RentLowQ', array([0.])),\n",
       " ('RentMedian', array([0.])),\n",
       " ('RentHighQ', array([0.])),\n",
       " ('RentQrange', array([0.])),\n",
       " ('MedRent', array([0.])),\n",
       " ('MedRentPctHousInc', array([0.])),\n",
       " ('MedOwnCostPctInc', array([0.])),\n",
       " ('MedOwnCostPctIncNoMtg', array([0.])),\n",
       " ('PctForeignBorn', array([0.])),\n",
       " ('PctBornSameState', array([0.])),\n",
       " ('PctSameHouse85', array([0.])),\n",
       " ('PctSameCity85', array([0.])),\n",
       " ('PctSameState85', array([0.])),\n",
       " ('PopDens', array([0.])),\n",
       " ('PctUsePubTrans', array([0.]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=1000)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.5)\n",
    "lr.sort_featureImportances(columns=pred_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.038974246789447275\n",
      "Root Mean Square Error: 0.3192202001848008\n",
      "R^2 score: -1.3989144889238103\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  69,  76,  93,  94, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06700266663502594\n",
      "Root Mean Square Error: 0.11398320430750422\n",
      "R^2 score: 0.6941448766218346\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Decision Tree class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor: # Least Square Regression Tree with either fixed parameter or pruning\n",
    "    class Node:\n",
    "        def __init__(self, isLeaf=False, feature=None, feature_importance=None, cut=None, average=None,\n",
    "                     left=None, right=None):\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature # if internal, on wich feature it executes the split\n",
    "            self.feature_importance = feature_importance # solution variance reduction\n",
    "            self.cut = cut # if internal, threahold value for the cut\n",
    "            self.avg = average # mean of seen training values\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            if self.cut:\n",
    "                print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"Leaf => {}\\n\".format(self.avg))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            if self.cut:\n",
    "                print(\"|    \"*level+\"{} => {}\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"|    \"*level+\"Leaf: {}\".format(self.avg))                \n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self, root=None, feature_importances=None):\n",
    "        self.root = root\n",
    "        self.feature_importances = feature_importances\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, depth, minElems_perLeaf, post_pruning=False):\n",
    "        \n",
    "        self.feature_importances = {k:0 for k in range(X.shape[1])}\n",
    "        \n",
    "        if not post_pruning:\n",
    "            self.root = self.learn(X, y.flatten(), depth, minElems_perLeaf)\n",
    "        else:\n",
    "            # train dataset, pruning dataset\n",
    "            X_trn, X_val, y_trn, y_val = trainTest_split(X, y.flatten(), train_amount=0.7)\n",
    "            self.root = self.learn(X_trn, y_trn, depth, minElems_perLeaf)\n",
    "            self.prune(X_val, y_val)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElems_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElems_perLeaf: # leaf # or fraction error of the root node??? \n",
    "            return self.Node(isLeaf=True, average=np.mean(y))\n",
    "            \n",
    "        best_costDescent = 0 # split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "                \n",
    "                if sorted_x[i2]<sorted_x[i2+1]: # for a different value\n",
    "                    # try to maximise this value: it is directly correlated \n",
    "                    # to the possible split information gain\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "                        \n",
    "        # update the importance for the selected feature\n",
    "        feature_importance = np.var(y) - (np.sum(np.square(y))-best_costDescent)/n\n",
    "        self.feature_importances[best_feature] += feature_importance\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "        \n",
    "        return self.Node(feature=best_feature, feature_importance=feature_importance,\n",
    "                         cut=best_cut, average=np.mean(y),\n",
    "                         left = self.learn(X[left_idxs], y[left_idxs], depth-1, minElems_perLeaf),\n",
    "                         right = self.learn(X[~left_idxs], y[~left_idxs], depth-1, minElems_perLeaf))\n",
    "    \n",
    "    def prune(self, X, y):\n",
    "        # for statistics purposes check errors on different dataset portions and average them\n",
    "        # in order to decide whether to prune or not (same code of k-fold cross-validation)\n",
    "        n,_ = X.shape\n",
    "        folds = 5\n",
    "        elemPerFold, remainder = np.divmod(n, folds)\n",
    "        foldsIdxsList = []\n",
    "        start = 0\n",
    "        for i in range(folds): \n",
    "            end = start+elemPerFold+int(remainder>0)\n",
    "            foldsIdxsList.append(np.arange(start,end)) \n",
    "            remainder -= 1\n",
    "            start = end\n",
    "        \n",
    "        # recursive: start checking if the root receive a possible positive pruning from its sons\n",
    "        self.test_pruning(self.root, X, y, foldsIdxsList)\n",
    "        return self\n",
    "    \n",
    "    def test_pruning(self, node, X, y, foldIdxs):\n",
    "        if node.isLeaf: # leaf: start point of new possible pruning\n",
    "            return True\n",
    "        \n",
    "        # check sons response: if one of them is negative to be pruned it means that it performs an important\n",
    "        # predictive split\n",
    "        if not self.test_pruning(node.left, X, y, foldIdxs) or not self.test_pruning(node.right, X, y, foldIdxs):\n",
    "            return False\n",
    "        \n",
    "        # else proceed with testing the goodness of the current node split\n",
    "        folds = len(foldIdxs)\n",
    "        results = np.empty(folds)\n",
    "\n",
    "        # not pruned errors on different folds\n",
    "        for i, idxs in enumerate(foldIdxs):\n",
    "            pred = self.predict(X[idxs])\n",
    "            results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "\n",
    "        not_prunErr = np.mean(results)\n",
    "\n",
    "        # pruned errors on different folds\n",
    "        node.isLeaf = True\n",
    "        for i, idxs in enumerate(foldIdxs):\n",
    "            pred = self.predict(X[idxs])\n",
    "            results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "\n",
    "        # if pruning improves the prediction RMSE then keep current node as leaf\n",
    "        node.isLeaf = np.mean(results) <= not_prunErr\n",
    "        \n",
    "        if node.isLeaf:\n",
    "            # lower feature importance computed during training phase\n",
    "            self.feature_importances[node.feature] -= node.feature_importance \n",
    "            node.left = None\n",
    "            node.right = None\n",
    "            \n",
    "        return node.isLeaf\n",
    "            \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n,_ = X.shape\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.cut:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.avg\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()\n",
    "        \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.feature_importances is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.feature_importances)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(self.feature_importances.items(), key=lambda kv: kv[1], reverse=True)[:n_printFeat]\n",
    "            \n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "            \n",
    "        return sorted(zip(map(lambda kv: round(kv[1], 4), self.feature_importances.items()), columns),\n",
    "                      reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    Leaf: 0.25373837485774514\n",
      "|    |    |    51 => -0.8254687790308851\n",
      "|    |    |    |    |    Leaf: -0.18807665637573817\n",
      "|    |    |    |    2 => -0.5304644667425261\n",
      "|    |    |    |    |    Leaf: -0.460075172362011\n",
      "|    |    51 => -0.9862140053031551\n",
      "|    |    |    |    |    Leaf: -0.7051040676671714\n",
      "|    |    |    |    3 => 0.36348633316142354\n",
      "|    |    |    |    |    Leaf: -0.48760381296684896\n",
      "|    |    |    100 => -0.9688937971654703\n",
      "|    |    |    |    |    Leaf: -0.6919543248199289\n",
      "|    |    |    |    91 => -0.9989308471966813\n",
      "|    |    |    |    |    Leaf: -0.8054220578776127\n",
      "|    43 => -0.3406484496672801\n",
      "|    |    |    Leaf: 0.5354368218382209\n",
      "|    |    100 => -0.3436407141542426\n",
      "|    |    |    Leaf: -0.15723147522447922\n",
      "50 => -0.5616087751371115\n",
      "|    |    |    |    |    Leaf: -0.6196401413935301\n",
      "|    |    |    |    2 => -0.8772111306506672\n",
      "|    |    |    |    |    Leaf: -0.7470893541249128\n",
      "|    |    |    36 => -0.6580419580419581\n",
      "|    |    |    |    Leaf: -0.15095643801371839\n",
      "|    |    51 => -0.9895410797676905\n",
      "|    |    |    |    |    Leaf: -0.56596426294721\n",
      "|    |    |    |    93 => -0.33759548322816335\n",
      "|    |    |    |    |    Leaf: -0.8269937873930681\n",
      "|    |    |    11 => 0.9515\n",
      "|    |    |    |    |    Leaf: -0.8730637125914785\n",
      "|    |    |    |    49 => -0.998068455162191\n",
      "|    |    |    |    |    Leaf: -0.9426387471861893\n",
      "|    50 => -0.8007312614259597\n",
      "|    |    |    |    |    Leaf: -0.5025815655461481\n",
      "|    |    |    |    18 => -0.00047562425683711496\n",
      "|    |    |    |    |    Leaf: -0.8883499757118516\n",
      "|    |    |    36 => -0.9508158508158508\n",
      "|    |    |    |    Leaf: -0.3834143799163476\n",
      "|    |    51 => -0.9957746634397725\n",
      "|    |    |    |    |    Leaf: -0.7998548910819849\n",
      "|    |    |    |    99 => -0.8031338831612012\n",
      "|    |    |    |    |    Leaf: -0.9272724350394478\n",
      "|    |    |    2 => -0.8993482983345402\n",
      "|    |    |    |    |    Leaf: -0.9340327231360225\n",
      "|    |    |    |    40 => -0.061740398638794325\n",
      "|    |    |    |    |    Leaf: -0.9715095185143283\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElems_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.08623683554319454\n",
      "Root Mean Square Error: 0.13630140453127904\n",
      "R^2 score: 0.5626443931984483\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 0.1273753198453829),\n",
       " (51, 0.05250052214366452),\n",
       " (43, 0.03702410084615772),\n",
       " (2, 0.0227134476249547),\n",
       " (36, 0.022471731578918124),\n",
       " (50, 0.01975491645884912),\n",
       " (3, 0.011630659040595813),\n",
       " (93, 0.0029850474401389698),\n",
       " (91, 0.002925972197938116),\n",
       " (18, 0.002438949641839816),\n",
       " (11, 0.0018461869496656062),\n",
       " (49, 0.0011808882008847903),\n",
       " (99, 0.001145454390114091),\n",
       " (40, 0.0002518754355045176),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (4, 0),\n",
       " (5, 0),\n",
       " (6, 0),\n",
       " (7, 0),\n",
       " (8, 0),\n",
       " (9, 0),\n",
       " (10, 0),\n",
       " (12, 0),\n",
       " (13, 0),\n",
       " (14, 0),\n",
       " (15, 0),\n",
       " (16, 0),\n",
       " (17, 0),\n",
       " (19, 0),\n",
       " (20, 0),\n",
       " (21, 0),\n",
       " (22, 0),\n",
       " (23, 0),\n",
       " (24, 0),\n",
       " (25, 0),\n",
       " (26, 0),\n",
       " (27, 0),\n",
       " (28, 0),\n",
       " (29, 0),\n",
       " (30, 0),\n",
       " (31, 0),\n",
       " (32, 0),\n",
       " (33, 0),\n",
       " (34, 0),\n",
       " (35, 0),\n",
       " (37, 0),\n",
       " (38, 0),\n",
       " (39, 0),\n",
       " (41, 0),\n",
       " (42, 0),\n",
       " (44, 0),\n",
       " (45, 0),\n",
       " (46, 0),\n",
       " (47, 0),\n",
       " (48, 0),\n",
       " (52, 0),\n",
       " (53, 0),\n",
       " (54, 0),\n",
       " (55, 0),\n",
       " (56, 0),\n",
       " (57, 0),\n",
       " (58, 0),\n",
       " (59, 0),\n",
       " (60, 0),\n",
       " (61, 0),\n",
       " (62, 0),\n",
       " (63, 0),\n",
       " (64, 0),\n",
       " (65, 0),\n",
       " (66, 0),\n",
       " (67, 0),\n",
       " (68, 0),\n",
       " (69, 0),\n",
       " (70, 0),\n",
       " (71, 0),\n",
       " (72, 0),\n",
       " (73, 0),\n",
       " (74, 0),\n",
       " (75, 0),\n",
       " (76, 0),\n",
       " (77, 0),\n",
       " (78, 0),\n",
       " (79, 0),\n",
       " (80, 0),\n",
       " (81, 0),\n",
       " (82, 0),\n",
       " (83, 0),\n",
       " (84, 0),\n",
       " (85, 0),\n",
       " (86, 0),\n",
       " (87, 0),\n",
       " (88, 0),\n",
       " (89, 0),\n",
       " (90, 0),\n",
       " (92, 0),\n",
       " (94, 0),\n",
       " (95, 0),\n",
       " (96, 0),\n",
       " (97, 0),\n",
       " (98, 0),\n",
       " (101, 0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndt.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    Leaf: 0.3911171178333399\n",
      "|    |    |    36 => -0.25920745920745936\n",
      "|    |    |    |    |    Leaf: -0.24302756467163994\n",
      "|    |    |    |    32 => -0.6820461384152459\n",
      "|    |    |    |    |    Leaf: -0.5804613692676657\n",
      "|    |    49 => -0.9290143055631903\n",
      "|    |    |    |    |    |    Leaf: -0.4705162689421543\n",
      "|    |    |    |    |    72 => 0.8078985860555827\n",
      "|    |    |    |    |    |    Leaf: -0.6811784953455264\n",
      "|    |    |    |    49 => -0.9773086130977316\n",
      "|    |    |    |    |    |    |    Leaf: -0.7606523098433898\n",
      "|    |    |    |    |    |    100 => -0.9215902816123689\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8620856576256664\n",
      "|    |    |    |    |    |    |    6 => -0.6144118827780007\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7932715877621869\n",
      "|    |    |    |    |    3 => 0.5069623517276947\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7167322547733567\n",
      "|    |    |    |    |    |    |    15 => -0.28607473266850897\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.575406762988748\n",
      "|    |    |    |    |    |    40 => 0.008264462809917383\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8484627649472425\n",
      "|    |    |    |    |    |    |    43 => 0.4561800934447121\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6945592788399959\n",
      "|    |    |    |    |    |    |    |    18 => -0.6223543400713436\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7670922900481273\n",
      "|    |    |    43 => 0.10179810278918322\n",
      "|    |    |    |    |    |    |    Leaf: -0.7026517911957353\n",
      "|    |    |    |    |    |    3 => 0.4905621454357917\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.2994836868907705\n",
      "|    |    |    |    |    |    |    0 => -0.9490048558924448\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.5998869322874569\n",
      "|    |    |    |    |    |    |    |    36 => -0.28135198135198153\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.4925406226674264\n",
      "|    |    |    |    |    80 => -0.8757546810516523\n",
      "|    |    |    |    |    |    |    Leaf: -0.7766963153932984\n",
      "|    |    |    |    |    |    37 => -0.4373397161908018\n",
      "|    |    |    |    |    |    |    Leaf: -0.5950210456113386\n",
      "|    |    |    |    98 => -0.9943399927148422\n",
      "|    |    |    |    |    |    Leaf: -0.1659489516944699\n",
      "|    |    |    |    |    38 => 0.1805013927576603\n",
      "|    |    |    |    |    |    Leaf: -0.45747593998577823\n",
      "|    3 => -0.24507478081485284\n",
      "|    |    |    Leaf: 0.48971127982933826\n",
      "|    |    100 => -0.25786858089453335\n",
      "|    |    |    |    Leaf: -0.5567215663411553\n",
      "|    |    |    13 => 0.6127842655193604\n",
      "|    |    |    |    |    Leaf: -0.09380645104300551\n",
      "|    |    |    |    76 => -0.3958333333333333\n",
      "|    |    |    |    |    Leaf: 0.08485996828803996\n",
      "49 => -0.9920596257845125\n",
      "|    |    |    Leaf: -0.019698515544602224\n",
      "|    |    81 => 0.7880419922806438\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8694165087693728\n",
      "|    |    |    |    |    |    |    3 => 0.015368746776689146\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6204988229474456\n",
      "|    |    |    |    |    |    44 => 0.29213785675821224\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.742879478778036\n",
      "|    |    |    |    |    |    |    50 => -0.5989031078610604\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6407361768096355\n",
      "|    |    |    |    |    46 => 0.20650764843142322\n",
      "|    |    |    |    |    |    Leaf: -0.4738265912293022\n",
      "|    |    |    |    59 => -0.5298804780876494\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8983774488802911\n",
      "|    |    |    |    |    |    |    |    |    |    18 => -0.12366230677764561\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.6801420413138821\n",
      "|    |    |    |    |    |    |    |    |    |    |    46 => 0.48353642727508406\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8121918794415516\n",
      "|    |    |    |    |    |    |    |    |    73 => -0.04546025367323864\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7240253430677568\n",
      "|    |    |    |    |    |    |    |    26 => -0.667307129263651\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7886031279122665\n",
      "|    |    |    |    |    |    |    |    |    93 => -0.7739953503819329\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8871343518260877\n",
      "|    |    |    |    |    |    |    |    |    |    |    74 => -0.9232890448734019\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9304898570311776\n",
      "|    |    |    |    |    |    |    |    |    |    13 => 0.29993853718500296\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8261525030217641\n",
      "|    |    |    |    |    |    |    4 => -0.968657496082187\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6242785430950422\n",
      "|    |    |    |    |    |    |    |    54 => -0.3583714886771439\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7935365902134595\n",
      "|    |    |    |    |    |    4 => -0.9784084973010622\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8575356636863706\n",
      "|    |    |    |    |    |    |    |    34 => -0.27990829512296805\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.75229779208791\n",
      "|    |    |    |    |    |    |    49 => -0.9967738083278205\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.845726614637852\n",
      "|    |    |    |    |    |    |    |    9 => -0.4507924085306203\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8849256300065221\n",
      "|    |    |    |    |    |    |    |    |    52 => -0.538497433504433\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9101959693127162\n",
      "|    |    |    |    |    |    |    |    |    |    38 => -0.16657381615598876\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9614082819154967\n",
      "|    |    |    |    |    2 => -0.7256646322540603\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9236854774239995\n",
      "|    |    |    |    |    |    |    91 => -0.999486806654407\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.980305410417639\n",
      "|    |    |    |    |    |    6 => -0.5539943797671618\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.82781465728558\n",
      "|    |    |    |    |    |    |    |    |    84 => -0.5334846765039728\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8617982771896197\n",
      "|    |    |    |    |    |    |    |    |    |    100 => -0.9274802135100313\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8922176141240418\n",
      "|    |    |    |    |    |    |    |    |    |    |    56 => -0.986870897155361\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9388683511989152\n",
      "|    |    |    |    |    |    |    |    77 => -0.6365159128978224\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9426949865952927\n",
      "|    |    |    |    |    |    |    75 => -0.22452854069105171\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7489896781545229\n",
      "|    |    |    |    |    |    |    |    98 => -0.9846171089131105\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7740743685212422\n",
      "|    |    |    |    |    |    |    |    |    23 => -0.9179229166666667\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8919290558993227\n",
      "|    |    |    |    |    |    |    |    |    |    |    57 => -0.6101354741595585\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.7999739414256248\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    92 => -0.9969369196898632\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8452739021169279\n",
      "|    |    |    |    |    |    |    |    |    |    5 => -0.9770936219396869\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9053190408677946\n",
      "|    |    |    46 => 0.018019185895773893\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6246338426778596\n",
      "|    |    |    |    |    |    |    |    60 => 0.8719164063341592\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7145776376209257\n",
      "|    |    |    |    |    |    |    28 => -0.5923988842398884\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.512522101940554\n",
      "|    |    |    |    |    |    12 => -0.850887512090555\n",
      "|    |    |    |    |    |    |    Leaf: -0.37591039383780966\n",
      "|    |    |    |    |    1 => -0.6983695652173914\n",
      "|    |    |    |    |    |    Leaf: -0.20083210769699622\n",
      "|    |    |    |    99 => -0.9276977107591831\n",
      "|    |    |    |    |    |    Leaf: -0.5426234359334491\n",
      "|    |    |    |    |    25 => -0.7488175182481752\n",
      "|    |    |    |    |    |    |    Leaf: -0.8395839209131732\n",
      "|    |    |    |    |    |    6 => -0.5092332396627861\n",
      "|    |    |    |    |    |    |    Leaf: -0.7529276145804348\n",
      "|    50 => -0.7681901279707495\n",
      "|    |    |    |    |    Leaf: -0.6518662797630879\n",
      "|    |    |    |    4 => 0.4384468048058505\n",
      "|    |    |    |    |    |    Leaf: -0.8407322648686881\n",
      "|    |    |    |    |    71 => -0.9947259338165482\n",
      "|    |    |    |    |    |    Leaf: -0.9245538021226696\n",
      "|    |    |    94 => -0.32063896284292165\n",
      "|    |    |    |    Leaf: -0.5374962694595401\n",
      "|    |    93 => -0.22749916971105943\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.871138141361141\n",
      "|    |    |    |    |    |    |    67 => 0.49525867242828003\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9026263215156961\n",
      "|    |    |    |    |    |    |    |    90 => -0.3759398496240601\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9258487807678888\n",
      "|    |    |    |    |    |    |    |    |    |    |    9 => -0.5494032478966935\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.897582441526473\n",
      "|    |    |    |    |    |    |    |    |    |    27 => -0.992201693099076\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.952644061957573\n",
      "|    |    |    |    |    |    |    |    |    |    |    27 => -0.9933642184796767\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9226016173982944\n",
      "|    |    |    |    |    |    |    |    |    40 => 0.048128342245989275\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.977477315474692\n",
      "|    |    |    |    |    |    |    |    |    |    35 => -0.22026676546869223\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9586920067899518\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    23 => -0.9385020833333333\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9459593751242199\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    2 => -0.9552084410882383\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.932821878602377\n",
      "|    |    |    |    |    |    |    |    |    |    |    2 => -0.9859315196027723\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9621929836184318\n",
      "|    |    |    |    |    |    75 => -0.8445766358688773\n",
      "|    |    |    |    |    |    |    Leaf: -0.8101831608609047\n",
      "|    |    |    |    |    48 => -0.05229860818220172\n",
      "|    |    |    |    |    |    |    Leaf: -0.697367200646606\n",
      "|    |    |    |    |    |    13 => 0.43976644130301146\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8293649216255249\n",
      "|    |    |    |    |    |    |    |    39 => -0.5153393244499536\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8813124688070033\n",
      "|    |    |    |    |    |    |    42 => -0.45531914893617\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9507669612610583\n",
      "|    |    |    |    3 => 0.6925219185146984\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8455515237325469\n",
      "|    |    |    |    |    |    |    88 => 0.2623762376237623\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8888501200117582\n",
      "|    |    |    |    |    |    |    |    38 => -0.2256267409470752\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9236704606184274\n",
      "|    |    |    |    |    |    7 => -0.40078521184361193\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7571097949322698\n",
      "|    |    |    |    |    |    |    8 => -0.7722956934554086\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8960837054409421\n",
      "|    |    |    |    |    35 => -0.7345313078918118\n",
      "|    |    |    |    |    |    Leaf: -0.5946964176085296\n",
      "|    |    |    49 => -0.998136694233988\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8049375845615461\n",
      "|    |    |    |    |    |    |    4 => -0.46961518370189803\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7484508398369352\n",
      "|    |    |    |    |    |    |    |    40 => 0.5323286339329119\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9243300222749283\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    16 => -0.2136071528359877\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8373630882011585\n",
      "|    |    |    |    |    |    |    |    |    |    |    35 => -0.39644312708410523\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8657850918454328\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    14 => -0.7856049004594181\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9505672279320435\n",
      "|    |    |    |    |    |    |    |    |    |    99 => -0.8744049624716475\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9702637804955252\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    43 => 0.8206144697720517\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9068710718907482\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    96 => 0.6402972027972026\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9309668614434683\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    82 => -0.7583081570996979\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8882244898860553\n",
      "|    |    |    |    |    |    |    |    |    |    |    67 => 0.48733645420717797\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9500923091871933\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    8 => -0.6958290946083419\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.910818945908583\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    2 => -0.7985931519602772\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9541214506234182\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    52 => 0.16238917405506303\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9879821388347843\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    35 => -0.10022230455724351\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9722827065779988\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    55 => -0.1084090909090909\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9605923688127449\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    46 => 0.43284936479128855\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9423407666520915\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    15 => -0.33581641235131565\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9822448357647309\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    41 => 0.17319587628865996\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9376069395308573\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    54 => -0.5870560574186363\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9912044424506166\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    42 => -0.46382978723404256\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9656269737161736\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    50 => -0.9290676416819013\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9139890377319324\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    42 => -0.5404255319148936\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9027904758119012\n",
      "|    |    |    |    |    |    |    |    |    49 => -0.9994066991813206\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8696320440964078\n",
      "|    |    |    |    |    |    |    |    |    |    48 => 0.5115984816533107\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9058681848362653\n",
      "|    |    |    |    |    |    |    |    |    |    |    95 => 0.7218642289824664\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9309285833116179\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    14 => -0.37978560490045943\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9408720641777046\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    9 => -0.4905106632752887\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9736852565886234\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9952845508665298\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9919350920662682\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    88 => -0.06930693069306954\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9842270541002505\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    44 => 0.7615778136779753\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9638845825990557\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9983526970517435\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9839421133693106\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    98 => -0.995572865588837\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9749870811305005\n",
      "|    |    |    |    |    |    6 => -0.785828984343637\n",
      "|    |    |    |    |    |    |    Leaf: -0.8303316898273901\n",
      "|    |    |    |    |    2 => -0.9405192924381918\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7355010533847438\n",
      "|    |    |    |    |    |    |    32 => -0.3463724506853895\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8875094039064306\n",
      "|    |    |    |    |    |    |    |    |    69 => 0.1848721283051582\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8815792379412843\n",
      "|    |    |    |    |    |    |    |    |    |    92 => -0.999521393701541\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9262792625276229\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    36 => -0.28811188811188826\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9390040072156381\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    95 => 0.04315899895099662\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9539219626670437\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    14 => -0.8453292496171516\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9734467543824781\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    49 => -0.9995772968608132\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9787585812946548\n",
      "|    |    |    |    |    |    |    |    |    |    |    96 => -0.023164335664335678\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8631262339176637\n",
      "|    |    |    |    |    |    |    |    83 => -0.7372505543237251\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9424768012437448\n",
      "|    |    |    |    |    |    |    |    |    26 => -0.6260064412238325\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9680200930747498\n",
      "|    |    |    |    |    |    |    |    |    |    28 => -0.35390516039051606\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9884447889476311\n",
      "|    |    |    |    |    |    40 => 0.03986387943607195\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.906151590235543\n",
      "|    |    |    |    |    |    |    |    |    10 => -0.986284585563199\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8721893177511892\n",
      "|    |    |    |    |    |    |    |    |    |    41 => 0.04742268041237119\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8363786531692086\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    94 => 0.6351429563606897\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9839496848679183\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    71 => -0.9723965449366649\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9545255793616091\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    20 => -0.7199001119435116\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9898398060182056\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    18 => -0.28204518430439957\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9214727069558727\n",
      "|    |    |    |    |    |    |    |    |    |    |    5 => -0.932226541977514\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8816587386766661\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    48 => 0.709405314213412\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9256447288804088\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    74 => -0.7184758084733016\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9482037986629154\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    72 => 0.9686331870632213\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9987120880868148\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    89 => 0.1390374331550801\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9586360618280164\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    38 => -0.0362116991643453\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9573743557127904\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    13 => 0.6024892440073752\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9732268023479218\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    40 => -0.4637822070977151\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9854584321571641\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    11 => 0.672\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9815293291462946\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    69 => -0.0807325530992632\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9916057318768293\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    22 => -0.9325146143692249\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.998570753446136\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    53 => -0.743172268907563\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9826747286301286\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    55 => -0.4810227272727272\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9434626103624792\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    39 => -0.44081189959714906\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9693365663632388\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    98 => -0.9858499817871053\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9931417365610633\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    94 => 0.36636184743604594\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.980575320321713\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    35 => -0.4981474620229715\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9795050048657394\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    69 => -0.40713047247507583\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9736019701039377\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    39 => -0.6439417415556243\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9667207397858907\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    67 => 0.6004081142719959\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9838944760239032\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    35 => -0.5414968506854391\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9454006616227867\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    49 => -0.999876790564811\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9488169848904435\n",
      "|    |    |    |    |    |    |    |    18 => -0.33840665873959574\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9205522651084523\n",
      "|    |    |    |    |    |    |    |    |    61 => -0.6313592486303157\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9180082415762346\n",
      "|    |    |    |    |    |    |    |    |    |    |    19 => 0.2478745498245969\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9696218856691091\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    24 => -0.5279423538831065\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9838299430195655\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    100 => -0.9076016933554206\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.970106619319562\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    52 => -0.729040286203142\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9612941836378655\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    11 => 0.9097\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9787468034079316\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    2 => -0.9933795386365987\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9861723929279679\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    49 => -0.9995469683844589\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9856545339711059\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    74 => -0.9360742040611683\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9954104099272442\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    90 => -0.7218045112781953\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9054912924611219\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    53 => 0.02993697478991597\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9526705622027004\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    73 => 0.82029385909833\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9926991824674378\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    24 => -0.6396552536146565\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9823155030850702\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    14 => -0.6171516079632465\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9743300958867203\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    68 => -0.9764468371467026\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9689107957581942\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    98 => -0.9976463336041917\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9517907540644751\n",
      "|    |    |    |    |    |    |    |    |    |    10 => -0.9965132431754778\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9474500139126287\n",
      "|    |    |    |    |    |    |    |    |    |    |    51 => -0.9957252134152635\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9616386868245198\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    83 => -0.5\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9955143918414577\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    3 => 0.9791645177926767\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9853836425764717\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    50 => -0.9129798903107862\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.97767619350479\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    1 => -0.5407608695652174\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.991729715166532\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    62 => -0.8605131229725744\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9735413981150755\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    16 => -0.3568035764179938\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9932041593606963\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    49 => -0.9992209372636511\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9735395052404234\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    75 => -0.12403493228705209\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9862149650810197\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    40 => -0.2639766650461837\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.982876424940264\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    95 => 0.6003296867975423\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9904775785842687\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    2 => -0.9601737871107893\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9946822841444617\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9987957703999379\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    56 => -0.9861415025528811\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9943797396792587\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    69 => -0.27384048547897705\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9814851620710825\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    23 => -0.9056020833333334\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    43 => 0.9093869460569166\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    29 => -0.9818877037633327\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9686502100144426\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    66 => -0.6949685534591196\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9764471710252209\n",
      "|    |    |    |    |    |    |    6 => -0.8346045764753112\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8851600835641062\n",
      "|    |    |    |    34 => -0.961025427261359\n",
      "|    |    |    |    |    Leaf: -0.5242675994752952\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=100, minElems_perLeaf=10, post_pruning=True)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.0847536097367572\n",
      "Root Mean Square Error: 0.13403475866886674\n",
      "R^2 score: 0.5770695935516105\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.06702521562253035),\n",
       " (100, 0.06412261027028934),\n",
       " (36, 0.06108907835644606),\n",
       " (49, 0.04547755072596042),\n",
       " (13, 0.02548077375491929),\n",
       " (94, 0.02416452845156344),\n",
       " (38, 0.019866649044650957),\n",
       " (32, 0.016564579887192003),\n",
       " (46, 0.016302005629638593),\n",
       " (98, 0.012522158583232646),\n",
       " (43, 0.01157343698456752),\n",
       " (72, 0.01096262448873916),\n",
       " (99, 0.009136088069872749),\n",
       " (4, 0.008880883383477027),\n",
       " (76, 0.007914468445432313),\n",
       " (37, 0.007637729770328492),\n",
       " (1, 0.007608964262434628),\n",
       " (54, 0.006684546017686161),\n",
       " (80, 0.006588530959434208),\n",
       " (50, 0.006186572357350506),\n",
       " (12, 0.006084909063030391),\n",
       " (0, 0.005515728487946607),\n",
       " (44, 0.005388847108452802),\n",
       " (81, 0.005320940794582167),\n",
       " (25, 0.004831572634684013),\n",
       " (6, 0.004740577343937272),\n",
       " (15, 0.004703170319397627),\n",
       " (40, 0.004396891676440733),\n",
       " (8, 0.004216425276131762),\n",
       " (35, 0.0037306561184097505),\n",
       " (28, 0.0031286369597798096),\n",
       " (18, 0.003096145019872206),\n",
       " (34, 0.002982834881994128),\n",
       " (73, 0.002226541420462243),\n",
       " (26, 0.001999925986279878),\n",
       " (93, 0.0019603527216376254),\n",
       " (60, 0.0019415726997025688),\n",
       " (59, 0.0018948095304858383),\n",
       " (71, 0.0018202577041378008),\n",
       " (7, 0.001782362615224103),\n",
       " (2, 0.0017723194544061608),\n",
       " (75, 0.0016637877691283802),\n",
       " (42, 0.0015182971835677058),\n",
       " (16, 0.001433211679717527),\n",
       " (14, 0.00127855595835383),\n",
       " (48, 0.0012312965176598594),\n",
       " (9, 0.0011418456185469923),\n",
       " (23, 0.0011159508149547334),\n",
       " (5, 0.0009013061641484738),\n",
       " (92, 0.0008600032457443945),\n",
       " (91, 0.0007992341083056875),\n",
       " (39, 0.0007733285501236285),\n",
       " (52, 0.0007500841008841226),\n",
       " (88, 0.0007245260571570672),\n",
       " (57, 0.0006709653547929421),\n",
       " (77, 0.0006604188188034797),\n",
       " (84, 0.0006406297771452093),\n",
       " (56, 0.000525782440377696),\n",
       " (67, 0.0005005209729603864),\n",
       " (83, 0.0004683996270901073),\n",
       " (74, 0.000439042848641039),\n",
       " (27, 0.00039052432918711366),\n",
       " (95, 0.0003819812006624301),\n",
       " (69, 0.00036376339141214284),\n",
       " (96, 0.00035801236586242575),\n",
       " (82, 0.00024017050170466701),\n",
       " (41, 0.0002336938892698614),\n",
       " (20, 0.00022887829444111012),\n",
       " (90, 0.00022029655981238344),\n",
       " (53, 0.00020546702855276787),\n",
       " (10, 0.00014008119098198285),\n",
       " (55, 8.32408919773868e-05),\n",
       " (11, 6.983063541930872e-05),\n",
       " (24, 6.415696761999625e-05),\n",
       " (19, 4.1077637423239286e-05),\n",
       " (61, 3.8814762240371026e-05),\n",
       " (62, 3.613572476470845e-05),\n",
       " (89, 2.4367599699383108e-05),\n",
       " (68, 2.400012387993428e-05),\n",
       " (29, 1.5308518142124282e-05),\n",
       " (66, 1.4559909368161333e-05),\n",
       " (51, 1.4305977605707425e-05),\n",
       " (22, 1.1790995771524032e-05),\n",
       " (17, 0),\n",
       " (21, 0),\n",
       " (30, 0),\n",
       " (31, 0),\n",
       " (33, 0),\n",
       " (45, 0),\n",
       " (47, 0),\n",
       " (58, 0.0),\n",
       " (63, 0),\n",
       " (64, 0),\n",
       " (65, 0),\n",
       " (70, 0),\n",
       " (78, 0),\n",
       " (79, 0),\n",
       " (85, 0),\n",
       " (86, 0),\n",
       " (87, 0),\n",
       " (97, 0),\n",
       " (101, 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndt.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  51,  43, 100,  36,   2,   3,  93,  11,  44,  27,  58,  94,\n",
       "        49,  99,  18,  61,  91,  41,  40,  74,  46,  30,  25,  60,   8,\n",
       "        65,  20,   9,  77,  13,  88,   4, 101,  95,  62,  47,  21,  68,\n",
       "        98,  73,  15,  33,  22,  89,  63,  26,  16,  85,  90,  23,  14,\n",
       "        28,  69,  38,  45,  71,  81,  12,   6,  32,  76,  67,  92,   5,\n",
       "        31,  57,  48,  86,  87,  72,  78,  52,  59,  96,  37,  75,  42,\n",
       "        83,  55,  35,  97,  53,  56,  82,  54,   0,   7,  17,  19,  79,\n",
       "        10,  70,   1,  39,  66,  24,  64,  34,  29,  84,  80])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.09497097426128245\n",
      "Root Mean Square Error: 0.14954065239741324\n",
      "R^2 score: 0.4735555175731505\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Random Forest class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalRandomForest_regressor: # post-pruning (kinda) with cross-validation or greedy \n",
    "    def __init__(self, n_trees, trees=None, bootstrap_subSpace=None, outOfBag_error=None, feat_importances=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = trees\n",
    "        self.bootSamplesIdxs_SubSp = bootstrap_subSpace\n",
    "        self.oob_error = outOfBag_error\n",
    "        self.feature_importances = feat_importances\n",
    "        \n",
    "    def fit(self, X, y, depth, minElems_perLeaf, post_pruning=True, verbose=False):\n",
    "        n,d = X.shape\n",
    "        \n",
    "        self.trees = []\n",
    "        self.bootSamplesIdxs_SubSp = []\n",
    "        self.feature_importances = {k:0 for k in range(d)}\n",
    "        \n",
    "        n_learn = int(n/3) # Bootstrap amount to be taken aside\n",
    "        # random subspace method amount\n",
    "        d_learn = int(d/self.n_trees) if int(d/self.n_trees) >= int(np.sqrt(d)) else int(np.sqrt(d))\n",
    "                \n",
    "        # Fitting the forest -----------------------------------\n",
    "        for i in range(self.n_trees):\n",
    "            if verbose: print(\"\\tFitting #{} tree\".format(i+1))\n",
    "            \n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            subspace_idxs = np.sort(np.random.permutation(d)[:d_learn])\n",
    "            self.bootSamplesIdxs_SubSp.append((bootstrap_idxs, subspace_idxs))\n",
    "                        \n",
    "            dt = NumericalDecisionTree_regressor()\n",
    "            \n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs][:,subspace_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=depth, minElems_perLeaf=minElems_perLeaf, post_pruning=post_pruning))\n",
    "            \n",
    "            for k,v in dt.feature_importances.items():\n",
    "                self.feature_importances[subspace_idxs[k]] += v\n",
    "        \n",
    "        # Out-Of-Bag Estimate for the forest -------------------\n",
    "        oob_errors = []\n",
    "        for sampleIdx in range(n):\n",
    "            # finds all the tree that not have current sample in their bootstrapp set (not trained on it)\n",
    "            # and the relative subspace on wich they have been trained\n",
    "            missingTreesIdx_subspc = [(idx,subspace_idxs) for idx,(bootstrap_idxs,subspace_idxs)\n",
    "                                      in enumerate(self.bootSamplesIdxs_SubSp)\n",
    "                                      if sampleIdx not in bootstrap_idxs]\n",
    "\n",
    "            if len(missingTreesIdx_subspc) == 0: continue\n",
    "            \n",
    "            regr_results = np.empty(len(missingTreesIdx_subspc)) # regression estimate of the selected trees\n",
    "            for i, (missingTree_idx, tree_subSpace) in enumerate(missingTreesIdx_subspc):\n",
    "                # reshape in order to correctly use the decision_tree.predict(...): it needs a matrix (num,dim)\n",
    "                # while numpy matrix indexing returns (dim,)\n",
    "                regr_results[i] = self.trees[missingTree_idx].predict(X[sampleIdx,tree_subSpace].reshape(1,-1))\n",
    "            \n",
    "            # done at this level of granularity because a sample might end up in \n",
    "            # being part of no bootstrap set of any tree (so we cannot predict wich value in y will be used)\n",
    "            oob_errors.append(np.square(y[sampleIdx]-np.mean(regr_results)))\n",
    "            #oob_errors.append(r2_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            #oob_errors.append(explained_variance_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            \n",
    "        self.oob_error = np.sqrt(np.mean(oob_errors))\n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"Trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n,_ = X.shape\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, (tree,(_,subspace_idxs)) in enumerate(zip(self.trees, self.bootSamplesIdxs_SubSp)):\n",
    "            results[row] = tree.predict(X[:,subspace_idxs])\n",
    "            \n",
    "        return np.mean(results,axis=0)\n",
    "    \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.feature_importances is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.feature_importances)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(self.feature_importances.items(), key=lambda kv: kv[1], reverse=True)[:n_printFeat]\n",
    "\n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "        return sorted(zip(columns, map(lambda kv: round(kv[1], 4), self.feature_importances.items())),\n",
    "                      key=lambda kv: kv[1], reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 trees\n",
      "Training 10 trees\n",
      "Training 20 trees\n",
      "Training 30 trees\n",
      "Training 50 trees\n",
      "Training 70 trees\n",
      "Training 100 trees\n",
      "Training 150 trees\n",
      "Training 200 trees\n"
     ]
    }
   ],
   "source": [
    "n_trees = [1,10,20,30,50,70,100,150,200]\n",
    "oob_errors = np.empty(len(n_trees))\n",
    "for i, num_t in enumerate(n_trees):\n",
    "    print(\"Training {} trees\".format(num_t))\n",
    "    nrf = NumericalRandomForest_regressor(num_t)\n",
    "    # no train and test, cause it's a forest\n",
    "    nrf.fit(data, values, depth=100, minElems_perLeaf=5);\n",
    "    oob_errors[i] = nrf.oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Out-Of-Bag errors per estimator amount (trees number)\")\n",
    "plt.errorbar(n_trees, oob_errors, fmt='--o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  49,   3, 100,  51,  43,  44,  99,   2,  91,  69,  41,  40,\n",
       "        46,  92,  68,  93,  38,  66,  65,  45,  73,  82,  71,  59,  11,\n",
       "        15,   1,   4,  88,  26,  10,  36,  29,   6,  74,  14,  78,  62,\n",
       "         9,  94,  89,  17,   0,  77,  24,  18,  98,  32,   8,  30,  28,\n",
       "        96,  25,  86,  34,  23,  21,  35,  80,  81,   5,  60,  72,  39,\n",
       "        75,  27,  67,  57,  61,  22,  33,  97,   7,  58,  53,  16,  37,\n",
       "        76,  47,  48,  95,  12,  90,  63,  19, 101,  79,  42,  20,  55,\n",
       "        31,  64,  87,  52,  56,  13,  83,  84,  54,  85,  70])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=100)\n",
    "# oob error not working, need to perform evaluation on test split\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07796936773408789\n",
      "Root Mean Square Error: 0.09510583826804896\n",
      "R^2 score: 0.7870643430529308\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors - Models definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regulariser: 1\n",
      "Residual variance: 0.011264853457839813\n",
      "Root Mean Square Error: 0.10614492762758365\n",
      "R^2 score: 0.7347639651407888\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[1,1.3,1.5,1.7,2]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)[0]\n",
    "print(\"Best regulariser: {}\".format(win_regulariser))\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06059611468919796\n",
      "Root Mean Square Error: 0.11111080712617025\n",
      "R^2 score: 0.7093658514952168\n"
     ]
    }
   ],
   "source": [
    "rf = NumericalRandomForest_regressor(100)\n",
    "rf.fit(trainVal_data, trainVal_values, depth=100, minElems_perLeaf=10);\n",
    "\n",
    "pred = rf.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1002318475113632"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(63, 0.8102703090800211),\n",
       " (20, 0.7747147379215364),\n",
       " (50, 0.7262307849364978),\n",
       " (44, 0.6375338131574837),\n",
       " (67, 0.5282026531361459),\n",
       " (49, 0.4677182960905943),\n",
       " (100, 0.4642171242352701),\n",
       " (3, 0.43004436190169865),\n",
       " (0, 0.4249408828123755),\n",
       " (17, 0.4179337891872721),\n",
       " (65, 0.40635643267033555),\n",
       " (91, 0.39314729279963967),\n",
       " (45, 0.392260720772472),\n",
       " (29, 0.37227305434156266),\n",
       " (37, 0.3711159779695846),\n",
       " (90, 0.3662534111236556),\n",
       " (10, 0.36219254222920694),\n",
       " (74, 0.3613896443847644),\n",
       " (68, 0.35211681021890984),\n",
       " (43, 0.33390132017174884),\n",
       " (8, 0.3280911011755026),\n",
       " (40, 0.32626454762021406),\n",
       " (27, 0.3187609061829714),\n",
       " (99, 0.31485295794954055),\n",
       " (5, 0.3029501049012625),\n",
       " (2, 0.2935040362973256),\n",
       " (22, 0.2918601745679328),\n",
       " (30, 0.2864631478335665),\n",
       " (41, 0.28368526045952014),\n",
       " (39, 0.2768459220527722),\n",
       " (26, 0.27505669420963913),\n",
       " (51, 0.27196091104433323),\n",
       " (12, 0.27085083915006664),\n",
       " (38, 0.2679486491842313),\n",
       " (18, 0.2638465257468755),\n",
       " (93, 0.2591862843066315),\n",
       " (80, 0.25802624475960423),\n",
       " (79, 0.24786155355270398),\n",
       " (64, 0.24550662724844646),\n",
       " (14, 0.24450125833451425),\n",
       " (28, 0.23484988014680816),\n",
       " (16, 0.23419791817750274),\n",
       " (48, 0.2268180642651015),\n",
       " (9, 0.2185457838972767),\n",
       " (71, 0.2182928215403106),\n",
       " (55, 0.2180433522184357),\n",
       " (89, 0.2176344212654721),\n",
       " (69, 0.2146649016148257),\n",
       " (76, 0.2134378626152517),\n",
       " (56, 0.21246742538701766),\n",
       " (66, 0.21077685945847563),\n",
       " (46, 0.21055442936085755),\n",
       " (4, 0.20858183854607762),\n",
       " (78, 0.20097643838843215),\n",
       " (58, 0.19668217961447496),\n",
       " (34, 0.1966338994156529),\n",
       " (60, 0.1861586671740947),\n",
       " (81, 0.1852511216198045),\n",
       " (54, 0.18244873584096588),\n",
       " (92, 0.17684452004167317),\n",
       " (94, 0.17375490446271485),\n",
       " (87, 0.17175640494426425),\n",
       " (84, 0.17169553273584817),\n",
       " (96, 0.16489587852511084),\n",
       " (24, 0.15928306693400993),\n",
       " (6, 0.1572663073699495),\n",
       " (42, 0.1557229310288753),\n",
       " (31, 0.1544975037492422),\n",
       " (72, 0.14884080337966543),\n",
       " (1, 0.1485114539998944),\n",
       " (13, 0.1466918470634371),\n",
       " (59, 0.14457231126412232),\n",
       " (83, 0.14211324705889705),\n",
       " (53, 0.14050364992645395),\n",
       " (97, 0.13178334650878076),\n",
       " (25, 0.13164701894227904),\n",
       " (61, 0.13154500516484102),\n",
       " (19, 0.1280527280542239),\n",
       " (88, 0.12455641462837232),\n",
       " (15, 0.12284160918103033),\n",
       " (85, 0.12167993523495703),\n",
       " (62, 0.12024574520002373),\n",
       " (101, 0.11821374096188739),\n",
       " (36, 0.11754896771670917),\n",
       " (73, 0.11745122100369379),\n",
       " (47, 0.11729058588880086),\n",
       " (11, 0.11518527984376782),\n",
       " (98, 0.11198656573993367),\n",
       " (57, 0.11134733110300249),\n",
       " (86, 0.10928113437983374),\n",
       " (35, 0.10744899664176508),\n",
       " (21, 0.09701605226173987),\n",
       " (77, 0.0953148475707695),\n",
       " (52, 0.08602398294223672),\n",
       " (82, 0.07453049775052334),\n",
       " (75, 0.07408384284226203),\n",
       " (95, 0.07058686126073876),\n",
       " (33, 0.06813589917305923),\n",
       " (7, 0.04576708583139109),\n",
       " (32, 0.04405849025882402),\n",
       " (23, 0.04212276007980778),\n",
       " (70, 0.03548909684491847)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM - Not Working (use the sklearn model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_SupportVector_regression:\n",
    "    def __init__(self, weight=None, alpha=None, bias=None):\n",
    "        self.x = alpha\n",
    "        self.w = weight\n",
    "        self.bias = bias\n",
    "        self.Nabla = None\n",
    "                \n",
    "    def SMO2_ab(self, n, H, f, a, LB, UB, maxiter, eps, alpha_s):\n",
    "        \"\"\"\n",
    "        % min_{x} .5 x H x + f' x \n",
    "        %         LB <= x <= UB\n",
    "        %         a' x = b\n",
    "        % n         grandezza problema length(x)\n",
    "        % maxiter   max num it\n",
    "        % eps       precisione\n",
    "        % alpha_s   punto di inizio valido per x\n",
    "        % Nabla     ....\n",
    "        % err       flag di ok\n",
    "        % x         valore della soluzione ottima\n",
    "        % bias      ....\n",
    "        \"\"\"\n",
    "        self.x = alpha_s\n",
    "        self.Nabla = f\n",
    "        for i in range(n):\n",
    "            if (self.x[i] != 0.0):\n",
    "                for j in range(n):\n",
    "                    self.Nabla[j] += H[j,i] * self.x[i]\n",
    "        iter_ = 0\n",
    "        while True:\n",
    "            minF_up = float(\"inf\");\n",
    "            maxF_low = float(\"-inf\");\n",
    "            for i in range(n): \n",
    "                F_i = self.Nabla[i]/a[i]\n",
    "                if (LB[i] < self.x[i]) and (self.x[i] < UB[i]) :\n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == LB[i])) or ((a[i] < 0) and (self.x[i] == UB[i]))) : \n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == UB[i])) or ((a[i] < 0) and (self.x[i] == LB[i]))) : \n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "            if ((maxF_low - minF_up) <= eps):\n",
    "                err = 0.0\n",
    "                break\n",
    "\n",
    "            iter_ += 1\n",
    "            if (iter_ >= maxiter):\n",
    "                err = 1.0\n",
    "                break\n",
    "\n",
    "            if (a[u] > 0):\n",
    "                tau_lb = (LB[u]-self.x[u])*a[u] \n",
    "                tau_ub = (UB[u]-self.x[u])*a[u] \n",
    "            else:\n",
    "                tau_ub = (LB[u]-self.x[u])*a[u] \n",
    "                tau_lb = (UB[u]-self.x[u])*a[u]\n",
    "\n",
    "            if (a[v] > 0):\n",
    "                tau_lb = max(tau_lb,(self.x[v]-UB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-LB[v])*a[v]) \n",
    "            else:\n",
    "                tau_lb = max(tau_lb,(self.x[v]-LB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-UB[v])*a[v])\n",
    "\n",
    "            tau = (self.Nabla[v]/a[v]-self.Nabla[u]/a[u])/(H[u,u]/(a[u]*a[u])\n",
    "                                                           +H[v,v]/(a[v]*a[v])\n",
    "                                                           -2*H[v,u]/(a[u]*a[v]))\n",
    "            tau = min(max(tau,tau_lb),tau_ub)\n",
    "            self.x[u] += tau/a[u]\n",
    "            self.x[v] -= tau/a[v]\n",
    "\n",
    "            for i in range(n):\n",
    "                self.Nabla[i] += H[u,i]*tau/a[u] - H[v,i]*tau/a[v]\n",
    "\n",
    "        tsv = 0\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for k in range(n):\n",
    "            if ((self.x[k] > LB[k]) and (self.x[k] < UB[k])):\n",
    "                self.bias -= self.Nabla[k]/a[k]\n",
    "                tsv += 1\n",
    "\n",
    "        if (tsv > 0):\n",
    "            self.bias /= tsv\n",
    "        else:    \n",
    "            self.bias = -(maxF_low + minF_up)/2.0\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def fit(self, X, y, C):\n",
    "        n = X.shape[0]\n",
    "        cov = np.matmul(X, X.T)\n",
    "        Q = np.matmul(np.matmul(np.diag(y.flatten()), cov),\n",
    "                      np.diag(y.flatten()))\n",
    "        \n",
    "        if self.SMO2_ab(n,Q,-np.ones(n),y.flatten(),\n",
    "                   np.zeros(n),C*np.ones(n),10000000,.0001,np.zeros(n)):\n",
    "            print(\"Problem in SMO\")\n",
    "            \n",
    "        self.w = np.matmul(np.matmul(X.T, np.diag(y.flatten())),\n",
    "                           self.x)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.matmul(X, self.w) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 7.510985071370466\n",
      "Root Mean Square Error: 7.947978992854457\n",
      "R^2 score: -1486.1249411517315\n"
     ]
    }
   ],
   "source": [
    "lsvr = linear_SupportVector_regression()\n",
    "lsvr.fit(trainVal_data, trainVal_values, C=1.0);\n",
    "\n",
    "pred = lsvr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   8,   27,   38,   73,   91,  105,  108,  110,  117,  248,  252,\n",
       "         263,  268,  374,  457,  503,  511,  557,  558,  588,  604,  635,\n",
       "         706,  822,  845,  846,  986, 1003, 1014, 1030, 1081, 1104, 1129,\n",
       "        1131, 1192, 1200, 1268, 1316, 1335, 1437, 1475]),)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lsvr.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel=\"linear\", tol=.0001, C=1)\n",
    "svr.fit(trainVal_data, trainVal_values.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(svr.dual_coef_)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.08268315587685623\n",
      "Root Mean Square Error: 0.11123349946631246\n",
      "R^2 score: 0.7087236408509083\n"
     ]
    }
   ],
   "source": [
    "pred = svr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Features Elimination\n",
    "<img src=\"img/Algo1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFitting #1 tree\n",
      "\tFitting #2 tree\n",
      "\tFitting #3 tree\n",
      "\tFitting #4 tree\n",
      "\tFitting #5 tree\n",
      "\tFitting #6 tree\n",
      "\tFitting #7 tree\n",
      "\tFitting #8 tree\n",
      "\tFitting #9 tree\n",
      "\tFitting #10 tree\n",
      "\tFitting #11 tree\n",
      "\tFitting #12 tree\n",
      "\tFitting #13 tree\n",
      "\tFitting #14 tree\n",
      "\tFitting #15 tree\n",
      "\tFitting #16 tree\n",
      "\tFitting #17 tree\n",
      "\tFitting #18 tree\n",
      "\tFitting #19 tree\n",
      "\tFitting #20 tree\n",
      "\tFitting #21 tree\n",
      "\tFitting #22 tree\n",
      "\tFitting #23 tree\n",
      "\tFitting #24 tree\n",
      "\tFitting #25 tree\n",
      "\tFitting #26 tree\n",
      "\tFitting #27 tree\n",
      "\tFitting #28 tree\n",
      "\tFitting #29 tree\n",
      "\tFitting #30 tree\n",
      "\tFitting #31 tree\n",
      "\tFitting #32 tree\n",
      "\tFitting #33 tree\n",
      "\tFitting #34 tree\n",
      "\tFitting #35 tree\n",
      "\tFitting #36 tree\n",
      "\tFitting #37 tree\n",
      "\tFitting #38 tree\n",
      "\tFitting #39 tree\n",
      "\tFitting #40 tree\n",
      "\tFitting #41 tree\n",
      "\tFitting #42 tree\n",
      "\tFitting #43 tree\n",
      "\tFitting #44 tree\n",
      "\tFitting #45 tree\n",
      "\tFitting #46 tree\n",
      "\tFitting #47 tree\n",
      "\tFitting #48 tree\n",
      "\tFitting #49 tree\n",
      "\tFitting #50 tree\n",
      "\tFitting #51 tree\n",
      "\tFitting #52 tree\n",
      "\tFitting #53 tree\n",
      "\tFitting #54 tree\n",
      "\tFitting #55 tree\n",
      "\tFitting #56 tree\n",
      "\tFitting #57 tree\n",
      "\tFitting #58 tree\n",
      "\tFitting #59 tree\n",
      "\tFitting #60 tree\n",
      "\tFitting #61 tree\n",
      "\tFitting #62 tree\n",
      "\tFitting #63 tree\n",
      "\tFitting #64 tree\n",
      "\tFitting #65 tree\n",
      "\tFitting #66 tree\n",
      "\tFitting #67 tree\n",
      "\tFitting #68 tree\n",
      "\tFitting #69 tree\n",
      "\tFitting #70 tree\n",
      "\tFitting #71 tree\n",
      "\tFitting #72 tree\n",
      "\tFitting #73 tree\n",
      "\tFitting #74 tree\n",
      "\tFitting #75 tree\n",
      "\tFitting #76 tree\n",
      "\tFitting #77 tree\n",
      "\tFitting #78 tree\n",
      "\tFitting #79 tree\n",
      "\tFitting #80 tree\n",
      "\tFitting #81 tree\n",
      "\tFitting #82 tree\n",
      "\tFitting #83 tree\n",
      "\tFitting #84 tree\n",
      "\tFitting #85 tree\n",
      "\tFitting #86 tree\n",
      "\tFitting #87 tree\n",
      "\tFitting #88 tree\n",
      "\tFitting #89 tree\n",
      "\tFitting #90 tree\n",
      "\tFitting #91 tree\n",
      "\tFitting #92 tree\n",
      "\tFitting #93 tree\n",
      "\tFitting #94 tree\n",
      "\tFitting #95 tree\n",
      "\tFitting #96 tree\n",
      "\tFitting #97 tree\n",
      "\tFitting #98 tree\n",
      "\tFitting #99 tree\n",
      "\tFitting #100 tree\n"
     ]
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(100)\n",
    "nrf.fit(trainVal_data, trainVal_values, depth=200, minElems_perLeaf=5, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(72, 0.9524484627192393),\n",
       " (67, 0.8635572920627046),\n",
       " (50, 0.8597412689485155),\n",
       " (51, 0.7986621465066222),\n",
       " (80, 0.7922560196953415),\n",
       " (92, 0.735176462474321),\n",
       " (46, 0.7240044612761096),\n",
       " (0, 0.7168609329164001),\n",
       " (39, 0.7048973517453145),\n",
       " (77, 0.6877236971606682),\n",
       " (96, 0.6821037093695926),\n",
       " (35, 0.6734603468430364),\n",
       " (44, 0.6664170744000407),\n",
       " (30, 0.664593396423625),\n",
       " (32, 0.6627270246139443),\n",
       " (49, 0.6315688504902344),\n",
       " (2, 0.6258600725012444),\n",
       " (43, 0.6168410333464215),\n",
       " (4, 0.6145966962681682),\n",
       " (100, 0.6099221141498852),\n",
       " (79, 0.6002166065281893),\n",
       " (93, 0.5893493362203396),\n",
       " (60, 0.575923427389239),\n",
       " (1, 0.5402170456067623),\n",
       " (71, 0.5390105076035147),\n",
       " (17, 0.5255308583625676),\n",
       " (73, 0.5178316888557923),\n",
       " (15, 0.5048902247773122),\n",
       " (5, 0.5045457655092186),\n",
       " (20, 0.4998211230271873),\n",
       " (56, 0.49493915386369775),\n",
       " (31, 0.4659674107956854),\n",
       " (7, 0.4630751833275072),\n",
       " (69, 0.4622076399790457),\n",
       " (6, 0.4558022433634242),\n",
       " (99, 0.45441874676147676),\n",
       " (21, 0.4353547203113011),\n",
       " (78, 0.4311748929790993),\n",
       " (3, 0.4262273335482855),\n",
       " (47, 0.421389549428635),\n",
       " (48, 0.4126862642188166),\n",
       " (29, 0.3919674375177876),\n",
       " (59, 0.38797461019565566),\n",
       " (76, 0.382114579508358),\n",
       " (55, 0.36563875637416826),\n",
       " (62, 0.3635958451803366),\n",
       " (38, 0.35974232548634266),\n",
       " (40, 0.35781765570527274),\n",
       " (14, 0.35505034424777837),\n",
       " (90, 0.3505870883813183),\n",
       " (24, 0.33906433692786714),\n",
       " (8, 0.337583454674145),\n",
       " (66, 0.3366812969510376),\n",
       " (41, 0.32993115305124704),\n",
       " (36, 0.32792804970997663),\n",
       " (63, 0.3271091404915788),\n",
       " (88, 0.3157171834921916),\n",
       " (91, 0.3127575462798386),\n",
       " (95, 0.29579127297263136),\n",
       " (86, 0.2906012468142726),\n",
       " (23, 0.2862062521539355),\n",
       " (94, 0.2849065131939466),\n",
       " (84, 0.28021553304513624),\n",
       " (27, 0.276946137768313),\n",
       " (68, 0.2747521149614915),\n",
       " (81, 0.27116251827684695),\n",
       " (13, 0.2664209666009259),\n",
       " (74, 0.26511020856178374),\n",
       " (65, 0.26191907103595),\n",
       " (98, 0.2572803178870469),\n",
       " (10, 0.2551931339509319),\n",
       " (61, 0.2547569260818179),\n",
       " (22, 0.25233492438639316),\n",
       " (12, 0.24338661535991427),\n",
       " (97, 0.24249133603130898),\n",
       " (33, 0.22685058191910823),\n",
       " (34, 0.22131778908579236),\n",
       " (9, 0.22108408514286862),\n",
       " (42, 0.2207560251717587),\n",
       " (16, 0.21584058934853362),\n",
       " (57, 0.21167579663749933),\n",
       " (18, 0.19370494641008645),\n",
       " (58, 0.1920288836357905),\n",
       " (45, 0.18393171531488162),\n",
       " (52, 0.17362147388558624),\n",
       " (87, 0.1622752176704651),\n",
       " (85, 0.16108985420556082),\n",
       " (37, 0.15860291847960425),\n",
       " (26, 0.15558164924290666),\n",
       " (64, 0.15018855557406566),\n",
       " (75, 0.13799078019238276),\n",
       " (53, 0.13399391894435925),\n",
       " (82, 0.13017426710737678),\n",
       " (89, 0.13011071906127689),\n",
       " (11, 0.12281630555262715),\n",
       " (25, 0.12014116632058855),\n",
       " (19, 0.10456672113747151),\n",
       " (83, 0.10001396403195043),\n",
       " (28, 0.08025669264059196),\n",
       " (101, 0.07638201498813593),\n",
       " (54, 0.05904037448402011),\n",
       " (70, 0.013737722420237212)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Features sorted by their score:\\n\")\n",
    "nrf.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PctHousOccup', 0.9524),\n",
       " ('PctPersOwnOccup', 0.8636),\n",
       " ('PctKidsBornNeverMar', 0.8597),\n",
       " ('NumImmig', 0.7987),\n",
       " ('OwnOccMedVal', 0.7923),\n",
       " ('NumStreet', 0.7352),\n",
       " ('PctTeen2Par', 0.724),\n",
       " ('population', 0.7169),\n",
       " ('MalePctNevMarr', 0.7049),\n",
       " ('PctHousNoPhone', 0.6877),\n",
       " ('PctSameCity85', 0.6821),\n",
       " ('PctEmplProfServ', 0.6735),\n",
       " ('PctKids2Par', 0.6664),\n",
       " ('PctNotHSGrad', 0.6646),\n",
       " ('PctUnemployed', 0.6627),\n",
       " ('NumKidsBornNeverMar', 0.6316),\n",
       " ('racepctblack', 0.6259),\n",
       " ('PctFam2Par', 0.6168),\n",
       " ('racePctAsian', 0.6146),\n",
       " ('PctUsePubTrans', 0.6099),\n",
       " ('OwnOccLowQuart', 0.6002),\n",
       " ('PctForeignBorn', 0.5893),\n",
       " ('PctSpeakEnglOnly', 0.5759),\n",
       " ('householdsize', 0.5402),\n",
       " ('HousVacant', 0.539),\n",
       " ('pctWPubAsst', 0.5255),\n",
       " ('PctHousOwnOcc', 0.5178),\n",
       " ('pctWInvInc', 0.5049),\n",
       " ('racePctHisp', 0.5045),\n",
       " ('perCapInc', 0.4998),\n",
       " ('PctRecentImmig', 0.4949),\n",
       " ('PctBSorMore', 0.466),\n",
       " ('agePct12t29', 0.4631),\n",
       " ('PctHousLess3BR', 0.4622),\n",
       " ('agePct12t21', 0.4558),\n",
       " ('PopDens', 0.4544),\n",
       " ('whitePerCap', 0.4354),\n",
       " ('PctWOFullPlumb', 0.4312),\n",
       " ('racePctWhite', 0.4262),\n",
       " ('PctWorkMomYoungKids', 0.4214),\n",
       " ('PctWorkMom', 0.4127),\n",
       " ('PctLess9thGrade', 0.392),\n",
       " ('PctRecImmig10', 0.388),\n",
       " ('MedYrHousBuilt', 0.3821),\n",
       " ('PctImmigRec10', 0.3656),\n",
       " ('PctLargHouseFam', 0.3636),\n",
       " ('MalePctDivorce', 0.3597),\n",
       " ('FemalePctDiv', 0.3578),\n",
       " ('pctWFarmSelf', 0.3551),\n",
       " ('MedOwnCostPctIncNoMtg', 0.3506),\n",
       " ('AsianPerCap', 0.3391),\n",
       " ('agePct16t24', 0.3376),\n",
       " ('PersPerRentOccHous', 0.3367),\n",
       " ('TotalPctDiv', 0.3299),\n",
       " ('PctOccupManu', 0.3279),\n",
       " ('PctLargHouseOccup', 0.3271),\n",
       " ('MedRentPctHousInc', 0.3157),\n",
       " ('NumInShelters', 0.3128),\n",
       " ('PctSameHouse85', 0.2958),\n",
       " ('RentQrange', 0.2906),\n",
       " ('indianPerCap', 0.2862),\n",
       " ('PctBornSameState', 0.2849),\n",
       " ('RentMedian', 0.2802),\n",
       " ('NumUnderPov', 0.2769),\n",
       " ('PctPersDenseHous', 0.2748),\n",
       " ('OwnOccHiQuart', 0.2712),\n",
       " ('pctWWage', 0.2664),\n",
       " ('PctVacantBoarded', 0.2651),\n",
       " ('PersPerOwnOccHous', 0.2619),\n",
       " ('LandArea', 0.2573),\n",
       " ('numbUrban', 0.2552),\n",
       " ('PctNotSpeakEnglWell', 0.2548),\n",
       " ('blackPerCap', 0.2523),\n",
       " ('medIncome', 0.2434),\n",
       " ('PctSameState85', 0.2425),\n",
       " ('PctEmploy', 0.2269),\n",
       " ('PctEmplManu', 0.2213),\n",
       " ('agePct65up', 0.2211),\n",
       " ('PersPerFam', 0.2208),\n",
       " ('pctWSocSec', 0.2158),\n",
       " ('PctRecImmig5', 0.2117),\n",
       " ('pctWRetire', 0.1937),\n",
       " ('PctRecImmig8', 0.192),\n",
       " ('PctYoungKids2Par', 0.1839),\n",
       " ('PctImmigRecent', 0.1736),\n",
       " ('MedRent', 0.1623),\n",
       " ('RentHighQ', 0.1611),\n",
       " ('PctOccupMgmtProf', 0.1586),\n",
       " ('HispPerCap', 0.1556),\n",
       " ('PersPerOccupHous', 0.1502),\n",
       " ('PctVacMore6Mos', 0.138),\n",
       " ('PctImmigRec5', 0.134),\n",
       " ('OwnOccQrange', 0.1302),\n",
       " ('MedOwnCostPctInc', 0.1301),\n",
       " ('pctUrban', 0.1228),\n",
       " ('OtherPerCap', 0.1201),\n",
       " ('medFamInc', 0.1046),\n",
       " ('RentLowQ', 0.1),\n",
       " ('PctPopUnderPov', 0.0803),\n",
       " ('LemasPctOfficDrugUn', 0.0764),\n",
       " ('PctImmigRec8', 0.059),\n",
       " ('MedNumBR', 0.0137)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Features sorted by their score:\\n\")\n",
    "nrf.sort_featureImportances(columns=pred_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe(estimator, parameters_dict, data, vals, n_features=None, verbose=False):\n",
    "    _,d = data.shape\n",
    "    nf = n_features if n_features else int(d/2)\n",
    "    train_feat = list(range(d))    \n",
    "    accuracies = []\n",
    "\n",
    "    tr_data, ts_data, tr_vals, ts_vals = trainTest_split(in_matrix=data, out_vect=vals, train_amount=0.7)\n",
    "    \n",
    "    while d > nf:\n",
    "        if verbose:\n",
    "            update = round((data.shape[1]-d)*100/(data.shape[1]-nf), 2) # just print completion rate\n",
    "            print(\"\\t[\"+'#'*(int(update/5))+' '*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "            \n",
    "        estimator.fit(X=tr_data[:,train_feat], y=tr_vals, **parameters_dict)\n",
    "        pred = estimator.predict(ts_data)\n",
    "        rem = Regression_evaluationMetric(ts_vals, pred)\n",
    "        accuracies.append(rem.rootMeanSquareError())\n",
    "            \n",
    "        rank = estimator.sort_featureImportances()\n",
    "        toDiscard_idx = rank[-1][0]\n",
    "        train_feat.remove(train_feat[toDiscard_idx])\n",
    "\n",
    "        d -= 1\n",
    "    \n",
    "    return (accuracies, train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[                    ] 0.0%\n",
      "\t[                   ] 1.9607843137254901%\n",
      "\t[                   ] 3.9215686274509802%\n",
      "\t[#                  ] 5.882352941176471%\n",
      "\t[#                  ] 7.8431372549019605%\n",
      "\t[#                  ] 9.803921568627452%\n",
      "\t[##                 ] 11.764705882352942%\n",
      "\t[##                 ] 13.72549019607843%\n",
      "\t[###                ] 15.686274509803921%\n",
      "\t[###                ] 17.647058823529413%\n",
      "\t[###                ] 19.607843137254903%\n",
      "\t[####               ] 21.568627450980394%\n",
      "\t[####               ] 23.529411764705884%\n",
      "\t[#####              ] 25.49019607843137%\n",
      "\t[#####              ] 27.45098039215686%\n",
      "\t[#####              ] 29.41176470588235%\n",
      "\t[######             ] 31.372549019607842%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[#######            ] 35.294117647058826%\n",
      "\t[#######            ] 37.254901960784316%\n",
      "\t[#######            ] 39.21568627450981%\n",
      "\t[########           ] 41.1764705882353%\n",
      "\t[########           ] 43.13725490196079%\n",
      "\t[#########          ] 45.09803921568628%\n",
      "\t[#########          ] 47.05882352941177%\n",
      "\t[#########          ] 49.01960784313726%\n",
      "\t[##########         ] 50.98039215686274%\n",
      "\t[##########         ] 52.94117647058823%\n",
      "\t[##########         ] 54.90196078431372%\n",
      "\t[###########        ] 56.86274509803921%\n",
      "\t[###########        ] 58.8235294117647%\n",
      "\t[############       ] 60.78431372549019%\n",
      "\t[############       ] 62.745098039215684%\n",
      "\t[############       ] 64.70588235294117%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[#############      ] 68.62745098039215%\n",
      "\t[##############     ] 70.58823529411765%\n",
      "\t[##############     ] 72.54901960784314%\n",
      "\t[##############     ] 74.50980392156863%\n",
      "\t[###############    ] 76.47058823529412%\n",
      "\t[###############    ] 78.43137254901961%\n",
      "\t[################   ] 80.3921568627451%\n",
      "\t[################   ] 82.3529411764706%\n",
      "\t[################   ] 84.31372549019608%\n",
      "\t[#################  ] 86.27450980392157%\n",
      "\t[#################  ] 88.23529411764706%\n",
      "\t[################## ] 90.19607843137256%\n",
      "\t[################## ] 92.15686274509804%\n",
      "\t[################## ] 94.11764705882354%\n",
      "\t[###################] 96.07843137254902%\n",
      "\t[###################] 98.03921568627452%\n"
     ]
    }
   ],
   "source": [
    "p_dict = {\"depth\":200, \"minElems_perLeaf\":5, \"post_pruning\":True, \"verbose\":False}\n",
    "acc, surv_feature = rfe(NumericalRandomForest_regressor(100), \n",
    "                        p_dict, trainVal_data, trainVal_values, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "x = range(trainVal_data.shape[1],len(surv_feature),-1)\n",
    "plt.plot(x, acc)\n",
    "plt.xlim(trainVal_data.shape[1], len(surv_feature))\n",
    "#range(trainVal_data.shape[1],len(surv_feature),-1)\n",
    "plt.xlabel(\"\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09799842559913774,\n",
       " 0.10118422846576454,\n",
       " 0.10675588212230948,\n",
       " 0.3578045637546183,\n",
       " 0.42871972936785474,\n",
       " 0.4344407716031294,\n",
       " 0.40735678723037877,\n",
       " 0.3149620396649444,\n",
       " 0.4234442793652079,\n",
       " 0.5285179535535334,\n",
       " 0.5033563809044974,\n",
       " 0.529257905151419,\n",
       " 0.6501229276800635,\n",
       " 0.5778064746455581,\n",
       " 0.5189102008546668,\n",
       " 0.6313167233045371,\n",
       " 0.598637972280999,\n",
       " 0.6329146949513154,\n",
       " 0.4827892786891847,\n",
       " 0.5459299985792916,\n",
       " 0.5510831525313181,\n",
       " 0.5146098504389461,\n",
       " 0.5414997757717673,\n",
       " 0.5876058957695781,\n",
       " 0.7239666935480839,\n",
       " 0.7114154320105924,\n",
       " 0.6897679521112138,\n",
       " 0.6777023147281463,\n",
       " 0.6976351749292563,\n",
       " 0.6616408237821788,\n",
       " 0.6711709613776295,\n",
       " 0.7304576653257334,\n",
       " 0.7184425210454861,\n",
       " 0.607138026255972,\n",
       " 0.6928778357069496,\n",
       " 0.6179849478785793,\n",
       " 0.7396443051973854,\n",
       " 0.61667434904054,\n",
       " 0.5779917778616999,\n",
       " 0.5293469944396975,\n",
       " 0.5763175583851204,\n",
       " 0.6439721543494762,\n",
       " 0.6569083004537558,\n",
       " 0.5949857398911372,\n",
       " 0.6017128016375768,\n",
       " 0.6310190008440479,\n",
       " 0.5854352721912811,\n",
       " 0.7192331074927207,\n",
       " 0.7205639348559967,\n",
       " 0.6105221663158675,\n",
       " 0.7328934555880128]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['population', 'racepctblack', 'racePctWhite', 'racePctHisp',\n",
       "       'agePct12t21', 'agePct16t24', 'numbUrban', 'pctWInvInc', 'pctWSocSec',\n",
       "       'pctWPubAsst', 'pctWRetire', 'perCapInc', 'whitePerCap', 'blackPerCap',\n",
       "       'indianPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade',\n",
       "       'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctOccupMgmtProf',\n",
       "       'MalePctDivorce', 'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv',\n",
       "       'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par',\n",
       "       'PctTeen2Par', 'NumKidsBornNeverMar', 'PctKidsBornNeverMar',\n",
       "       'PctRecImmig5', 'PctRecImmig8', 'PersPerRentOccHous',\n",
       "       'PctPersDenseHous', 'PctHousLess3BR', 'HousVacant', 'PctHousOwnOcc',\n",
       "       'PctVacantBoarded', 'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccQrange',\n",
       "       'MedRent', 'MedOwnCostPctInc', 'NumInShelters', 'PctForeignBorn',\n",
       "       'PctBornSameState', 'PopDens', 'PctUsePubTrans'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_features.columns[surv_feature]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
