{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cross validation purposes: create the cartesian product between the chosen values sets\n",
    "from itertools import product \n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    # for each column that contain at least 1 NaN value...\n",
    "    for nanCol in np.unique(np.where(pred_features.isna())[1]):\n",
    "        nanRows = np.where(pred_features.iloc[:,nanCol].isna())[0] #find NaN rows for the current column\n",
    "        available = df.iloc[~nanRows, nanCol]\n",
    "        value = value_withStrategy(available, strategy) #compute the filling value\n",
    "        df.iloc[nanRows, nanCol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- TBD <br>\n",
    "A thourough study from scratch of outliers detection is needed here, but for now it feels like it exceeds the course final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values rows on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_naSample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = \"robbPerPop\"\n",
    "data,values = drop_naSample(pred_features, regr_values[dep_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTest_split(in_matrix, out_vect, train_amount=0.7):\n",
    "    n = in_matrix.shape[0]\n",
    "\n",
    "    trVl_Amount = int(n*train_amount) #training-validation amount\n",
    "    indexes = np.random.permutation(n)\n",
    "    idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "    idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "    return in_matrix[idxTrVl], in_matrix[idxTs], out_vect[idxTrVl], out_vect[idxTs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal_data, test_data, trainVal_values, test_values = trainTest_split(data, values, train_amount=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_evaluationMetric:\n",
    "    def __init__(self, true, predicted):\n",
    "        self.true = true.flatten()\n",
    "        self.predicted = predicted.flatten()\n",
    "        self.residuals = self.true-self.predicted\n",
    "    \n",
    "    def meanSquareError(self):\n",
    "        return np.mean(np.square(self.residuals))\n",
    "    \n",
    "    def rootMeanSquareError(self):\n",
    "        return np.sqrt(np.mean(np.square(self.residuals)))\n",
    "    \n",
    "    def meanAbsoluteError(self):\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def rSquared(self):\n",
    "        ss_residual = np.sum(np.square(self.residuals))\n",
    "        ss_total = np.sum(np.square(self.true-np.mean(self.true)))        \n",
    "        return 1 - ss_residual/ss_total\n",
    "    \n",
    "    def adjusted_rSquared(self, p):\n",
    "        n = self.true.shape[0]\n",
    "        return 1-(1-self.rSquared)*((n-1)/(n-p-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def kFold_crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor, verbose=False):\n",
    "    nVal = train_data.shape[0]\n",
    "    \n",
    "    # Validation indexes adjustment -------------------------------\n",
    "    elemPerFold, remainder = np.divmod(nVal,k) #the remainder will be distributed across the firsts folds\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    # in each fold put as many samples as the division quotient +1 if the remainder is still positive\n",
    "    # then decrease the division remainder by 1\n",
    "    for i in range(k): \n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation --------------------------------------------\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values())) # build all the hyp-par combination\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList): # for each fold\n",
    "        if verbose: print(\"#{} fold:\".format(row+1))\n",
    "        for col, params in enumerate(params_product):\n",
    "            \n",
    "            if verbose:\n",
    "                update = col*100/len(params_product) # just print completion rate\n",
    "                print(\"\\t[\"+\"#\"*(int(update/5))+\" \"*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)} # {argument_name:argument_value, ... }\n",
    "            \n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            rem = Regression_evaluationMetric(trainVal_values[valIdx], pred)\n",
    "            #val_results[row,col] = rem.rSquared()\n",
    "            val_results[row,col] = rem.rootMeanSquareError()\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            # project each column on the current residuals\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            # find the most correlated variable\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "            \n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance # update the solution vector: canonical basis over the found column\n",
    "            residual -= np.matmul(data_matrix, self.weights) # update the residual\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "np.where(mp.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 5.269389430468309e+27\n",
      "Root Mean Square Error: 4627129228976257.0\n",
      "R^2 score: -5.31151267218903e+32\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 34, 38, 50, 76, 77, 92, 93, 94])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06770615757006777\n",
      "Root Mean Square Error: 0.11066107282253887\n",
      "R^2 score: 0.696202135542545\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression: # Iterative Soft Thresholding Algorithm (Proximal Gradient)\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        self.weights = np.zeros((data_matrix.shape[1],1))\n",
    "        n = data_matrix.shape[0]\n",
    "        # convergence step-size: n/(2*||X^t*X||_2)\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # gradient step of the lasso formulation\n",
    "            dist = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            coord_descent = (step/n)*np.matmul(data_matrix.T, dist)\n",
    "            self.weights -= coord_descent\n",
    "\n",
    "            # soft thresholding operator\n",
    "            upper = self.weights > softTresh  # elem to be reduced\n",
    "            lower = self.weights < -softTresh # elem to be increased\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  27,  49,  51,  71,  91,  92,  98, 101])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=10)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.8)\n",
    "np.where(lr.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.04008533496126917\n",
      "Root Mean Square Error: 0.8508416746212191\n",
      "R^2 score: -16.959426805944137\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  69,  76,  93,  94, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.060300291599253035\n",
      "Root Mean Square Error: 0.11399559259872948\n",
      "R^2 score: 0.6776177772388571\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor:\n",
    "    class Node:\n",
    "        def __init__(self, value, isLeaf=False, feature=None, left=None, right=None):\n",
    "            self.value = value\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.value))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            print(\"|    \"*level+\"{} => {}\".format(self.feature, self.value))\n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElem_perLeaf):\n",
    "        self.root = self.learn(X, y, depth, minElem_perLeaf)\n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElem_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElem_perLeaf: # leaf\n",
    "            return self.Node(value=np.mean(y), isLeaf=True)\n",
    "            \n",
    "        best_costDescent = 0 # split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "                \n",
    "                if sorted_x[i2]<sorted_x[i2+1]: # for a different value\n",
    "                    # try to maximise this value: it is directly correlated \n",
    "                    # to the possible split information gain\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "\n",
    "        return self.Node(value=best_cut, feature=best_feature,\n",
    "                        left=self.learn(X[left_idxs],y[left_idxs],depth-1,minElem_perLeaf),\n",
    "                        right=self.learn(X[~left_idxs],y[~left_idxs],depth-1,minElem_perLeaf))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.value:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.value\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    |    None => -0.5836556302960617\n",
      "|    |    |    |    3 => 0.3350180505415161\n",
      "|    |    |    |    |    None => -0.24188477314843734\n",
      "|    |    |    28 => -0.6141910739191074\n",
      "|    |    |    |    |    None => -0.6017587329349463\n",
      "|    |    |    |    17 => -0.6424731182795699\n",
      "|    |    |    |    |    None => -0.8004590431350379\n",
      "|    |    44 => -0.3463920301561659\n",
      "|    |    |    |    None => 0.5455826299726605\n",
      "|    |    |    30 => 0.0983379501385041\n",
      "|    |    |    |    None => 0.014806570294108479\n",
      "|    100 => -0.7708448371065709\n",
      "|    |    |    |    |    None => -0.5656053318493196\n",
      "|    |    |    |    71 => -0.9649109603316119\n",
      "|    |    |    |    |    None => -0.7601109476929329\n",
      "|    |    |    3 => 0.42331098504383713\n",
      "|    |    |    |    |    None => -0.4068260308712821\n",
      "|    |    |    |    40 => 0.15751093825960127\n",
      "|    |    |    |    |    None => -0.5829119624484732\n",
      "|    |    100 => -0.9431253451131971\n",
      "|    |    |    |    |    None => -0.6456106630751042\n",
      "|    |    |    |    85 => -0.4822954822954823\n",
      "|    |    |    |    |    None => -0.7589761936067141\n",
      "|    |    |    91 => -0.9992729760937433\n",
      "|    |    |    |    |    None => -0.8924805554451377\n",
      "|    |    |    |    44 => 0.00901992460958534\n",
      "|    |    |    |    |    None => -0.7719006859146781\n",
      "50 => -0.6515539305301645\n",
      "|    |    |    |    |    None => -0.8363592876054617\n",
      "|    |    |    |    69 => 0.21261378413524046\n",
      "|    |    |    |    |    None => -0.915151549567418\n",
      "|    |    |    3 => 0.7003610108303251\n",
      "|    |    |    |    |    None => -0.8653757137417455\n",
      "|    |    |    |    14 => -0.7212863705972434\n",
      "|    |    |    |    |    None => -0.7775232697255726\n",
      "|    |    6 => -0.8641107988759534\n",
      "|    |    |    None => -0.10712135198361705\n",
      "|    49 => -0.9981840824782915\n",
      "|    |    |    |    |    None => -0.9173508495304459\n",
      "|    |    |    |    90 => -0.8571428571428571\n",
      "|    |    |    |    |    None => -0.7250864570497277\n",
      "|    |    |    34 => -0.6932055022926219\n",
      "|    |    |    |    None => -0.621444881698489\n",
      "|    |    59 => -0.46674839105117993\n",
      "|    |    |    |    |    None => -0.7465820818101827\n",
      "|    |    |    |    38 => 0.11866295264623972\n",
      "|    |    |    |    |    None => -0.917997856345036\n",
      "|    |    |    2 => -0.8993482983345402\n",
      "|    |    |    |    |    None => -0.9368937674578252\n",
      "|    |    |    |    40 => 0.003403014098201318\n",
      "|    |    |    |    |    None => -0.9716250229985908\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElem_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.0768367339889725\n",
      "Root Mean Square Error: 0.12163078362311287\n",
      "R^2 score: 0.6329865554970941\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50, 100,  44,   3,  49,  28,   6,  30,  59,  69,  40,  18,   2,\n",
       "        34,  95,  54,  75,  71,  36,  33,  14,  90,  91,  38,  26,  45,\n",
       "         4,  72,   0,  53,  43,  24,   8,  85,  16,  74,  99,  23,   1,\n",
       "       101,  21,  39,  97,  42,  68,  58,  57,  87,  51,  94,  79,  92,\n",
       "        89,  41,   5,  63,  32,  65,  98,  82,  93,  84,  66,  77,  86,\n",
       "         7,  12,  48,  37,  62,   9,  76,  25,  22,  29,  61,  17,  11,\n",
       "        78,  67,  47,  27,  15,  56,  83,  96,  73,  60,  88,  81,  20,\n",
       "        46,  52,  10,  35,  55,  19,  13,  31,  64,  80,  70])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.08211697629952187\n",
      "Root Mean Square Error: 0.1330305062716748\n",
      "R^2 score: 0.5609667102365496\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalRandomForest_regressor:\n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.boot_samplesIdxs = []\n",
    "        self.oob_error = None\n",
    "        \n",
    "    def fit(self, X, y, depths, minElems_perLeaf):\n",
    "        n = X.shape[0]\n",
    "        n_learn = int(n/3) # Bootstrap amount to be taken aside\n",
    "        \n",
    "        val_folds = 3\n",
    "        params_dict = {\"depth\":depths, \"minElem_perLeaf\":minElems_perLeaf}\n",
    "        \n",
    "        # Fitting the forest -----------------------------------\n",
    "        for i in range(self.n_trees):\n",
    "            print(\"Fitting #{} tree\".format(i+1))\n",
    "            \n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            self.boot_samplesIdxs.append(bootstrap_idxs)\n",
    "                        \n",
    "            dt = NumericalDecisionTree_regressor()\n",
    "            # find the best hyp-par for the current setting (bootstrapping)\n",
    "            win_params = kFold_crossValidation_selectionGrid(val_folds, params_dict, \n",
    "                                                             X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "                                                             dt, verbose=True)\n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=win_params[0], minElem_perLeaf=win_params[1]))\n",
    "        \n",
    "        # Out-Of-Bag Estimate for the forest -------------------\n",
    "        oob_errors = []\n",
    "        for sampleIdx in range(n):\n",
    "            missingBoot_TreesIdx = [idx for idx,bootstrap_idxs in enumerate(self.boot_samplesIdxs) \n",
    "                                    if sampleIdx not in bootstrap_idxs]\n",
    "\n",
    "            if len(missingBoot_TreesIdx) == 0: continue\n",
    "            \n",
    "            regr_results = np.empty(len(missingBoot_TreesIdx)) # regression estimate of the selected trees\n",
    "            for i, missing_tree in enumerate(missingBoot_TreesIdx):\n",
    "                # reshape in order to correctly use the decision_tree.predict(...): it needs a matrix (num,dim)\n",
    "                # while numpy matrix indexing returns (dim,)\n",
    "                regr_results[i] = self.trees[missing_tree].predict(X[sampleIdx].reshape(1,-1))\n",
    "            \n",
    "            # done at this level of granularity because a sample might end up in \n",
    "            # being part of no bootstrap set of any tree (so we cannot predict wich value in y will be used)\n",
    "            oob_errors.append(np.square(y[sampleIdx]-np.mean(regr_results)))\n",
    "            #oob_errors.append(r2_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            #oob_errors.append(explained_variance_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            \n",
    "        self.oob_error = np.sqrt(np.mean(oob_errors))\n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, tree in enumerate(self.trees):\n",
    "            results[row] = tree.predict(X)\n",
    "            \n",
    "        return np.mean(results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "Fitting #2 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "Fitting #3 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.012484787097209819"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(3)\n",
    "# no train and test, cause it's a forest\n",
    "nrf.fit(data,values,depths=[10,20],minElems_perLeaf=[20,30]);\n",
    "nrf.oob_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/.conda/envs/bcb/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 50,  49, 100,   3,  69,  68,  99,   2,  44,  40,  89,  93,  65,\n",
       "        51,  62,  41,  46,  71,  59,  43,  37,  64,  27,  13,  58,  10,\n",
       "        98,  95,  38,  20,  92,  74,  88,  15,  24,  26,  72,  94,  96,\n",
       "        91,  61,   9,   8,  14, 101,  90,  53,  82,  79,  18,  17,  52,\n",
       "        55,  36,  23,  16,  73,  78,  34,   1,   0,  33,   4,  45,   5,\n",
       "         6,  67,  22,  28,  66,   7,  25,  32,  30,  11,  39,  35,  47,\n",
       "        48,  97,  19,  63,  75,  57,  76,  29,  87,  86,  77,  31,  21,\n",
       "        60,  83,  42,  84,  54,  85,  80,  81,  12,  56,  70])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06984967636427451\n",
      "Root Mean Square Error: 0.09488427275252621\n",
      "R^2 score: 0.7766512991217931\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.010547880708005426\n",
      "Root Mean Square Error: 0.10270376665375863\n",
      "R^2 score: 0.7383217013912104\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[2,2.05,2.1,2.2,3]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #2 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #3 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #4 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #5 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Residual variance: 0.06723570485101438\n",
      "Root Mean Square Error: 0.10805473249461588\n",
      "R^2 score: 0.710343986511752\n"
     ]
    }
   ],
   "source": [
    "rf = NumericalRandomForest_regressor(5)\n",
    "rf.fit(trainVal_data, trainVal_values, depths=[10,20,30], minElems_perLeaf=[20,30,40]);\n",
    "\n",
    "pred = rf.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012694196598918848"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_SupportVector_regression:\n",
    "    def __init__(self, weight=None, alpha=None, bias=None):\n",
    "        self.x = alpha\n",
    "        self.w = weight\n",
    "        self.bias = bias\n",
    "        self.Nabla = None\n",
    "                \n",
    "    def SMO2_ab(self, n, H, f, a, LB, UB, maxiter, eps, alpha_s):\n",
    "        \"\"\"\n",
    "        % min_{x} .5 x H x + f' x \n",
    "        %         LB <= x <= UB\n",
    "        %         a' x = b\n",
    "        % n         grandezza problema length(x)\n",
    "        % maxiter   max num it\n",
    "        % eps       precisione\n",
    "        % alpha_s   punto di inizio valido per x\n",
    "        % Nabla     ....\n",
    "        % err       flag di ok\n",
    "        % x         valore della soluzione ottima\n",
    "        % bias      ....\n",
    "        \"\"\"\n",
    "        self.x = alpha_s\n",
    "        self.Nabla = f\n",
    "        for i in range(n):\n",
    "            if (self.x[i] != 0.0):\n",
    "                for j in range(n):\n",
    "                    self.Nabla[j] += H[j,i] * self.x[i]\n",
    "        iter_ = 0\n",
    "        while True:\n",
    "            minF_up = float(\"inf\");\n",
    "            maxF_low = float(\"-inf\");\n",
    "            for i in range(n): \n",
    "                F_i = self.Nabla[i]/a[i]\n",
    "                if (LB[i] < self.x[i]) and (self.x[i] < UB[i]) :\n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == LB[i])) or ((a[i] < 0) and (self.x[i] == UB[i]))) : \n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == UB[i])) or ((a[i] < 0) and (self.x[i] == LB[i]))) : \n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "            if ((maxF_low - minF_up) <= eps):\n",
    "                err = 0.0\n",
    "                break\n",
    "\n",
    "            iter_ += 1\n",
    "            if (iter_ >= maxiter):\n",
    "                err = 1.0\n",
    "                break\n",
    "\n",
    "            if (a[u] > 0):\n",
    "                tau_lb = (LB[u]-self.x[u])*a[u] \n",
    "                tau_ub = (UB[u]-self.x[u])*a[u] \n",
    "            else:\n",
    "                tau_ub = (LB[u]-self.x[u])*a[u] \n",
    "                tau_lb = (UB[u]-self.x[u])*a[u]\n",
    "\n",
    "            if (a[v] > 0):\n",
    "                tau_lb = max(tau_lb,(self.x[v]-UB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-LB[v])*a[v]) \n",
    "            else:\n",
    "                tau_lb = max(tau_lb,(self.x[v]-LB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-UB[v])*a[v])\n",
    "\n",
    "            tau = (self.Nabla[v]/a[v]-self.Nabla[u]/a[u])/(H[u,u]/(a[u]*a[u])\n",
    "                                                           +H[v,v]/(a[v]*a[v])\n",
    "                                                           -2*H[v,u]/(a[u]*a[v]))\n",
    "            tau = min(max(tau,tau_lb),tau_ub)\n",
    "            self.x[u] += tau/a[u]\n",
    "            self.x[v] -= tau/a[v]\n",
    "\n",
    "            for i in range(n):\n",
    "                self.Nabla[i] += H[u,i]*tau/a[u] - H[v,i]*tau/a[v]\n",
    "\n",
    "        tsv = 0\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for k in range(n):\n",
    "            if ((self.x[k] > LB[k]) and (self.x[k] < UB[k])):\n",
    "                self.bias -= self.Nabla[k]/a[k]\n",
    "                tsv += 1\n",
    "\n",
    "        if (tsv > 0):\n",
    "            self.bias /= tsv\n",
    "        else:    \n",
    "            self.bias = -(maxF_low + minF_up)/2.0\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def fit(self, X, y, C):\n",
    "        n = X.shape[0]\n",
    "        cov = np.matmul(X, X.T)\n",
    "        Q = np.matmul(np.matmul(np.diag(y.flatten()), cov),\n",
    "                      np.diag(y.flatten()))\n",
    "        \n",
    "        if self.SMO2_ab(n,Q,-np.ones(n),y.flatten(),\n",
    "                   np.zeros(n),C*np.ones(n),10000000,.0001,np.zeros(n)):\n",
    "            print(\"Problem in SMO\")\n",
    "            \n",
    "        self.w = np.matmul(np.matmul(X.T, np.diag(y.flatten())),\n",
    "                           self.x)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.matmul(X, self.w) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 4.180784258259315\n",
      "Root Mean Square Error: 8.229964539864008\n",
      "R^2 score: -1679.3157268395166\n"
     ]
    }
   ],
   "source": [
    "lsvr = linear_SupportVector_regression()\n",
    "lsvr.fit(trainVal_data, trainVal_values, C=1.0);\n",
    "\n",
    "pred = lsvr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   7,   13,   39,   74,   95,  110,  118,  125,  127,  132,  251,\n",
       "         253,  268,  306,  328,  399,  402,  435,  469,  506,  516,  558,\n",
       "         575,  590,  608,  645,  659,  721,  922,  933, 1001, 1050, 1090,\n",
       "        1121, 1205, 1212, 1223, 1275, 1316, 1431, 1444, 1468, 1485]),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lsvr.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel=\"linear\", tol=.0001, C=1)\n",
    "svr.fit(trainVal_data, trainVal_values.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(svr.dual_coef_)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07142833734841693\n",
      "Root Mean Square Error: 0.10400869315939919\n",
      "R^2 score: 0.7316298286755705\n"
     ]
    }
   ],
   "source": [
    "pred = svr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
