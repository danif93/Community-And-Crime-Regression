{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "# cross validation purposes: create the cartesian product between the chosen values sets\n",
    "from itertools import product \n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    # for each column that contain at least 1 NaN value...\n",
    "    for nanCol in np.unique(np.where(pred_features.isna())[1]):\n",
    "        nanRows = np.where(pred_features.iloc[:,nanCol].isna())[0] #find NaN rows for the current column\n",
    "        available = df.iloc[~nanRows, nanCol]\n",
    "        value = value_withStrategy(available, strategy) #compute the filling value\n",
    "        df.iloc[nanRows, nanCol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,values = drop_sample(pred_features, regr_values[\"robbPerPop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTest_split(in_matrix, out_vect, train_amount=0.7):\n",
    "    n = in_matrix.shape[0]\n",
    "\n",
    "    trVl_Amount = int(n*train_amount) #training-validation amount\n",
    "    indexes = np.random.permutation(n)\n",
    "    idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "    idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "    return in_matrix[idxTrVl], in_matrix[idxTs], out_vect[idxTrVl], out_vect[idxTs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal_data, test_data, trainVal_values, test_values = trainTest_split(data, values, train_amount=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def kFold_crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor, verbose=False):\n",
    "    nVal = train_data.shape[0]\n",
    "    \n",
    "    # Validation indexes adjustment -------------------------------\n",
    "    elemPerFold, remainder = np.divmod(nVal,k) #the remainder will be distributed across the firsts folds\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    # in each fold put as many samples as the division quotient +1 if the remainder is still positive\n",
    "    # then decrease the division remainder by 1\n",
    "    for i in range(k): \n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation --------------------------------------------\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values())) # build all the hyp-par combination\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList): # for each fold\n",
    "        if verbose: print(\"#{} fold:\".format(row+1))\n",
    "        for col, params in enumerate(params_product):\n",
    "            \n",
    "            if verbose:\n",
    "                update = col*100/len(params_product) # just print completion rate\n",
    "                print(\"\\t[\"+\"#\"*(int(update/5))+\" \"*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)} # {argument_name:argument_value, ... }\n",
    "            \n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            #val_results[row,col] = r2_score(trainVal_values[valIdx],pred)\n",
    "            #val_results[row,col] = explained_variance_score(trainVal_values[valIdx],pred)\n",
    "            val_results[row,col] = np.mean(np.square(train_values[valIdx]-pred)) # MSE\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            # project each column on the current residuals\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            # find the most correlated variable\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "            \n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance # update the solution vector: canonical basis over the found column\n",
    "            residual -= np.matmul(data_matrix, self.weights) # update the residual\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "np.where(mp.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 2.1477284527146093e+31\n",
      "R^2 score: -5.2554165574405774e+32\n",
      "Explained Variance Score: -4.480652353657181e+29\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 34, 38, 50, 76, 77, 92, 93, 94])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.06847062493726377\n",
      "R^2 score: 0.6652543463899951\n",
      "Explained Variance Score: 0.6654209600448748\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression: # Iterative Soft Thresholding Algorithm (Proximal Gradient)\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        self.weights = np.zeros((data_matrix.shape[1],1))\n",
    "        n = data_matrix.shape[0]\n",
    "        # convergence step-size: n/(2*||X^t*X||_2)\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # gradient step of the lasso formulation\n",
    "            dist = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            coord_descent = (step/n)*np.matmul(data_matrix.T, dist)\n",
    "            self.weights -= coord_descent\n",
    "\n",
    "            # soft thresholding operator\n",
    "            upper = self.weights > softTresh  # elem to be reduced\n",
    "            lower = self.weights < -softTresh # elem to be increased\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  27,  49,  51,  71,  91,  92,  98, 101])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=10)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.8)\n",
    "np.where(lr.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.7262956170196937\n",
      "R^2 score: -16.772200235357317\n",
      "Explained Variance Score: 0.005051931736064108\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  76,  93,  94, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.06121756805249363\n",
      "R^2 score: 0.6328578269018734\n",
      "Explained Variance Score: 0.6335227522165594\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor:\n",
    "    class Node:\n",
    "        def __init__(self, value, isLeaf=False, feature=None, left=None, right=None):\n",
    "            self.value = value\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.value))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            print(\"|    \"*level+\"{} => {}\".format(self.feature, self.value))\n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElem_perLeaf):\n",
    "        self.root = self.learn(X, y, depth, minElem_perLeaf)\n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElem_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElem_perLeaf: # leaf\n",
    "            return self.Node(value=np.mean(y), isLeaf=True)\n",
    "            \n",
    "        best_costDescent = 0 # split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "                \n",
    "                if sorted_x[i2]<sorted_x[i2+1]: # for a different value\n",
    "                    # try to maximise this value: it is directly correlated \n",
    "                    # to the possible split information gain\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "\n",
    "        return self.Node(value=best_cut, feature=best_feature,\n",
    "                        left=self.learn(X[left_idxs],y[left_idxs],depth-1,minElem_perLeaf),\n",
    "                        right=self.learn(X[~left_idxs],y[~left_idxs],depth-1,minElem_perLeaf))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.value:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.value\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    None => 0.8259154730514591\n",
      "|    |    |    61 => -0.45812679363422903\n",
      "|    |    |    |    |    None => 0.29717059406609014\n",
      "|    |    |    |    100 => -0.3042517945886251\n",
      "|    |    |    |    |    None => -0.08725058484568764\n",
      "|    |    50 => -0.09104204753199274\n",
      "|    |    |    |    |    None => 0.03270277472288843\n",
      "|    |    |    |    5 => -0.7431963854155721\n",
      "|    |    |    |    |    None => -0.2864708046168475\n",
      "|    |    |    74 => -0.809726748558536\n",
      "|    |    |    |    None => -0.6061568902845685\n",
      "|    100 => -0.5549420209828824\n",
      "|    |    |    |    |    None => -0.08184993303780658\n",
      "|    |    |    |    89 => -0.16042780748663116\n",
      "|    |    |    |    |    None => -0.441234339354147\n",
      "|    |    |    40 => 0.3033543996110841\n",
      "|    |    |    |    |    None => -0.49114670977373215\n",
      "|    |    |    |    61 => -0.9668666840594834\n",
      "|    |    |    |    |    None => -0.6937619053928631\n",
      "|    |    50 => -0.48957952468007315\n",
      "|    |    |    |    |    None => -0.7156861446066415\n",
      "|    |    |    |    95 => 0.07372995654128578\n",
      "|    |    |    |    |    None => -0.5838985869593653\n",
      "|    |    |    50 => -0.7243144424131627\n",
      "|    |    |    |    |    None => -0.7201936284577298\n",
      "|    |    |    |    85 => -0.05250305250305254\n",
      "|    |    |    |    |    None => -0.8342618457099517\n",
      "49 => -0.9917070572468947\n",
      "|    |    |    |    |    None => -0.9207477044162659\n",
      "|    |    |    |    11 => 0.9989\n",
      "|    |    |    |    |    None => -0.9663549601012105\n",
      "|    |    |    41 => -0.2118556701030927\n",
      "|    |    |    |    |    None => -0.9668954280923332\n",
      "|    |    |    |    11 => 0.7828\n",
      "|    |    |    |    |    None => -0.9847747949210428\n",
      "|    |    3 => 0.8752965446106241\n",
      "|    |    |    |    None => -0.373763873982501\n",
      "|    |    |    4 => -0.6031690753961344\n",
      "|    |    |    |    |    None => -0.9007877878945147\n",
      "|    |    |    |    49 => -0.9991261607750442\n",
      "|    |    |    |    |    None => -0.9516510749169652\n",
      "|    3 => 0.6783909231562661\n",
      "|    |    |    |    |    None => -0.7989818665709664\n",
      "|    |    |    |    98 => -0.996749698786741\n",
      "|    |    |    |    |    None => -0.6533658681956843\n",
      "|    |    |    14 => -0.9433384379785605\n",
      "|    |    |    |    None => -0.3813230689050541\n",
      "|    |    49 => -0.9976552296718648\n",
      "|    |    |    |    |    None => -0.9001979005020774\n",
      "|    |    |    |    35 => -0.7943682845498332\n",
      "|    |    |    |    |    None => -0.6569256182286353\n",
      "|    |    |    98 => -0.9984589089075065\n",
      "|    |    |    |    |    None => -0.7996000986818652\n",
      "|    |    |    |    0 => -0.9999286159605687\n",
      "|    |    |    |    |    None => -0.14994280363760026\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElem_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.0638563134391556\n",
      "R^2 score: 0.5709126687760459\n",
      "Explained Variance Score: 0.5715468520381957\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49, 100,   3,  50,  14,  40,  89,  29,  98,  74,  41,  61,  88,\n",
       "         0,  36,  68,  18,  79,  95,  99,   4,  35,   9,  45,  92,  72,\n",
       "        15,  38,  12,  39,  24,   2,  78,  77,  19,  52,  62,  30,  66,\n",
       "        85,  33,  34,  25,  73,  23,  11,  90,   6,  55,  94,  69,  76,\n",
       "         1,  83,  46,  53,  44,  93,  57,  27,  42,  48,  16,  71,   5,\n",
       "        64,  58,  13,  28,  22,   8,  75,  51,  20,  32,  17,  43,  96,\n",
       "        70,  84,  86,  81,  37,  59,  67,  26,  80,  47,   7,  54,  97,\n",
       "        65,  63,  87,  21,  31,  10,  91,  82,  56, 101,  60])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07299162980474981\n",
      "R^2 score: 0.5520523728524112\n",
      "Explained Variance Score: 0.5521896252959912\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests\n",
    "\n",
    "class NumericalRandomForest_regressor:\n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.boot_samplesIdxs = []\n",
    "        self.oob_error = None\n",
    "        \n",
    "    def fit(self, X, y, depths, minElems_perLeaf):\n",
    "        n = X.shape[0]\n",
    "        n_learn = int(n/3) # Bootstrap amount to be taken aside\n",
    "        \n",
    "        val_folds = 3\n",
    "        params_dict = {\"depth\":depths, \"minElem_perLeaf\":minElems_perLeaf}\n",
    "        \n",
    "        # Fitting the forest -----------------------------------\n",
    "        for i in range(self.n_trees):\n",
    "            print(\"Fitting #{} tree\".format(i+1))\n",
    "            \n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            self.boot_samplesIdxs.append(bootstrap_idxs)\n",
    "                        \n",
    "            dt = NumericalDecisionTree_regressor()\n",
    "            # find the best hyp-par for the current setting (bootstrapping)\n",
    "            win_params = kFold_crossValidation_selectionGrid(val_folds, params_dict, \n",
    "                                                             X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "                                                             dt, verbose=True)\n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=win_params[0], minElem_perLeaf=win_params[1]))\n",
    "        \n",
    "        # Out-Of-Bag Estimate for the forest -------------------\n",
    "        oob_errors = []\n",
    "        for sampleIdx in range(n):\n",
    "            missingBoot_TreesIdx = [idx for idx,bootstrap_idxs in enumerate(self.boot_samplesIdxs) \n",
    "                                    if sampleIdx not in bootstrap_idxs]\n",
    "\n",
    "            if len(missingBoot_TreesIdx) == 0: continue\n",
    "            \n",
    "            regr_results = np.empty(len(missingBoot_TreesIdx)) # regression estimate of the selected trees\n",
    "            for i, missing_tree in enumerate(missingBoot_TreesIdx):\n",
    "                regr_results[i] = self.trees[missing_tree].predict(X[sampleIdx].reshape(1,-1))\n",
    "            \n",
    "            # done at this level of granularity because a sample might end up in \n",
    "            # being part of no bootstrap set of any tree (so we cannot predict wich value in y will be used)\n",
    "            oob_errors.append(np.mean(np.square(np.mean(regr_results)-y[sampleIdx])))\n",
    "            #oob_errors.append(r2_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            #oob_errors.append(explained_variance_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            \n",
    "        self.oob_error = np.mean(oob_errors)\n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, tree in enumerate(self.trees):\n",
    "            results[row] = tree.predict(X)\n",
    "            \n",
    "        return np.mean(results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "Fitting #2 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "Fitting #3 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[#####               ] 25.0%\n",
      "\t[##########          ] 50.0%\n",
      "\t[###############     ] 75.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.011414943558457733"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(3)\n",
    "# no train and test, cause it's a forest\n",
    "nrf.fit(data,values,depths=[10,20],minElems_perLeaf=[20,30]);\n",
    "nrf.oob_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/.conda/envs/bcb/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 50,  49, 100,   3,  92,  69,  62,  51,  15,  38,   2,  14,  46,\n",
       "        99,  40,  63,  71,   4,  94,   0,  35,  39,  37,  28,  68,  82,\n",
       "        41,  44,  22,  67,  75,  43,  90,  18,  98,  20,  52,  47,   5,\n",
       "        86,  21,  88,   6,  64,  23,  58,  16,  24,  29,  60, 101,  73,\n",
       "         8,  76,  97,  95,  34,  78,  17,   9,  10,  25,  96,  59,  89,\n",
       "        32,  72,  45,  66,  79,  74,  48,  87,   7,  31,  33,  30,  83,\n",
       "        26,  61,  84,  93,  65,  36,  77,  27,  85,  13,  11,   1,  80,\n",
       "        19,  55,  54,  42,  53,  91,  56,  57,  12,  81,  70])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07115004850034741\n",
      "R^2 score: 0.7041212016715361\n",
      "Explained Variance Score: 0.7063583679107304\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.010942369403192055\n",
      "R^2 score: 0.7322440401323482\n",
      "Explained Variance Score: 0.7322662657251657\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[2,2.05,2.1,2.2,3]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #2 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #3 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #4 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Fitting #5 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "\t[##                 ] 11.11111111111111%\n",
      "\t[####               ] 22.22222222222222%\n",
      "\t[######             ] 33.333333333333336%\n",
      "\t[########           ] 44.44444444444444%\n",
      "\t[###########        ] 55.55555555555556%\n",
      "\t[#############      ] 66.66666666666667%\n",
      "\t[###############    ] 77.77777777777777%\n",
      "\t[#################  ] 88.88888888888889%\n",
      "Mean Square Error: 0.06321970785144537\n",
      "R^2 score: 0.6537923640090676\n",
      "Explained Variance Score: 0.6546955980416632\n"
     ]
    }
   ],
   "source": [
    "rf = NumericalRandomForest_regressor(5)\n",
    "rf.fit(trainVal_data, trainVal_values, depths=[10,20,30], minElems_perLeaf=[20,30,40]);\n",
    "\n",
    "pred = rf.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011831909775350221"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
