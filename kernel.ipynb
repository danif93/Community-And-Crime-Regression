{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cross validation purposes: create the cartesian product between the chosen values sets\n",
    "from itertools import product \n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    # for each column that contain at least 1 NaN value...\n",
    "    for nanCol in np.unique(np.where(pred_features.isna())[1]):\n",
    "        nanRows = np.where(pred_features.iloc[:,nanCol].isna())[0] #find NaN rows for the current column\n",
    "        available = df.iloc[~nanRows, nanCol]\n",
    "        value = value_withStrategy(available, strategy) #compute the filling value\n",
    "        df.iloc[nanRows, nanCol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- TBD <br>\n",
    "A thourough study from scratch of outliers detection is needed here, but for now it feels like it exceeds the course final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values rows on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_naSample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = \"robbPerPop\"\n",
    "data,values = drop_naSample(pred_features, regr_values[dep_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTest_split(in_matrix, out_vect, train_amount=0.7):\n",
    "    n,_ = in_matrix.shape\n",
    "\n",
    "    trVl_Amount = int(n*train_amount) #training-validation amount\n",
    "    indexes = np.random.permutation(n)\n",
    "    idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "    idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "    return in_matrix[idxTrVl], in_matrix[idxTs], out_vect[idxTrVl], out_vect[idxTs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal_data, test_data, trainVal_values, test_values = trainTest_split(data, values, train_amount=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_evaluationMetric:\n",
    "    def __init__(self, true, predicted):\n",
    "        self.true = true.flatten()\n",
    "        self.predicted = predicted.flatten()\n",
    "        self.residuals = self.true-self.predicted\n",
    "    \n",
    "    def meanSquareError(self):\n",
    "        return np.mean(np.square(self.residuals))\n",
    "    \n",
    "    def rootMeanSquareError(self):\n",
    "        return np.sqrt(np.mean(np.square(self.residuals)))\n",
    "    \n",
    "    def meanAbsoluteError(self):\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def rSquared(self):\n",
    "        ss_residual = np.sum(np.square(self.residuals))\n",
    "        ss_total = np.sum(np.square(self.true-np.mean(self.true)))        \n",
    "        return 1 - ss_residual/ss_total\n",
    "    \n",
    "    def adjusted_rSquared(self, p):\n",
    "        n = self.true.shape[0]\n",
    "        return 1-(1-self.rSquared)*((n-1)/(n-p-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def kFold_crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor, verbose=False):\n",
    "    nVal,_ = train_data.shape\n",
    "    \n",
    "    # Validation indexes adjustment -------------------------------\n",
    "    elemPerFold, remainder = np.divmod(nVal,k) #the remainder will be distributed across the firsts folds\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    # in each fold put as many samples as the division quotient +1 if the remainder is still positive\n",
    "    # then decrease the division remainder by 1\n",
    "    for i in range(k): \n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation --------------------------------------------\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values())) # build all the hyp-par combination\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList): # for each fold\n",
    "        if verbose: print(\"#{} fold:\".format(row+1))\n",
    "        for col, params in enumerate(params_product):\n",
    "            \n",
    "            if verbose:\n",
    "                update = col*100/len(params_product) # just print completion rate\n",
    "                print(\"\\t[\"+\"#\"*(int(update/5))+\" \"*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)} # {argument_name:argument_value, ... }\n",
    "            \n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            rem = Regression_evaluationMetric(trainVal_values[valIdx], pred)\n",
    "            #val_results[row,col] = rem.rSquared()\n",
    "            val_results[row,col] = rem.rootMeanSquareError()\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            # project each column on the current residuals\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            # find the most correlated variable\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "            \n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance # update the solution vector: canonical basis over the found column\n",
    "            residual -= np.matmul(data_matrix, self.weights) # update the residual\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 92])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "np.where(mp.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 1.3142650125262349e+29\n",
      "Root Mean Square Error: 4621874818400587.0\n",
      "R^2 score: -5.8928653264817184e+32\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 34, 38, 50, 76, 77, 92, 93, 94])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06476593732076837\n",
      "Root Mean Square Error: 0.10910037408315335\n",
      "R^2 score: 0.671645752809883\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression: # Iterative Soft Thresholding Algorithm (Proximal Gradient)\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        self.weights = np.zeros((data_matrix.shape[1],1))\n",
    "        n = data_matrix.shape[0]\n",
    "        # convergence step-size: n/(2*||X^t*X||_2)\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # gradient step of the lasso formulation\n",
    "            dist = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            coord_descent = (step/n)*np.matmul(data_matrix.T, dist)\n",
    "            self.weights -= coord_descent\n",
    "\n",
    "            # soft thresholding operator\n",
    "            upper = self.weights > softTresh  # elem to be reduced\n",
    "            lower = self.weights < -softTresh # elem to be increased\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  27,  49,  51,  71,  91,  92,  98, 101])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=10)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.8)\n",
    "np.where(lr.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.035949252124582956\n",
      "Root Mean Square Error: 0.8445367975114345\n",
      "R^2 score: -18.675569556094118\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  76,  93,  94, 100])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.05678934411253694\n",
      "Root Mean Square Error: 0.11321034607324221\n",
      "R^2 score: 0.6464405946799007\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor: # Least Square Regression Tree with either fixed parameter or pruning\n",
    "    class Node:\n",
    "        def __init__(self, isLeaf=False, feature=None, feature_importance=None, cut=None, average=None,\n",
    "                     left=None, right=None):\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature # if internal, on wich feature it executes the split\n",
    "            self.feature_importance = feature_importance # solution variance reduction\n",
    "            self.cut = cut # if internal, threahold value for the cut\n",
    "            self.avg = average # mean of seen training values\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            if self.cut:\n",
    "                print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"Leaf => {}\\n\".format(self.avg))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            if self.cut:\n",
    "                print(\"|    \"*level+\"{} => {}\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"|    \"*level+\"Leaf: {}\".format(self.avg))                \n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self, root=None, feature_importances=None):\n",
    "        self.root = root\n",
    "        self.feature_importances = feature_importances\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, depth, minElem_perLeaf, pruning=False):\n",
    "        \n",
    "        self.feature_importances = {k:0 for k in range(X.shape[1])}\n",
    "        \n",
    "        if not pruning:\n",
    "            self.root = self.learn(X, y.flatten(), depth, minElem_perLeaf)\n",
    "        else:\n",
    "            # train dataset, pruning dataset\n",
    "            X_trn, X_val, y_trn, y_val = trainTest_split(X, y.flatten(), train_amount=0.7)\n",
    "            self.root = self.learn(X_trn, y_trn, depth, minElem_perLeaf)\n",
    "            self.prune(X_val, y_val)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElem_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElem_perLeaf: # leaf # or fraction error of the root node??? \n",
    "            return self.Node(isLeaf=True, average=np.mean(y))\n",
    "            \n",
    "        best_costDescent = 0 # split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "                \n",
    "                if sorted_x[i2]<sorted_x[i2+1]: # for a different value\n",
    "                    # try to maximise this value: it is directly correlated \n",
    "                    # to the possible split information gain\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "                        \n",
    "        # update the importance for the selected feature\n",
    "        feature_importance = np.var(y) - (np.sum(np.square(y))-best_costDescent)/n\n",
    "        self.feature_importances[best_feature] += feature_importance\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "        \n",
    "        return self.Node(feature=best_feature, feature_importance=feature_importance,\n",
    "                         cut=best_cut, average=np.mean(y),\n",
    "                         left = self.learn(X[left_idxs], y[left_idxs], depth-1, minElem_perLeaf),\n",
    "                         right = self.learn(X[~left_idxs], y[~left_idxs], depth-1, minElem_perLeaf))\n",
    "    \n",
    "    def prune(self, X, y):\n",
    "        # for statistics purposes check errors on different dataset portions and average them\n",
    "        # in order to decide whether to prune or not (same code of k-fold cross-validation)\n",
    "        n,_ = X.shape\n",
    "        folds = 5\n",
    "        elemPerFold, remainder = np.divmod(n, folds)\n",
    "        foldsIdxsList = []\n",
    "        start = 0\n",
    "        for i in range(folds): \n",
    "            end = start+elemPerFold+int(remainder>0)\n",
    "            foldsIdxsList.append(np.arange(start,end)) \n",
    "            remainder -= 1\n",
    "            start = end\n",
    "        \n",
    "        # recursive: start checking if the root receive a possible positive pruning from its sons\n",
    "        self.test_pruning(self.root, X, y, foldsIdxsList)\n",
    "        return self\n",
    "    \n",
    "    def test_pruning(self, node, X, y, foldIdxs):\n",
    "        if node.isLeaf: # leaf: start point of new possible pruning\n",
    "            return True\n",
    "        \n",
    "        # check sons response: if one of them is negative to be pruned it means that it performs an important\n",
    "        # predictive split\n",
    "        if not self.test_pruning(node.left, X, y, foldIdxs) or not self.test_pruning(node.right, X, y, foldIdxs):\n",
    "            return False\n",
    "        \n",
    "        # else proceed with testing the goodness of the current node split\n",
    "        folds = len(foldIdxs)\n",
    "        results = np.empty(folds)\n",
    "\n",
    "        # not pruned errors on different folds\n",
    "        for i, idxs in enumerate(foldIdxs):\n",
    "            pred = self.predict(X[idxs])\n",
    "            results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "\n",
    "        not_prunErr = np.mean(results)\n",
    "\n",
    "        # pruned errors on different folds\n",
    "        node.isLeaf = True\n",
    "        for i, idxs in enumerate(foldIdxs):\n",
    "            pred = self.predict(X[idxs])\n",
    "            results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "\n",
    "        # if pruning improves the prediction RMSE then keep current node as leaf\n",
    "        node.isLeaf = np.mean(results) <= not_prunErr\n",
    "        \n",
    "        if node.isLeaf:\n",
    "            # lower feature importance computed during training phase\n",
    "            self.feature_importances[node.feature] -= node.feature_importance \n",
    "            node.left = None\n",
    "            node.right = None\n",
    "            \n",
    "        return node.isLeaf\n",
    "            \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n,_ = X.shape\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.cut:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.avg\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()\n",
    "        \n",
    "    def print_featureImportances(self):\n",
    "        print([(k,v) for k,v in sorted(self.feature_importances.items(), key=lambda kv: kv[1], reverse=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    |    Leaf: -0.06859882898361261\n",
      "|    |    |    |    42 => -0.17021276595744683\n",
      "|    |    |    |    |    Leaf: -0.3795427870865701\n",
      "|    |    |    74 => -0.857107044372023\n",
      "|    |    |    |    Leaf: -0.6101972943249725\n",
      "|    |    44 => -0.5331179321486268\n",
      "|    |    |    |    Leaf: 0.1907227549262237\n",
      "|    |    |    37 => -0.6700290647974012\n",
      "|    |    |    |    Leaf: 0.766500451240285\n",
      "|    100 => -0.7800478556966685\n",
      "|    |    |    |    |    Leaf: -0.4643401529895524\n",
      "|    |    |    |    18 => -0.46325802615933415\n",
      "|    |    |    |    |    Leaf: -0.2054652338867468\n",
      "|    |    |    38 => 0.16768802228412272\n",
      "|    |    |    |    |    Leaf: -0.35726011021157494\n",
      "|    |    |    |    96 => 0.7990967365967365\n",
      "|    |    |    |    |    Leaf: -0.6019697884586385\n",
      "|    |    100 => -0.9556414503957298\n",
      "|    |    |    |    Leaf: -0.5748053336160026\n",
      "|    |    |    41 => 0.26237113402061873\n",
      "|    |    |    |    |    Leaf: -0.8909879291383445\n",
      "|    |    |    |    76 => 0.22916666666666663\n",
      "|    |    |    |    |    Leaf: -0.7538590981966584\n",
      "50 => -0.47861060329067645\n",
      "|    |    |    |    |    Leaf: 0.011381855282161402\n",
      "|    |    |    |    91 => -0.7070948980028225\n",
      "|    |    |    |    |    Leaf: -0.7226604278338898\n",
      "|    |    |    94 => -0.6685959023035073\n",
      "|    |    |    |    Leaf: -0.022163038341438124\n",
      "|    |    51 => -0.9968553625190899\n",
      "|    |    |    |    |    Leaf: -0.8531794814163758\n",
      "|    |    |    |    2 => -0.942795076031861\n",
      "|    |    |    |    |    Leaf: -0.9327394333953144\n",
      "|    |    |    44 => 0.09127625201938616\n",
      "|    |    |    |    |    Leaf: -0.8077667569302209\n",
      "|    |    |    |    29 => -0.5717448178707989\n",
      "|    |    |    |    |    Leaf: -0.6676854538682261\n",
      "|    50 => -0.7612431444241317\n",
      "|    |    |    |    Leaf: -0.4847822342356667\n",
      "|    |    |    93 => 0.06941215543008972\n",
      "|    |    |    |    |    Leaf: -0.9095919845591905\n",
      "|    |    |    |    40 => 0.053475935828876886\n",
      "|    |    |    |    |    Leaf: -0.9639909039950465\n",
      "|    |    3 => 0.7585353274883961\n",
      "|    |    |    |    |    Leaf: -0.8270654364663984\n",
      "|    |    |    |    0 => -0.999941607308741\n",
      "|    |    |    |    |    Leaf: -0.14994280363760026\n",
      "|    |    |    99 => -0.8463316289724763\n",
      "|    |    |    |    |    Leaf: -0.7390291193526874\n",
      "|    |    |    |    41 => 0.25103092783505165\n",
      "|    |    |    |    |    Leaf: -0.9106799993713897\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElem_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06540013440039279\n",
      "Root Mean Square Error: 0.11599508981488645\n",
      "R^2 score: 0.6288329888004036\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(44, 0.06962163735211466), (37, 0.06215999166988643), (100, 0.06128364452888896), (74, 0.024432119208364744), (42, 0.024153879925208857), (50, 0.023355295640304405), (18, 0.01562792597514728), (38, 0.010954559491003331), (94, 0.007536197202554325), (0, 0.0073930942201939475), (96, 0.005234294939369014), (41, 0.004621503536219432), (91, 0.004310264244827965), (29, 0.004279966735905912), (51, 0.004264621889466011), (76, 0.0027973363054363144), (99, 0.0015393606728029072), (2, 0.001076901202835631), (3, 0.0009037672266171033), (93, 0.0005371235402911339), (40, 0.00036856784794784777), (1, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0), (16, 0), (17, 0), (19, 0), (20, 0), (21, 0), (22, 0), (23, 0), (24, 0), (25, 0), (26, 0), (27, 0), (28, 0), (30, 0), (31, 0), (32, 0), (33, 0), (34, 0), (35, 0), (36, 0), (39, 0), (43, 0), (45, 0), (46, 0), (47, 0), (48, 0), (49, 0), (52, 0), (53, 0), (54, 0), (55, 0), (56, 0), (57, 0), (58, 0), (59, 0), (60, 0), (61, 0), (62, 0), (63, 0), (64, 0), (65, 0), (66, 0), (67, 0), (68, 0), (69, 0), (70, 0), (71, 0), (72, 0), (73, 0), (75, 0), (77, 0), (78, 0), (79, 0), (80, 0), (81, 0), (82, 0), (83, 0), (84, 0), (85, 0), (86, 0), (87, 0), (88, 0), (89, 0), (90, 0), (92, 0), (95, 0), (97, 0), (98, 0), (101, 0)]\n"
     ]
    }
   ],
   "source": [
    "ndt.print_featureImportances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    Leaf: 0.19314924496384922\n",
      "|    |    |    51 => -0.8367909142541375\n",
      "|    |    |    |    |    Leaf: -0.646243811088586\n",
      "|    |    |    |    52 => -0.24731684554363048\n",
      "|    |    |    |    |    |    Leaf: -0.08567087578893451\n",
      "|    |    |    |    |    17 => -0.3599910394265233\n",
      "|    |    |    |    |    |    |    Leaf: -0.27978075463864716\n",
      "|    |    |    |    |    |    101 => -0.5957886044591247\n",
      "|    |    |    |    |    |    |    Leaf: -0.40994680391269817\n",
      "|    |    69 => 0.3112267013437364\n",
      "|    |    |    |    |    |    Leaf: -0.68162899951269\n",
      "|    |    |    |    |    71 => -0.8584454530718106\n",
      "|    |    |    |    |    |    Leaf: -0.8320970968981463\n",
      "|    |    |    |    44 => 0.22065158858373723\n",
      "|    |    |    |    |    |    |    Leaf: -0.5084631477285609\n",
      "|    |    |    |    |    |    41 => 0.12268041237113414\n",
      "|    |    |    |    |    |    |    Leaf: -0.6319842254837154\n",
      "|    |    |    |    |    87 => -0.515451174289246\n",
      "|    |    |    |    |    |    |    Leaf: -0.6670630661666954\n",
      "|    |    |    |    |    |    2 => -0.7669390710665149\n",
      "|    |    |    |    |    |    |    Leaf: -0.7777026937499172\n",
      "|    |    |    3 => 0.22031975244971647\n",
      "|    |    |    |    |    |    Leaf: -0.3060086361354398\n",
      "|    |    |    |    |    99 => -0.9004452746387939\n",
      "|    |    |    |    |    |    Leaf: -0.4965542320155351\n",
      "|    |    |    |    38 => -0.08746518105849582\n",
      "|    |    |    |    |    Leaf: -0.681089425077182\n",
      "|    3 => -0.24187725631768942\n",
      "|    |    |    Leaf: 0.4483809927622782\n",
      "|    |    100 => -0.28934290447266703\n",
      "|    |    |    Leaf: -0.09044091991184255\n",
      "49 => -0.9874838169145703\n",
      "|    |    |    |    |    Leaf: -0.30714402441555916\n",
      "|    |    |    |    50 => -0.3111517367458867\n",
      "|    |    |    |    |    Leaf: -0.6099552587528101\n",
      "|    |    |    40 => 0.26251823043266886\n",
      "|    |    |    |    |    |    Leaf: -0.4170770524071792\n",
      "|    |    |    |    |    18 => -0.4439952437574316\n",
      "|    |    |    |    |    |    Leaf: -0.669117939340939\n",
      "|    |    |    |    99 => -0.7703703536190719\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.5261623670018949\n",
      "|    |    |    |    |    |    |    14 => -0.6232771822358346\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8376109145676265\n",
      "|    |    |    |    |    |    |    |    |    49 => -0.9929543158369617\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.6854303124526124\n",
      "|    |    |    |    |    |    |    |    |    |    90 => -0.3533834586466164\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.7611466656066569\n",
      "|    |    |    |    |    |    |    |    |    |    |    18 => -0.5902497027348395\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8057855924056186\n",
      "|    |    |    |    |    |    |    |    86 => -0.6500622665006226\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6823377759101416\n",
      "|    |    |    |    |    |    63 => -0.842589549786395\n",
      "|    |    |    |    |    |    |    Leaf: -0.8727758565100061\n",
      "|    |    |    |    |    41 => -0.12783505154639174\n",
      "|    |    |    |    |    |    Leaf: -0.8717096633143857\n",
      "|    |    50 => -0.5440585009140768\n",
      "|    |    |    |    Leaf: -0.588976781368561\n",
      "|    |    |    4 => -0.5880201985025248\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6720771333801504\n",
      "|    |    |    |    |    |    |    |    89 => 0.29411764705882326\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7710202152703246\n",
      "|    |    |    |    |    |    |    |    |    54 => 0.01051850018562056\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.7666255912867195\n",
      "|    |    |    |    |    |    |    |    |    |    75 => -0.005442349069737895\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8336447112135789\n",
      "|    |    |    |    |    |    |    3 => 0.18009283135636944\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6708121883460756\n",
      "|    |    |    |    |    |    44 => 0.14216478190630055\n",
      "|    |    |    |    |    |    |    Leaf: -0.6197757448304014\n",
      "|    |    |    |    |    68 => -0.8677658142664872\n",
      "|    |    |    |    |    |    |    Leaf: -0.7490214784486756\n",
      "|    |    |    |    |    |    101 => -0.6699009083402148\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7048756034326651\n",
      "|    |    |    |    |    |    |    36 => 0.08298368298368286\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7849659692685491\n",
      "|    |    |    |    |    |    |    |    92 => -0.9986599023643151\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9232862070640819\n",
      "|    |    |    |    |    |    |    |    |    |    25 => -0.8446423357664233\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8958275364047117\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    34 => 0.11379741558982892\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.861555652723121\n",
      "|    |    |    |    |    |    |    |    |    |    |    91 => -0.9983321216268228\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8355991042917147\n",
      "|    |    |    |    |    |    |    |    |    61 => -0.9861727106704931\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9278133322733235\n",
      "|    |    |    |    49 => -0.9962165225748119\n",
      "|    |    |    |    |    |    Leaf: -0.5080141157972378\n",
      "|    |    |    |    |    24 => 0.6768238119907691\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7979474087913091\n",
      "|    |    |    |    |    |    |    |    |    99 => -0.8555175384838047\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.847710452432796\n",
      "|    |    |    |    |    |    |    |    |    |    |    57 => -0.9212242849974912\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8691152893164261\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    55 => 0.0960227272727272\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9067711561517129\n",
      "|    |    |    |    |    |    |    |    |    |    71 => -0.9932554477456406\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9269249431033427\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    90 => -0.5413533834586466\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9632741936196243\n",
      "|    |    |    |    |    |    |    |    |    |    |    61 => -0.9809548656404905\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8868457199895765\n",
      "|    |    |    |    |    |    |    |    14 => -0.7702909647779479\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.626487878346208\n",
      "|    |    |    |    |    |    |    |    |    89 => 0.5133689839572191\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.773982721840177\n",
      "|    |    |    |    |    |    |    |    |    |    45 => 0.6775465498357063\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8462296776245181\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    41 => 0.0561855670103093\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9111049277205814\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    58 => -0.9621152328334649\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9543135774005909\n",
      "|    |    |    |    |    |    |    |    |    |    |    43 => 0.23177120203879376\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.6433067005869804\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    43 => 0.21902874132804762\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8441394392421939\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    8 => -0.7363513055272974\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9027396836754074\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    74 => -0.8202557031837554\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.7849554182450262\n",
      "|    |    |    |    |    |    |    35 => -0.8605038903297517\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6517470286600151\n",
      "|    |    |    |    |    |    2 => -0.9545877728354195\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8136458595575342\n",
      "|    |    |    |    |    |    |    56 => -0.4259664478482859\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8952279683587073\n",
      "|    |    |    |    |    |    |    |    |    0 => -0.9978476754854217\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9486022838303847\n",
      "|    |    |    |    |    |    |    |    23 => -0.9750041666666667\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9774847292337454\n",
      "|    50 => -0.7663619744058501\n",
      "|    |    |    |    Leaf: -0.5123424891680248\n",
      "|    |    |    5 => -0.3835242198171693\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9418993167353464\n",
      "|    |    |    |    |    |    |    |    86 => -0.6102117061021171\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8474751891454997\n",
      "|    |    |    |    |    |    |    74 => -0.9801955377287541\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7823336115859072\n",
      "|    |    |    |    |    |    2 => -0.913520223440571\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.816471080135269\n",
      "|    |    |    |    |    |    |    |    99 => -0.7171590166418287\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.85913794702601\n",
      "|    |    |    |    |    |    |    |    |    49 => -0.9957957149653971\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8966684775167503\n",
      "|    |    |    |    |    |    |    |    |    |    |    100 => -0.8643475059819621\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8884472181367677\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    27 => -0.9904535726354522\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9704292598039865\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    29 => -0.6780036224592474\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9359347740633266\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    39 => -0.5136349550666253\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9576128579189358\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    51 => -0.9993149011167544\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.922600145162454\n",
      "|    |    |    |    |    |    |    |    |    |    94 => -0.586873480726936\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.880119957776276\n",
      "|    |    |    |    |    |    |    40 => -0.0831307729703451\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9325909478948394\n",
      "|    |    |    |    |    |    |    |    |    14 => -0.8453292496171516\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8638682407812273\n",
      "|    |    |    |    |    |    |    |    51 => -0.9946305915135116\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9577527203237741\n",
      "|    |    |    |    |    |    |    |    |    |    |    24 => -0.5754250459190882\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9833028825887009\n",
      "|    |    |    |    |    |    |    |    |    |    83 => -0.1507760532150776\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9491681131383798\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    22 => -0.9093673392419386\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9746422098848858\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    3 => 0.8790097988653947\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9183645226496123\n",
      "|    |    |    |    |    |    |    |    |    |    |    95 => 0.5412857785104151\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9108237306750644\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    100 => -0.9571139333701455\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9402331138229695\n",
      "|    |    |    |    |    |    |    |    |    99 => -0.823586213446887\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9448574065976777\n",
      "|    |    |    |    |    |    |    |    |    |    |    25 => -0.837014598540146\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9713731102012694\n",
      "|    |    |    |    |    |    |    |    |    |    71 => -0.9937938540629414\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.990891865749758\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    28 => -0.6825313807531381\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9683675407330851\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    3 => 0.9875193398659103\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9836526612871168\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    29 => -0.8563091165224391\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9742031155454856\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    93 => -0.9762537363002325\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    60 => 0.8583432080146502\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9811848259596402\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    64 => -0.2551020408163264\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9596383599881632\n",
      "|    |    |    |    |    82 => -0.9009063444108761\n",
      "|    |    |    |    |    |    Leaf: -0.7541807817277865\n",
      "|    |    |    |    99 => -0.8944615433323007\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8942669664138837\n",
      "|    |    |    |    |    |    |    95 => 0.7928967480893151\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8714296440575409\n",
      "|    |    |    |    |    |    |    |    |    |    7 => -0.22329461802715533\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9423443000181084\n",
      "|    |    |    |    |    |    |    |    |    71 => -0.9922828427853554\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9933767054011916\n",
      "|    |    |    |    |    |    |    |    |    |    1 => -0.220108695652174\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9533615315571298\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    42 => -0.3659574468085106\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9649304589400785\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    60 => 0.9464612732952709\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.97618903900787\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    31 => -0.7160541586073501\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9918025908406319\n",
      "|    |    |    |    |    |    |    |    |    |    |    35 => -0.5831789551685811\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9385582983309263\n",
      "|    |    |    |    |    |    |    |    82 => -0.8489425981873111\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8966048769284449\n",
      "|    |    |    |    |    |    8 => -0.8241776873516446\n",
      "|    |    |    |    |    |    |    Leaf: -0.8883633006938647\n",
      "|    |    |    |    |    2 => -0.9328643839867591\n",
      "|    |    |    |    |    |    |    Leaf: -0.9151648634231349\n",
      "|    |    |    |    |    |    41 => 0.16546391752577327\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9900359078321473\n",
      "|    |    |    |    |    |    |    |    |    69 => -0.29150411790203734\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9637211644207708\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    38 => -0.28077994428969355\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9727413430955189\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    22 => -0.8111587780501603\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9921559274423288\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    72 => 0.9644076060458313\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.981312279519538\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    42 => -0.1914893617021276\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9759488191932443\n",
      "|    |    |    |    |    |    |    |    |    |    |    42 => -0.2723404255319148\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.962488903022353\n",
      "|    |    |    |    |    |    |    |    |    |    29 => -0.917890923727108\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9838701841325366\n",
      "|    |    |    |    |    |    |    |    |    |    |    100 => -0.9053929688937972\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9904923009426726\n",
      "|    |    |    |    |    |    |    |    1 => -0.34510869565217406\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9167384381638863\n",
      "|    |    |    |    |    |    |    |    |    79 => 0.007835019237034002\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9792344079182731\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    96 => -0.1006701631701632\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9655526258062332\n",
      "|    |    |    |    |    |    |    |    |    |    |    32 => -0.8930123704446673\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9902626321515696\n",
      "|    |    |    |    |    |    |    |    |    |    76 => 0.4375\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9657970169557402\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    23 => -0.9426916666666667\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9345797281958192\n",
      "|    |    |    |    |    |    |    |    |    |    |    2 => -0.957794558808317\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9210822700109976\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    60 => 0.9685446515135191\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9425563019791267\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    30 => -0.32202216066482\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9827473589315887\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    29 => -0.7297242906017307\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9507095440632826\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    50 => -0.8409506398537477\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9646045059250131\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    45 => 0.8175246440306682\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9722948770609461\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    94 => -0.01551105452019913\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9846784416089183\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    40 => -0.4394749635391346\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9468184247370957\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    58 => -0.9806629834254144\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9684764861852749\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    75 => -0.4582964181749145\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9857285138220861\n",
      "|    |    |    |    |    |    |    11 => 0.7791\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9325966265187953\n",
      "|    |    |    |    |    |    |    |    24 => 0.0591908821174586\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9592249561641779\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    16 => -0.17588711930706896\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9714253745736038\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    48 => 0.4019401096583717\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9801910667673677\n",
      "|    |    |    |    |    |    |    |    |    |    |    40 => -0.496353913466213\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.988327525362943\n",
      "|    |    |    |    |    |    |    |    |    |    24 => -0.9413177600904252\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9088568235922849\n",
      "|    |    |    |    |    |    |    |    |    0 => -0.9959914169581401\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9731464182710357\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    78 => -0.5647279549718573\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9954831804416413\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    76 => 0.39583333333333337\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9922530950077955\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    78 => -0.9549718574108819\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9888522302164628\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    22 => -0.9314868942108241\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9987647941299013\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    19 => -0.4640676410419956\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9989399901949092\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    82 => -0.7311178247734138\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9882751137670246\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    34 => -0.5000000000000001\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9806618877891287\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    43 => 0.7549200056633161\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9915552552194441\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    12 => -0.5480877316811754\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9752146993073972\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    12 => -0.6046236025061216\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9793003640839233\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    42 => -0.2893617021276595\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9960981543841185\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    78 => -0.7298311444652907\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9793150864423273\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    60 => 0.9404287407088224\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.98618012216613\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    37 => -0.4310138485211147\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9818959158705551\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    88 => 0.09900990099009876\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9920004593375824\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    69 => -0.3111183355006502\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9966521356989219\n",
      "|    |    |    |    |    |    |    |    |    |    |    60 => 0.8473553808036196\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9762545184495338\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    90 => -0.593984962406015\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9568046004425541\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    96 => 0.6134906759906757\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9503032069713312\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    66 => -0.2264150943396228\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9706156448613816\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    89 => 0.12299465240641705\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9783426746697408\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    60 => 0.814068727781967\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9947397013422374\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    14 => -0.6385911179173047\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9881455570130689\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    26 => -0.6638852291026204\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.998326951190965\n",
      "|    |    |    |    |    |    |    |    |    |    15 => -0.28583443469902675\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9680919381837615\n",
      "|    |    3 => 0.7885507993811245\n",
      "|    |    |    |    |    Leaf: -0.5025815655461481\n",
      "|    |    |    |    9 => -0.38661710037174735\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8096211348288305\n",
      "|    |    |    |    |    |    |    34 => -0.3538974572738642\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8657968697321562\n",
      "|    |    |    |    |    |    |    |    92 => -0.9977026897673973\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9569396255010598\n",
      "|    |    |    |    |    |    |    |    |    77 => -0.9417922948073703\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9060230640466758\n",
      "|    |    |    |    |    |    3 => 0.5422382671480145\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7900147076360458\n",
      "|    |    |    |    |    |    |    40 => -0.3028682547399124\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6085162954423995\n",
      "|    |    |    |    |    10 => -0.9971253238619697\n",
      "|    |    |    |    |    |    Leaf: -0.6071338659882605\n",
      "|    |    |    99 => -0.8027720551154571\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8336955033500727\n",
      "|    |    |    |    |    |    |    |    35 => -0.30196369025565034\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8722030586190329\n",
      "|    |    |    |    |    |    |    |    |    69 => 0.0020589510186388438\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.93866665488878\n",
      "|    |    |    |    |    |    |    |    |    |    15 => -0.315030637991109\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8953836572988301\n",
      "|    |    |    |    |    |    |    96 => -0.2695221445221445\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7897382217452178\n",
      "|    |    |    |    |    |    49 => -0.997740528511611\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9288304369196368\n",
      "|    |    |    |    |    |    |    |    |    |    |    51 => -0.9974338797961122\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9615017688913623\n",
      "|    |    |    |    |    |    |    |    |    |    51 => -0.9978064353205682\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8989501486222081\n",
      "|    |    |    |    |    |    |    |    |    10 => -0.9969789270534202\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9744837973084586\n",
      "|    |    |    |    |    |    |    |    39 => -0.5106910443136039\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.947913768202356\n",
      "|    |    |    |    |    |    |    |    |    71 => -0.992207581687238\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9666017410661049\n",
      "|    |    |    |    |    |    |    |    |    |    0 => -0.9979159142510851\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9924523375915302\n",
      "|    |    |    |    |    |    |    |    |    |    |    74 => -0.9779393331662071\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9678669805473478\n",
      "|    |    |    |    |    |    |    46 => 0.5479647394347937\n",
      "|    |    |    |    |    |    |    |    |    98 => -0.9955448457507916\n",
      "|    |    |    |    |    |    |    |    10 => -0.9971387071523035\n",
      "|    |    |    |    |    |    |    |    |    8 => -0.7521193624957612\n",
      "|    |    |    |    |    76 => -0.3541666666666667\n",
      "|    |    |    |    |    |    Leaf: -0.8345112692292403\n",
      "|    |    |    |    1 => -0.5760869565217392\n",
      "|    |    |    |    |    Leaf: -0.7886101946443004\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=100, minElem_perLeaf=10, pruning=True)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.062098107613760584\n",
      "Root Mean Square Error: 0.12414235461417542\n",
      "R^2 score: 0.5748618135584248\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.08437559577034112), (100, 0.0719042140165036), (51, 0.05852751161056811), (50, 0.028449060341569465), (69, 0.023300662350671436), (49, 0.022838974518344147), (99, 0.01753242368465365), (38, 0.014861822507211856), (40, 0.01414864455260557), (18, 0.011671790902563834), (52, 0.01151880208301218), (44, 0.009497764506940141), (41, 0.007625530995009112), (17, 0.00585882344972834), (14, 0.00525367998654249), (101, 0.005179557155882869), (9, 0.005150846144585731), (71, 0.004923200784036352), (74, 0.004450117218989524), (2, 0.004295468469618582), (10, 0.00380481445178369), (86, 0.003765862203045354), (87, 0.0036370332167295315), (43, 0.0032514493548001545), (4, 0.003220307316960427), (34, 0.0029576817075927355), (89, 0.0027264630517829652), (63, 0.0026430998865404065), (68, 0.002213887521839729), (92, 0.0018548821129251252), (35, 0.0017368667150074425), (90, 0.001709278979930168), (36, 0.0016885144270813615), (24, 0.001526559610022017), (1, 0.0012000668319733843), (8, 0.0011197835256584578), (56, 0.0011083159625038129), (61, 0.0010080118458729017), (45, 0.0010042234464219433), (23, 0.0008278092507135397), (0, 0.0008234974960077848), (96, 0.0008050767532880851), (82, 0.0007241919827981676), (75, 0.0006862798263592248), (77, 0.0006301206115403213), (25, 0.0006294131841772549), (76, 0.0006050465701791996), (54, 0.0005496706395926284), (57, 0.0005477329990557319), (58, 0.0004981532429722907), (95, 0.00042405724812702705), (7, 0.00041561061405057884), (15, 0.0004122321222908487), (94, 0.0003987176478215327), (83, 0.00039332819021461877), (5, 0.00037324459392128044), (39, 0.0003387463735996776), (55, 0.00026586830758444104), (46, 0.00026292591603910244), (27, 0.00023387902093252865), (29, 0.00023314365128480403), (60, 0.00020963614806487768), (91, 0.00016528185694333615), (22, 0.000150811025670293), (42, 0.00012286467794420304), (64, 0.00011574104605941243), (79, 0.00011372487562245567), (30, 6.80996169961454e-05), (66, 5.998742062431417e-05), (11, 4.9240749116842434e-05), (32, 4.347907369711439e-05), (31, 3.997614673066748e-05), (28, 3.469125083588774e-05), (16, 3.2392822116788936e-05), (12, 3.123315569404493e-05), (93, 2.4561606259398834e-05), (78, 2.3473393467401974e-05), (26, 2.284062028719092e-05), (48, 1.881731256428605e-05), (88, 1.418080537240922e-05), (72, 1.2503819367306452e-05), (37, 9.008543740790097e-06), (19, 4.679676834505626e-06), (6, 0), (13, 0), (20, 0), (21, 0), (33, 0), (47, 0), (53, 0), (59, 0), (62, 0), (65, 0), (67, 0), (70, 0), (73, 0), (80, 0), (81, 0), (84, 0), (85, 0), (97, 0), (98, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "ndt.print_featureImportances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50, 100,  44,  51,   3,  74,   6,  42,  37,  66,  38,  40,  18,\n",
       "        91,   2,  99,  41,   0,  93,  46,  96,  49,  73,  78,  36,   5,\n",
       "        29,  14,  28,  88,  23,  98,  26,  52,   8,  24,  31,  95,  34,\n",
       "        89,  11,  58,  76,  43,  59,   7,   9,  94,  82,  71,  86,  77,\n",
       "        25,  45,   4,  69,  90,  83,  12,  72,  27,  48,  13,  22,  64,\n",
       "        60,  67,  85,  92,   1,  57,  39,  75,  47,  21,  17,  55,  79,\n",
       "        53,  65,  56,  35,  87,  10,  16,  15,  97,  32,  54,  62,  20,\n",
       "        30,  63,  80,  33,  68,  61,  19,  81,  84,  70, 101])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07434363826362839\n",
      "Root Mean Square Error: 0.12263888489662965\n",
      "R^2 score: 0.5850970289177095\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalRandomForest_regressor: # post-pruning (kinda) with cross-validation or greedy \n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.bootSubSp_samplesIdxs = []\n",
    "        self.oob_error = None\n",
    "        self.feature_importances = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElems_perLeaf):\n",
    "        n,d = X.shape\n",
    "        \n",
    "        self.feature_importances = {k:0 for k in range(d)}\n",
    "        \n",
    "        n_learn = int(n/3) # Bootstrap amount to be taken aside\n",
    "        d_learn = int(np.sqrt(d)) # random subspace method amount\n",
    "        \n",
    "        #val_folds = 3\n",
    "        #params_dict = {\"depth\":depths, \"minElem_perLeaf\":minElems_perLeaf}\n",
    "        \n",
    "        # Fitting the forest -----------------------------------\n",
    "        for i in range(self.n_trees):\n",
    "            print(\"Fitting #{} tree\".format(i+1))\n",
    "            \n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            subspace_idxs = np.sort(np.random.permutation(d)[:d_learn])\n",
    "            self.bootSubSp_samplesIdxs.append((bootstrap_idxs, subspace_idxs))\n",
    "                        \n",
    "            dt = NumericalDecisionTree_regressor()\n",
    "            \n",
    "            # find the best hyp-par for the current setting (bootstrapping)\n",
    "            #win_params = kFold_crossValidation_selectionGrid(val_folds, params_dict, \n",
    "            #                                                 X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "            #                                                 dt, verbose=True)\n",
    "            #self.trees.append(dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "            #                         depth=win_params[0], minElem_perLeaf=win_params[1]))\n",
    "            \n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs][:,subspace_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=depth, minElem_perLeaf=minElems_perLeaf, pruning=True))\n",
    "            \n",
    "            for k,v in dt.feature_importances.items():\n",
    "                self.feature_importances[subspace_idxs[k]] += v\n",
    "        \n",
    "        # Out-Of-Bag Estimate for the forest -------------------\n",
    "        oob_errors = []\n",
    "        for sampleIdx in range(n):\n",
    "            missingBoot_TreesIdx = [(idx,subspace_idxs) for idx,(bootstrap_idxs,subspace_idxs) in enumerate(self.bootSubSp_samplesIdxs) \n",
    "                                    if sampleIdx not in bootstrap_idxs]\n",
    "\n",
    "            if len(missingBoot_TreesIdx) == 0: continue\n",
    "            \n",
    "            regr_results = np.empty(len(missingBoot_TreesIdx)) # regression estimate of the selected trees\n",
    "            for i, (missing_tree, tree_subSpace) in enumerate(missingBoot_TreesIdx):\n",
    "                # reshape in order to correctly use the decision_tree.predict(...): it needs a matrix (num,dim)\n",
    "                # while numpy matrix indexing returns (dim,)\n",
    "                regr_results[i] = self.trees[missing_tree].predict(X[sampleIdx,tree_subSpace].reshape(1,-1))\n",
    "            \n",
    "            # done at this level of granularity because a sample might end up in \n",
    "            # being part of no bootstrap set of any tree (so we cannot predict wich value in y will be used)\n",
    "            oob_errors.append(np.square(y[sampleIdx]-np.mean(regr_results)))\n",
    "            #oob_errors.append(r2_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            #oob_errors.append(explained_variance_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            \n",
    "        self.oob_error = np.sqrt(np.mean(oob_errors))\n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"Trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n,_ = X.shape\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, (tree,(_,subspace_idxs)) in enumerate(zip(self.trees, self.bootSubSp_samplesIdxs)):\n",
    "            results[row] = tree.predict(X[:,subspace_idxs])\n",
    "            \n",
    "        return np.mean(results,axis=0)\n",
    "    \n",
    "    def print_featureImportances(self):\n",
    "        print([(k,v) for k,v in sorted(self.feature_importances.items(), key=lambda kv: kv[1], reverse=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(size=(5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1,[1,2]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "Fitting #2 tree\n",
      "Fitting #3 tree\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13055868094831646"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(3)\n",
    "# no train and test, cause it's a forest\n",
    "nrf.fit(data, values, depth=100, minElems_perLeaf=5);\n",
    "nrf.oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(99, 0.15566421653666093), (3, 0.13049725352514754), (57, 0.11493047410018982), (34, 0.11322429125451826), (51, 0.1124435063719215), (26, 0.10047303639577089), (23, 0.09100090823803786), (2, 0.0879900563867197), (12, 0.083404047467246), (14, 0.07836381086034548), (21, 0.07694193082487932), (47, 0.06306424837671218), (76, 0.05937313885296492), (43, 0.05530174488231513), (66, 0.049198681127203894), (77, 0.047164211684424), (73, 0.044430035564181763), (37, 0.04200230295640027), (8, 0.038138077527618365), (40, 0.026636182973730108), (52, 0.026078021628253574), (48, 0.01921726533042986), (13, 0.01698531580510679), (90, 0.014093436553970752), (28, 0.00648772489582577), (60, 0.005963099507035082), (59, 0.0034920098813538522), (53, 0.0034677762544725035), (95, 0.001961266998716618), (0, 0), (1, 0), (4, 0), (5, 0), (6, 0), (7, 0), (9, 0), (10, 0), (11, 0), (15, 0), (16, 0), (17, 0), (18, 0), (19, 0), (20, 0), (22, 0), (24, 0), (25, 0), (27, 0), (29, 0), (30, 0), (31, 0), (32, 0), (33, 0), (35, 0), (36, 0), (38, 0), (39, 0), (41, 0), (42, 0), (44, 0), (45, 0), (46, 0), (49, 0), (50, 0), (54, 0), (55, 0), (56, 0), (58, 0), (61, 0), (62, 0), (63, 0), (64, 0), (65, 0), (67, 0), (68, 0), (69, 0), (70, 0), (71, 0), (72, 0), (74, 0), (75, 0), (78, 0), (79, 0), (80, 0), (81, 0), (82, 0), (83, 0), (84, 0), (85, 0), (86, 0), (87, 0), (88, 0), (89, 0), (91, 0), (92, 0), (93, 0), (94, 0), (96, 0), (97, 0), (98, 0), (100, 0), (101, 0)]\n"
     ]
    }
   ],
   "source": [
    "nrf.print_featureImportances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/.conda/envs/bcb/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 50, 100,   3,  49,  46,  44,  10,  99,  51,   2,  68,  38,  78,\n",
       "        40,  65,  93,  32,  69,  83,  15,  14,  27,  84,  41,  20,  34,\n",
       "        96,  66,  59,  71,  74,  94,   6,  39,  70,  92,   4,  73,  75,\n",
       "        89,  28,  43,  90,  33,  61,  72,  35,  13,  30,  25,  88,  22,\n",
       "        76,  47,   5,  53,  86,   1,  67,  23,  62,   8,  29,  60,  17,\n",
       "         7, 101,  16,  52,  95,  45,  54,  24,  48,  56,  11,  63,  37,\n",
       "        77,  12,   0,  82,  80,  26,  36,  18,  57,  55,  21,  87,  19,\n",
       "        98,  97,  31,  42,  91,  64,  81,  85,   9,  79,  58])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "# oob error not working, need to perform evaluation on test split\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06386462647128814\n",
      "Root Mean Square Error: 0.09876585539075922\n",
      "R^2 score: 0.7309061219268678\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.010685591460581137\n",
      "Root Mean Square Error: 0.10352228359071869\n",
      "R^2 score: 0.704363640444624\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[2,2.05,2.1,2.2,3]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "Fitting #2 tree\n",
      "Fitting #3 tree\n",
      "Fitting #4 tree\n",
      "Fitting #5 tree\n",
      "Fitting #6 tree\n",
      "Fitting #7 tree\n",
      "Fitting #8 tree\n",
      "Fitting #9 tree\n",
      "Fitting #10 tree\n",
      "Fitting #11 tree\n",
      "Fitting #12 tree\n",
      "Fitting #13 tree\n",
      "Fitting #14 tree\n",
      "Fitting #15 tree\n",
      "Fitting #16 tree\n",
      "Fitting #17 tree\n",
      "Fitting #18 tree\n",
      "Fitting #19 tree\n",
      "Fitting #20 tree\n",
      "Fitting #21 tree\n",
      "Fitting #22 tree\n",
      "Fitting #23 tree\n",
      "Fitting #24 tree\n",
      "Fitting #25 tree\n",
      "Fitting #26 tree\n",
      "Fitting #27 tree\n",
      "Fitting #28 tree\n",
      "Fitting #29 tree\n",
      "Fitting #30 tree\n",
      "Fitting #31 tree\n",
      "Fitting #32 tree\n",
      "Fitting #33 tree\n",
      "Fitting #34 tree\n",
      "Fitting #35 tree\n",
      "Fitting #36 tree\n",
      "Fitting #37 tree\n",
      "Fitting #38 tree\n",
      "Fitting #39 tree\n",
      "Fitting #40 tree\n",
      "Fitting #41 tree\n",
      "Fitting #42 tree\n",
      "Fitting #43 tree\n",
      "Fitting #44 tree\n",
      "Fitting #45 tree\n",
      "Fitting #46 tree\n",
      "Fitting #47 tree\n",
      "Fitting #48 tree\n",
      "Fitting #49 tree\n",
      "Fitting #50 tree\n",
      "Fitting #51 tree\n",
      "Fitting #52 tree\n",
      "Fitting #53 tree\n",
      "Fitting #54 tree\n",
      "Fitting #55 tree\n",
      "Fitting #56 tree\n",
      "Fitting #57 tree\n",
      "Fitting #58 tree\n",
      "Fitting #59 tree\n",
      "Fitting #60 tree\n",
      "Fitting #61 tree\n",
      "Fitting #62 tree\n",
      "Fitting #63 tree\n",
      "Fitting #64 tree\n",
      "Fitting #65 tree\n",
      "Fitting #66 tree\n",
      "Fitting #67 tree\n",
      "Fitting #68 tree\n",
      "Fitting #69 tree\n",
      "Fitting #70 tree\n",
      "Fitting #71 tree\n",
      "Fitting #72 tree\n",
      "Fitting #73 tree\n",
      "Fitting #74 tree\n",
      "Fitting #75 tree\n",
      "Fitting #76 tree\n",
      "Fitting #77 tree\n",
      "Fitting #78 tree\n",
      "Fitting #79 tree\n",
      "Fitting #80 tree\n",
      "Fitting #81 tree\n",
      "Fitting #82 tree\n",
      "Fitting #83 tree\n",
      "Fitting #84 tree\n",
      "Fitting #85 tree\n",
      "Fitting #86 tree\n",
      "Fitting #87 tree\n",
      "Fitting #88 tree\n",
      "Fitting #89 tree\n",
      "Fitting #90 tree\n",
      "Fitting #91 tree\n",
      "Fitting #92 tree\n",
      "Fitting #93 tree\n",
      "Fitting #94 tree\n",
      "Fitting #95 tree\n",
      "Fitting #96 tree\n",
      "Fitting #97 tree\n",
      "Fitting #98 tree\n",
      "Fitting #99 tree\n",
      "Fitting #100 tree\n",
      "Residual variance: 0.05236314744329154\n",
      "Root Mean Square Error: 0.10509992450430801\n",
      "R^2 score: 0.6952842047519372\n"
     ]
    }
   ],
   "source": [
    "rf = NumericalRandomForest_regressor(100)\n",
    "rf.fit(trainVal_data, trainVal_values, depth=100, minElems_perLeaf=10);\n",
    "\n",
    "pred = rf.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09992977321704354"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30, 1.2178680227941372), (39, 0.6701647017143142), (49, 0.6659491660238277), (44, 0.6382392500428182), (50, 0.6225635842194808), (62, 0.6218716434568194), (100, 0.5572697943487023), (71, 0.5560475815877705), (27, 0.509087921641871), (34, 0.5068620548994166), (45, 0.49411111055825846), (43, 0.4917319479706463), (91, 0.43180366015995286), (40, 0.4313690956894794), (67, 0.41333198252668646), (73, 0.4123353935077691), (0, 0.3881577499798254), (2, 0.381733763164473), (51, 0.37671500836115956), (29, 0.37548844897422123), (14, 0.37230806386790255), (68, 0.35467432904184865), (12, 0.32640732654711296), (3, 0.322538820036979), (63, 0.32083296700478364), (74, 0.31307657002969724), (99, 0.29823137874641725), (94, 0.2760106153577257), (69, 0.27337023063383514), (28, 0.26955794730674826), (78, 0.2621264762543446), (93, 0.2533951070725696), (26, 0.25077491854916667), (19, 0.24619182029119308), (54, 0.24183945314492106), (21, 0.2395214577318557), (53, 0.23637711194631805), (88, 0.23190499277464596), (84, 0.23168695298629682), (66, 0.2288465451045104), (59, 0.2249636090694935), (6, 0.2247618299479367), (18, 0.21872832024630712), (47, 0.2103832461387484), (89, 0.21015862610593183), (64, 0.20841472123785798), (10, 0.20644293104291644), (82, 0.20583959086506806), (48, 0.20459119777053658), (52, 0.2018800417430312), (58, 0.19194015029571504), (33, 0.19178837383294142), (32, 0.19124289784108622), (38, 0.18752974683137394), (98, 0.1774700297058862), (23, 0.1700066849742159), (17, 0.1651408729738514), (86, 0.16283527777622597), (57, 0.15863892490753106), (16, 0.15769988827926465), (42, 0.15735679321357832), (25, 0.14606947012826849), (83, 0.14529425568197568), (92, 0.14333541433604702), (70, 0.1408303732526316), (13, 0.13805869231448303), (36, 0.13677828994045443), (46, 0.1364526931316513), (15, 0.13116215351654403), (37, 0.13111976623461571), (77, 0.13084987873598464), (79, 0.13053889916578262), (9, 0.12889198916893047), (41, 0.12830323357424825), (4, 0.1276398455015445), (8, 0.12664932287125125), (7, 0.12134822575017166), (20, 0.11820666359706664), (80, 0.11160814466338519), (72, 0.11093253509151416), (65, 0.10577561847962542), (5, 0.10391577350659817), (56, 0.10274757373347235), (81, 0.09612660654385377), (75, 0.09126732782717942), (22, 0.08840587031229735), (87, 0.08720856602468664), (61, 0.08596675287010462), (95, 0.08394231615604081), (24, 0.08047362462297934), (55, 0.07859140230064143), (60, 0.07672419157354636), (31, 0.07384361322328911), (1, 0.07243063704036817), (76, 0.06784715225843964), (96, 0.06694708657493495), (35, 0.060850396160388974), (90, 0.06022285534874891), (101, 0.04339239116415433), (97, 0.034870408756003377), (85, 0.032652657224531256), (11, 0.020732744103582623)]\n"
     ]
    }
   ],
   "source": [
    "rf.print_featureImportances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_SupportVector_regression:\n",
    "    def __init__(self, weight=None, alpha=None, bias=None):\n",
    "        self.x = alpha\n",
    "        self.w = weight\n",
    "        self.bias = bias\n",
    "        self.Nabla = None\n",
    "                \n",
    "    def SMO2_ab(self, n, H, f, a, LB, UB, maxiter, eps, alpha_s):\n",
    "        \"\"\"\n",
    "        % min_{x} .5 x H x + f' x \n",
    "        %         LB <= x <= UB\n",
    "        %         a' x = b\n",
    "        % n         grandezza problema length(x)\n",
    "        % maxiter   max num it\n",
    "        % eps       precisione\n",
    "        % alpha_s   punto di inizio valido per x\n",
    "        % Nabla     ....\n",
    "        % err       flag di ok\n",
    "        % x         valore della soluzione ottima\n",
    "        % bias      ....\n",
    "        \"\"\"\n",
    "        self.x = alpha_s\n",
    "        self.Nabla = f\n",
    "        for i in range(n):\n",
    "            if (self.x[i] != 0.0):\n",
    "                for j in range(n):\n",
    "                    self.Nabla[j] += H[j,i] * self.x[i]\n",
    "        iter_ = 0\n",
    "        while True:\n",
    "            minF_up = float(\"inf\");\n",
    "            maxF_low = float(\"-inf\");\n",
    "            for i in range(n): \n",
    "                F_i = self.Nabla[i]/a[i]\n",
    "                if (LB[i] < self.x[i]) and (self.x[i] < UB[i]) :\n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == LB[i])) or ((a[i] < 0) and (self.x[i] == UB[i]))) : \n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == UB[i])) or ((a[i] < 0) and (self.x[i] == LB[i]))) : \n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "            if ((maxF_low - minF_up) <= eps):\n",
    "                err = 0.0\n",
    "                break\n",
    "\n",
    "            iter_ += 1\n",
    "            if (iter_ >= maxiter):\n",
    "                err = 1.0\n",
    "                break\n",
    "\n",
    "            if (a[u] > 0):\n",
    "                tau_lb = (LB[u]-self.x[u])*a[u] \n",
    "                tau_ub = (UB[u]-self.x[u])*a[u] \n",
    "            else:\n",
    "                tau_ub = (LB[u]-self.x[u])*a[u] \n",
    "                tau_lb = (UB[u]-self.x[u])*a[u]\n",
    "\n",
    "            if (a[v] > 0):\n",
    "                tau_lb = max(tau_lb,(self.x[v]-UB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-LB[v])*a[v]) \n",
    "            else:\n",
    "                tau_lb = max(tau_lb,(self.x[v]-LB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-UB[v])*a[v])\n",
    "\n",
    "            tau = (self.Nabla[v]/a[v]-self.Nabla[u]/a[u])/(H[u,u]/(a[u]*a[u])\n",
    "                                                           +H[v,v]/(a[v]*a[v])\n",
    "                                                           -2*H[v,u]/(a[u]*a[v]))\n",
    "            tau = min(max(tau,tau_lb),tau_ub)\n",
    "            self.x[u] += tau/a[u]\n",
    "            self.x[v] -= tau/a[v]\n",
    "\n",
    "            for i in range(n):\n",
    "                self.Nabla[i] += H[u,i]*tau/a[u] - H[v,i]*tau/a[v]\n",
    "\n",
    "        tsv = 0\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for k in range(n):\n",
    "            if ((self.x[k] > LB[k]) and (self.x[k] < UB[k])):\n",
    "                self.bias -= self.Nabla[k]/a[k]\n",
    "                tsv += 1\n",
    "\n",
    "        if (tsv > 0):\n",
    "            self.bias /= tsv\n",
    "        else:    \n",
    "            self.bias = -(maxF_low + minF_up)/2.0\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def fit(self, X, y, C):\n",
    "        n = X.shape[0]\n",
    "        cov = np.matmul(X, X.T)\n",
    "        Q = np.matmul(np.matmul(np.diag(y.flatten()), cov),\n",
    "                      np.diag(y.flatten()))\n",
    "        \n",
    "        if self.SMO2_ab(n,Q,-np.ones(n),y.flatten(),\n",
    "                   np.zeros(n),C*np.ones(n),10000000,.0001,np.zeros(n)):\n",
    "            print(\"Problem in SMO\")\n",
    "            \n",
    "        self.w = np.matmul(np.matmul(X.T, np.diag(y.flatten())),\n",
    "                           self.x)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.matmul(X, self.w) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 5.944963947364811\n",
      "Root Mean Square Error: 9.099746716634384\n",
      "R^2 score: -2283.277002086332\n"
     ]
    }
   ],
   "source": [
    "lsvr = linear_SupportVector_regression()\n",
    "lsvr.fit(trainVal_data, trainVal_values, C=1.0);\n",
    "\n",
    "pred = lsvr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  63,   84,   95,   98,  109,  110,  148,  230,  278,  299,  356,\n",
       "         364,  367,  404,  435,  482,  536,  564,  580,  598,  609,  622,\n",
       "         684,  716,  811,  832,  833,  900,  909,  993, 1004, 1020, 1091,\n",
       "        1097, 1113, 1115, 1188, 1204, 1255, 1303, 1318, 1433, 1449, 1479,\n",
       "        1493]),)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lsvr.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel=\"linear\", tol=.0001, C=1)\n",
    "svr.fit(trainVal_data, trainVal_values.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(svr.dual_coef_)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07147236509248299\n",
      "Root Mean Square Error: 0.11212491216741184\n",
      "R^2 score: 0.6531877808500957\n"
     ]
    }
   ],
   "source": [
    "pred = svr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
