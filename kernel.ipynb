{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cross validation purposes: create the cartesian product between the chosen values sets\n",
    "from itertools import product \n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    # for each column that contain at least 1 NaN value...\n",
    "    for nanCol in np.unique(np.where(pred_features.isna())[1]):\n",
    "        nanRows = np.where(pred_features.iloc[:,nanCol].isna())[0] #find NaN rows for the current column\n",
    "        available = df.iloc[~nanRows, nanCol]\n",
    "        value = value_withStrategy(available, strategy) #compute the filling value\n",
    "        df.iloc[nanRows, nanCol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- TBD <br>\n",
    "A thourough study from scratch of outliers detection is needed here, but for now it feels like it exceeds the course final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values rows on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_naSample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = \"robbPerPop\"\n",
    "data,values = drop_naSample(pred_features, regr_values[dep_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTest_split(in_matrix, out_vect, train_amount=0.7):\n",
    "    n,_ = in_matrix.shape\n",
    "\n",
    "    trVl_Amount = int(n*train_amount) #training-validation amount\n",
    "    indexes = np.random.permutation(n)\n",
    "    idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "    idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "    return in_matrix[idxTrVl], in_matrix[idxTs], out_vect[idxTrVl], out_vect[idxTs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal_data, test_data, trainVal_values, test_values = trainTest_split(data, values, train_amount=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_evaluationMetric:\n",
    "    def __init__(self, true, predicted):\n",
    "        self.true = true.flatten()\n",
    "        self.predicted = predicted.flatten()\n",
    "        self.residuals = self.true-self.predicted\n",
    "    \n",
    "    def meanSquareError(self):\n",
    "        return np.mean(np.square(self.residuals))\n",
    "    \n",
    "    def rootMeanSquareError(self):\n",
    "        return np.sqrt(np.mean(np.square(self.residuals)))\n",
    "    \n",
    "    def meanAbsoluteError(self):\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def rSquared(self):\n",
    "        ss_residual = np.sum(np.square(self.residuals))\n",
    "        ss_total = np.sum(np.square(self.true-np.mean(self.true)))        \n",
    "        return 1 - ss_residual/ss_total\n",
    "    \n",
    "    def adjusted_rSquared(self, p):\n",
    "        n = self.true.shape[0]\n",
    "        return 1-(1-self.rSquared())*((n-1)/(n-p-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variable Selection - Models Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def kFold_crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor, verbose=False):\n",
    "    nVal,_ = train_data.shape\n",
    "    \n",
    "    # Validation indexes adjustment -------------------------------\n",
    "    elemPerFold, remainder = np.divmod(nVal,k) #the remainder will be distributed across the firsts folds\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    # in each fold put as many samples as the division quotient +1 if the remainder is still positive\n",
    "    # then decrease the division remainder by 1\n",
    "    for i in range(k): \n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation --------------------------------------------\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values())) # build all the hyp-par combination\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList): # for each fold\n",
    "        if verbose: print(\"#{} fold:\".format(row+1))\n",
    "        for col, params in enumerate(params_product):\n",
    "            \n",
    "            if verbose:\n",
    "                update = col*100/len(params_product) # just print completion rate\n",
    "                print(\"\\t[\"+\"#\"*(int(update/5))+\" \"*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)} # {argument_name:argument_value, ... }\n",
    "            \n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            rem = Regression_evaluationMetric(trainVal_values[valIdx], pred)\n",
    "            #val_results[row,col] = rem.rSquared()\n",
    "            val_results[row,col] = rem.rootMeanSquareError()\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit - Not Working (use the sklearn model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            # project each column on the current residuals\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            # find the most correlated variable: the one that in norm maximise the projections\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "            \n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance # update the solution vector: canonical basis over the found column\n",
    "            residual -= np.matmul(data_matrix, self.weights) # update the residual\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)\n",
    "    \n",
    "    \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.weights)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(zip(range(d),self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]\n",
    "\n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "        return sorted(zip(columns, self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NumStreet', array([-4.64867656e+15])),\n",
       " ('NumKidsBornNeverMar', array([33.84253309])),\n",
       " ('population', array([0.])),\n",
       " ('householdsize', array([0.])),\n",
       " ('racepctblack', array([0.])),\n",
       " ('racePctWhite', array([0.])),\n",
       " ('racePctAsian', array([0.])),\n",
       " ('racePctHisp', array([0.])),\n",
       " ('agePct12t21', array([0.])),\n",
       " ('agePct12t29', array([0.])),\n",
       " ('agePct16t24', array([0.])),\n",
       " ('agePct65up', array([0.])),\n",
       " ('numbUrban', array([0.])),\n",
       " ('pctUrban', array([0.])),\n",
       " ('medIncome', array([0.])),\n",
       " ('pctWWage', array([0.])),\n",
       " ('pctWFarmSelf', array([0.])),\n",
       " ('pctWInvInc', array([0.])),\n",
       " ('pctWSocSec', array([0.])),\n",
       " ('pctWPubAsst', array([0.])),\n",
       " ('pctWRetire', array([0.])),\n",
       " ('medFamInc', array([0.])),\n",
       " ('perCapInc', array([0.])),\n",
       " ('whitePerCap', array([0.])),\n",
       " ('blackPerCap', array([0.])),\n",
       " ('indianPerCap', array([0.])),\n",
       " ('AsianPerCap', array([0.])),\n",
       " ('OtherPerCap', array([0.])),\n",
       " ('HispPerCap', array([0.])),\n",
       " ('NumUnderPov', array([0.])),\n",
       " ('PctPopUnderPov', array([0.])),\n",
       " ('PctLess9thGrade', array([0.])),\n",
       " ('PctNotHSGrad', array([0.])),\n",
       " ('PctBSorMore', array([0.])),\n",
       " ('PctUnemployed', array([0.])),\n",
       " ('PctEmploy', array([0.])),\n",
       " ('PctEmplManu', array([0.])),\n",
       " ('PctEmplProfServ', array([0.])),\n",
       " ('PctOccupManu', array([0.])),\n",
       " ('PctOccupMgmtProf', array([0.])),\n",
       " ('MalePctDivorce', array([0.])),\n",
       " ('MalePctNevMarr', array([0.])),\n",
       " ('FemalePctDiv', array([0.])),\n",
       " ('TotalPctDiv', array([0.])),\n",
       " ('PersPerFam', array([0.])),\n",
       " ('PctFam2Par', array([0.])),\n",
       " ('PctKids2Par', array([0.])),\n",
       " ('PctYoungKids2Par', array([0.])),\n",
       " ('PctTeen2Par', array([0.])),\n",
       " ('PctWorkMomYoungKids', array([0.])),\n",
       " ('PctWorkMom', array([0.])),\n",
       " ('PctKidsBornNeverMar', array([0.])),\n",
       " ('NumImmig', array([0.])),\n",
       " ('PctImmigRecent', array([0.])),\n",
       " ('PctImmigRec5', array([0.])),\n",
       " ('PctImmigRec8', array([0.])),\n",
       " ('PctImmigRec10', array([0.])),\n",
       " ('PctRecentImmig', array([0.])),\n",
       " ('PctRecImmig5', array([0.])),\n",
       " ('PctRecImmig8', array([0.])),\n",
       " ('PctRecImmig10', array([0.])),\n",
       " ('PctSpeakEnglOnly', array([0.])),\n",
       " ('PctNotSpeakEnglWell', array([0.])),\n",
       " ('PctLargHouseFam', array([0.])),\n",
       " ('PctLargHouseOccup', array([0.])),\n",
       " ('PersPerOccupHous', array([0.])),\n",
       " ('PersPerOwnOccHous', array([0.])),\n",
       " ('PersPerRentOccHous', array([0.])),\n",
       " ('PctPersOwnOccup', array([0.])),\n",
       " ('PctPersDenseHous', array([0.])),\n",
       " ('PctHousLess3BR', array([0.])),\n",
       " ('MedNumBR', array([0.])),\n",
       " ('HousVacant', array([0.])),\n",
       " ('PctHousOccup', array([0.])),\n",
       " ('PctHousOwnOcc', array([0.])),\n",
       " ('PctVacantBoarded', array([0.])),\n",
       " ('PctVacMore6Mos', array([0.])),\n",
       " ('MedYrHousBuilt', array([0.])),\n",
       " ('PctHousNoPhone', array([0.])),\n",
       " ('PctWOFullPlumb', array([0.])),\n",
       " ('OwnOccLowQuart', array([0.])),\n",
       " ('OwnOccMedVal', array([0.])),\n",
       " ('OwnOccHiQuart', array([0.])),\n",
       " ('OwnOccQrange', array([0.])),\n",
       " ('RentLowQ', array([0.])),\n",
       " ('RentMedian', array([0.])),\n",
       " ('RentHighQ', array([0.])),\n",
       " ('RentQrange', array([0.])),\n",
       " ('MedRent', array([0.])),\n",
       " ('MedRentPctHousInc', array([0.])),\n",
       " ('MedOwnCostPctInc', array([0.])),\n",
       " ('MedOwnCostPctIncNoMtg', array([0.])),\n",
       " ('NumInShelters', array([0.])),\n",
       " ('PctForeignBorn', array([0.])),\n",
       " ('PctBornSameState', array([0.])),\n",
       " ('PctSameHouse85', array([0.])),\n",
       " ('PctSameCity85', array([0.])),\n",
       " ('PctSameState85', array([0.])),\n",
       " ('LandArea', array([0.])),\n",
       " ('PopDens', array([0.])),\n",
       " ('PctUsePubTrans', array([0.])),\n",
       " ('LemasPctOfficDrugUn', array([0.]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "mp.sort_featureImportances(columns=pred_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 6.083548782132396e+27\n",
      "Root Mean Square Error: 4636771543543148.0\n",
      "R^2 score: -5.155517822168845e+32\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 38, 50, 67, 76, 77, 92, 93, 94])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07258847552889652\n",
      "Root Mean Square Error: 0.11645584431344774\n",
      "R^2 score: 0.6747903137943816\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso) with Proximal Gradient - Not Working (use the sklearn model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression: # Iterative Soft Thresholding Algorithm\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        n,d = data_matrix.shape\n",
    "        self.weights = np.zeros((d,1))\n",
    "        \n",
    "        # convergence step-size: n/(2*||X^t*X||_2)\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # gradient step of the mse formulation\n",
    "            resid = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            grad_descent = (step/n)*np.matmul(data_matrix.T, resid)\n",
    "            self.weights -= 2*grad_descent\n",
    "\n",
    "            # proximal operator\n",
    "            upper = self.weights > softTresh  # elem to be reduced\n",
    "            lower = self.weights < -softTresh # elem to be increased\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)\n",
    "    \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.weights)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(zip(range(d),self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]\n",
    "\n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "        return sorted(zip(columns, self.weights), key=lambda kv: abs(kv[1]), reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NumKidsBornNeverMar', array([0.09918527])),\n",
       " ('NumInShelters', array([0.08315037])),\n",
       " ('NumStreet', array([0.08102814])),\n",
       " ('NumUnderPov', array([0.07596358])),\n",
       " ('NumImmig', array([0.0717318])),\n",
       " ('population', array([0.06103532])),\n",
       " ('numbUrban', array([0.05567671])),\n",
       " ('HousVacant', array([0.04718719])),\n",
       " ('LemasPctOfficDrugUn', array([0.02893266])),\n",
       " ('LandArea', array([0.00852695])),\n",
       " ('householdsize', array([0.])),\n",
       " ('racepctblack', array([0.])),\n",
       " ('racePctWhite', array([0.])),\n",
       " ('racePctAsian', array([0.])),\n",
       " ('racePctHisp', array([0.])),\n",
       " ('agePct12t21', array([0.])),\n",
       " ('agePct12t29', array([0.])),\n",
       " ('agePct16t24', array([0.])),\n",
       " ('agePct65up', array([0.])),\n",
       " ('pctUrban', array([0.])),\n",
       " ('medIncome', array([0.])),\n",
       " ('pctWWage', array([0.])),\n",
       " ('pctWFarmSelf', array([0.])),\n",
       " ('pctWInvInc', array([0.])),\n",
       " ('pctWSocSec', array([0.])),\n",
       " ('pctWPubAsst', array([0.])),\n",
       " ('pctWRetire', array([0.])),\n",
       " ('medFamInc', array([0.])),\n",
       " ('perCapInc', array([0.])),\n",
       " ('whitePerCap', array([0.])),\n",
       " ('blackPerCap', array([0.])),\n",
       " ('indianPerCap', array([0.])),\n",
       " ('AsianPerCap', array([0.])),\n",
       " ('OtherPerCap', array([0.])),\n",
       " ('HispPerCap', array([0.])),\n",
       " ('PctPopUnderPov', array([0.])),\n",
       " ('PctLess9thGrade', array([0.])),\n",
       " ('PctNotHSGrad', array([0.])),\n",
       " ('PctBSorMore', array([0.])),\n",
       " ('PctUnemployed', array([0.])),\n",
       " ('PctEmploy', array([0.])),\n",
       " ('PctEmplManu', array([0.])),\n",
       " ('PctEmplProfServ', array([0.])),\n",
       " ('PctOccupManu', array([0.])),\n",
       " ('PctOccupMgmtProf', array([0.])),\n",
       " ('MalePctDivorce', array([0.])),\n",
       " ('MalePctNevMarr', array([0.])),\n",
       " ('FemalePctDiv', array([0.])),\n",
       " ('TotalPctDiv', array([0.])),\n",
       " ('PersPerFam', array([0.])),\n",
       " ('PctFam2Par', array([0.])),\n",
       " ('PctKids2Par', array([0.])),\n",
       " ('PctYoungKids2Par', array([0.])),\n",
       " ('PctTeen2Par', array([0.])),\n",
       " ('PctWorkMomYoungKids', array([0.])),\n",
       " ('PctWorkMom', array([0.])),\n",
       " ('PctKidsBornNeverMar', array([0.])),\n",
       " ('PctImmigRecent', array([0.])),\n",
       " ('PctImmigRec5', array([0.])),\n",
       " ('PctImmigRec8', array([0.])),\n",
       " ('PctImmigRec10', array([0.])),\n",
       " ('PctRecentImmig', array([0.])),\n",
       " ('PctRecImmig5', array([0.])),\n",
       " ('PctRecImmig8', array([0.])),\n",
       " ('PctRecImmig10', array([0.])),\n",
       " ('PctSpeakEnglOnly', array([0.])),\n",
       " ('PctNotSpeakEnglWell', array([0.])),\n",
       " ('PctLargHouseFam', array([0.])),\n",
       " ('PctLargHouseOccup', array([0.])),\n",
       " ('PersPerOccupHous', array([0.])),\n",
       " ('PersPerOwnOccHous', array([0.])),\n",
       " ('PersPerRentOccHous', array([0.])),\n",
       " ('PctPersOwnOccup', array([0.])),\n",
       " ('PctPersDenseHous', array([0.])),\n",
       " ('PctHousLess3BR', array([0.])),\n",
       " ('MedNumBR', array([0.])),\n",
       " ('PctHousOccup', array([0.])),\n",
       " ('PctHousOwnOcc', array([0.])),\n",
       " ('PctVacantBoarded', array([0.])),\n",
       " ('PctVacMore6Mos', array([0.])),\n",
       " ('MedYrHousBuilt', array([0.])),\n",
       " ('PctHousNoPhone', array([0.])),\n",
       " ('PctWOFullPlumb', array([0.])),\n",
       " ('OwnOccLowQuart', array([0.])),\n",
       " ('OwnOccMedVal', array([0.])),\n",
       " ('OwnOccHiQuart', array([0.])),\n",
       " ('OwnOccQrange', array([0.])),\n",
       " ('RentLowQ', array([0.])),\n",
       " ('RentMedian', array([0.])),\n",
       " ('RentHighQ', array([0.])),\n",
       " ('RentQrange', array([0.])),\n",
       " ('MedRent', array([0.])),\n",
       " ('MedRentPctHousInc', array([0.])),\n",
       " ('MedOwnCostPctInc', array([0.])),\n",
       " ('MedOwnCostPctIncNoMtg', array([0.])),\n",
       " ('PctForeignBorn', array([0.])),\n",
       " ('PctBornSameState', array([0.])),\n",
       " ('PctSameHouse85', array([0.])),\n",
       " ('PctSameCity85', array([0.])),\n",
       " ('PctSameState85', array([0.])),\n",
       " ('PopDens', array([0.])),\n",
       " ('PctUsePubTrans', array([0.]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=1000)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.5)\n",
    "lr.sort_featureImportances(columns=pred_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.03854860375202811\n",
      "Root Mean Square Error: 0.31678516219727826\n",
      "R^2 score: -1.4064151814525911\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  76,  93,  94, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.0650983258776175\n",
      "Root Mean Square Error: 0.12289048966747763\n",
      "R^2 score: 0.6378592097328893\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Decision Tree class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor: # Least Square Regression Tree with either fixed parameter or pruning\n",
    "    class Node:\n",
    "        def __init__(self, isLeaf=False, feature=None, feature_importance=None, cut=None, average=None,\n",
    "                     left=None, right=None):\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature # if internal, on wich feature it executes the split\n",
    "            self.feature_importance = feature_importance # solution variance reduction\n",
    "            self.cut = cut # if internal, threahold value for the cut\n",
    "            self.avg = average # mean of seen training values\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            if self.cut:\n",
    "                print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"Leaf => {}\\n\".format(self.avg))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            if self.cut:\n",
    "                print(\"|    \"*level+\"{} => {}\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"|    \"*level+\"Leaf: {}\".format(self.avg))                \n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self, root=None, feature_importances=None):\n",
    "        self.root = root\n",
    "        self.feature_importances = feature_importances\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, depth, minElems_perLeaf, post_pruning=False):\n",
    "        \n",
    "        self.feature_importances = {k:0 for k in range(X.shape[1])}\n",
    "        \n",
    "        if not post_pruning:\n",
    "            self.root = self.learn(X, y.flatten(), depth, minElems_perLeaf)\n",
    "        else:\n",
    "            # train dataset, pruning dataset\n",
    "            X_trn, X_val, y_trn, y_val = trainTest_split(X, y.flatten(), train_amount=0.7)\n",
    "            self.root = self.learn(X_trn, y_trn, depth, minElems_perLeaf)\n",
    "            self.prune(X_val, y_val)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElems_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElems_perLeaf: # leaf # or fraction error of the root node??? \n",
    "            return self.Node(isLeaf=True, average=np.mean(y))\n",
    "            \n",
    "        best_costDescent = 0 # split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "                \n",
    "                if sorted_x[i2]<sorted_x[i2+1]: # for a different value\n",
    "                    # try to maximise this value: it is directly correlated \n",
    "                    # to the possible split information gain\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "                        \n",
    "        # update the importance for the selected feature\n",
    "        feature_importance = np.var(y) - (np.sum(np.square(y))-best_costDescent)/n\n",
    "        self.feature_importances[best_feature] += feature_importance\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "        \n",
    "        return self.Node(feature=best_feature, feature_importance=feature_importance,\n",
    "                         cut=best_cut, average=np.mean(y),\n",
    "                         left = self.learn(X[left_idxs], y[left_idxs], depth-1, minElems_perLeaf),\n",
    "                         right = self.learn(X[~left_idxs], y[~left_idxs], depth-1, minElems_perLeaf))\n",
    "    \n",
    "    def prune(self, X, y):\n",
    "        # for statistics purposes check errors on different dataset portions and average them\n",
    "        # in order to decide whether to prune or not (same code of k-fold cross-validation)\n",
    "        n,_ = X.shape\n",
    "        folds = 5\n",
    "        elemPerFold, remainder = np.divmod(n, folds)\n",
    "        foldsIdxsList = []\n",
    "        start = 0\n",
    "        for i in range(folds): \n",
    "            end = start+elemPerFold+int(remainder>0)\n",
    "            foldsIdxsList.append(np.arange(start,end)) \n",
    "            remainder -= 1\n",
    "            start = end\n",
    "        \n",
    "        # recursive: start checking if the root receive a possible positive pruning from its sons\n",
    "        self.test_pruning(self.root, X, y, foldsIdxsList)\n",
    "        return self\n",
    "    \n",
    "    def test_pruning(self, node, X, y, foldIdxs):\n",
    "        if node.isLeaf: # leaf: start point of new possible pruning\n",
    "            return True\n",
    "        \n",
    "        # check sons response: if one of them is negative to be pruned it means that it performs an important\n",
    "        # predictive split\n",
    "        if not self.test_pruning(node.left, X, y, foldIdxs) or not self.test_pruning(node.right, X, y, foldIdxs):\n",
    "            return False\n",
    "        \n",
    "        # else proceed with testing the goodness of the current node split\n",
    "        folds = len(foldIdxs)\n",
    "        results = np.empty(folds)\n",
    "\n",
    "        # not pruned errors on different folds\n",
    "        for i, idxs in enumerate(foldIdxs):\n",
    "            pred = self.predict(X[idxs])\n",
    "            results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "\n",
    "        not_prunErr = np.mean(results)\n",
    "\n",
    "        # pruned errors on different folds\n",
    "        node.isLeaf = True\n",
    "        for i, idxs in enumerate(foldIdxs):\n",
    "            pred = self.predict(X[idxs])\n",
    "            results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "\n",
    "        # if pruning improves the prediction RMSE then keep current node as leaf\n",
    "        node.isLeaf = np.mean(results) <= not_prunErr\n",
    "        \n",
    "        if node.isLeaf:\n",
    "            # lower feature importance computed during training phase\n",
    "            self.feature_importances[node.feature] -= node.feature_importance \n",
    "            node.left = None\n",
    "            node.right = None\n",
    "            \n",
    "        return node.isLeaf\n",
    "            \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n,_ = X.shape\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.cut:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.avg\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()\n",
    "        \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.feature_importances is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.feature_importances)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(self.feature_importances.items(), key=lambda kv: kv[1], reverse=True)[:n_printFeat]\n",
    "            \n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "            \n",
    "        return sorted(zip(map(lambda kv: round(kv[1], 4), self.feature_importances.items()), columns),\n",
    "                      reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    Leaf: 0.766500451240285\n",
      "|    |    |    5 => -0.6932856992749816\n",
      "|    |    |    |    Leaf: 0.14047632718380415\n",
      "|    |    50 => 0.05155393053016444\n",
      "|    |    |    |    |    Leaf: -0.45882966084102944\n",
      "|    |    |    |    9 => -0.6037957346898846\n",
      "|    |    |    |    |    Leaf: -0.14818530738075997\n",
      "|    |    |    74 => -0.857107044372023\n",
      "|    |    |    |    |    Leaf: -0.4516628167699441\n",
      "|    |    |    |    51 => -0.9824548432458228\n",
      "|    |    |    |    |    Leaf: -0.7659917937574255\n",
      "|    100 => -0.7765507086324315\n",
      "|    |    |    |    |    Leaf: -0.5273981617663297\n",
      "|    |    |    |    74 => -0.5083980947605915\n",
      "|    |    |    |    |    Leaf: -0.7405081863673907\n",
      "|    |    |    3 => 0.3959773078906653\n",
      "|    |    |    |    |    Leaf: -0.5015688308697103\n",
      "|    |    |    |    75 => -0.695481584609543\n",
      "|    |    |    |    |    Leaf: 0.01874597895586087\n",
      "|    |    100 => -0.9582182956009571\n",
      "|    |    |    |    Leaf: -0.5742364616872707\n",
      "|    |    |    38 => 0.22228412256267427\n",
      "|    |    |    |    |    Leaf: -0.7181404483547029\n",
      "|    |    |    |    91 => -0.9992729760937433\n",
      "|    |    |    |    |    Leaf: -0.817645188217991\n",
      "50 => -0.563436928702011\n",
      "|    |    |    |    |    Leaf: -0.9162503919827926\n",
      "|    |    |    |    88 => 0.024752475247524552\n",
      "|    |    |    |    |    Leaf: -0.9579053798555328\n",
      "|    |    |    46 => 0.4182006740990406\n",
      "|    |    |    |    |    Leaf: -0.8355924792304328\n",
      "|    |    |    |    49 => -0.9928007779254184\n",
      "|    |    |    |    |    Leaf: -0.8862181192347747\n",
      "|    |    3 => 0.7680247550283652\n",
      "|    |    |    |    |    Leaf: -0.6664791038205992\n",
      "|    |    |    |    46 => 0.21350790770028516\n",
      "|    |    |    |    |    Leaf: -0.3749334181341178\n",
      "|    |    |    99 => -0.6887057636946261\n",
      "|    |    |    |    |    Leaf: -0.6525940763502787\n",
      "|    |    |    |    49 => -0.9827696343712622\n",
      "|    |    |    |    |    Leaf: -0.8032177936362267\n",
      "|    49 => -0.9972856013662978\n",
      "|    |    |    |    Leaf: -0.40801691304533455\n",
      "|    |    |    93 => -0.04500166057788102\n",
      "|    |    |    |    |    Leaf: -0.8992898670423811\n",
      "|    |    |    |    41 => 0.1123711340206186\n",
      "|    |    |    |    |    Leaf: -0.9653649515933285\n",
      "|    |    3 => 0.7875193398659103\n",
      "|    |    |    |    |    Leaf: -0.90506738779094\n",
      "|    |    |    |    44 => 0.49030694668820707\n",
      "|    |    |    |    |    Leaf: -0.8103003262965367\n",
      "|    |    |    11 => 0.9913\n",
      "|    |    |    |    |    Leaf: -0.9036374983904498\n",
      "|    |    |    |    49 => -0.9986541738617818\n",
      "|    |    |    |    |    Leaf: -0.9522837848154858\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElems_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07635312827226187\n",
      "Root Mean Square Error: 0.1306523054982993\n",
      "R^2 score: 0.5906686092960889\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 0.08311649250859615),\n",
       " (5, 0.07348241323137844),\n",
       " (100, 0.04438942353974491),\n",
       " (74, 0.0331455807419392),\n",
       " (51, 0.024196581005876662),\n",
       " (9, 0.022205765955201847),\n",
       " (46, 0.016739106557718784),\n",
       " (3, 0.016729867187264092),\n",
       " (75, 0.013498878182351147),\n",
       " (49, 0.006646502364549988),\n",
       " (38, 0.0032777433233030402),\n",
       " (99, 0.003219076033295288),\n",
       " (91, 0.0023686727901512505),\n",
       " (44, 0.0022201666760915725),\n",
       " (93, 0.0012203892102816082),\n",
       " (11, 0.0010863165631433518),\n",
       " (88, 0.0003611883367334296),\n",
       " (41, 0.000262841816613571),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (2, 0),\n",
       " (4, 0),\n",
       " (6, 0),\n",
       " (7, 0),\n",
       " (8, 0),\n",
       " (10, 0),\n",
       " (12, 0),\n",
       " (13, 0),\n",
       " (14, 0),\n",
       " (15, 0),\n",
       " (16, 0),\n",
       " (17, 0),\n",
       " (18, 0),\n",
       " (19, 0),\n",
       " (20, 0),\n",
       " (21, 0),\n",
       " (22, 0),\n",
       " (23, 0),\n",
       " (24, 0),\n",
       " (25, 0),\n",
       " (26, 0),\n",
       " (27, 0),\n",
       " (28, 0),\n",
       " (29, 0),\n",
       " (30, 0),\n",
       " (31, 0),\n",
       " (32, 0),\n",
       " (33, 0),\n",
       " (34, 0),\n",
       " (35, 0),\n",
       " (36, 0),\n",
       " (37, 0),\n",
       " (39, 0),\n",
       " (40, 0),\n",
       " (42, 0),\n",
       " (43, 0),\n",
       " (45, 0),\n",
       " (47, 0),\n",
       " (48, 0),\n",
       " (52, 0),\n",
       " (53, 0),\n",
       " (54, 0),\n",
       " (55, 0),\n",
       " (56, 0),\n",
       " (57, 0),\n",
       " (58, 0),\n",
       " (59, 0),\n",
       " (60, 0),\n",
       " (61, 0),\n",
       " (62, 0),\n",
       " (63, 0),\n",
       " (64, 0),\n",
       " (65, 0),\n",
       " (66, 0),\n",
       " (67, 0),\n",
       " (68, 0),\n",
       " (69, 0),\n",
       " (70, 0),\n",
       " (71, 0),\n",
       " (72, 0),\n",
       " (73, 0),\n",
       " (76, 0),\n",
       " (77, 0),\n",
       " (78, 0),\n",
       " (79, 0),\n",
       " (80, 0),\n",
       " (81, 0),\n",
       " (82, 0),\n",
       " (83, 0),\n",
       " (84, 0),\n",
       " (85, 0),\n",
       " (86, 0),\n",
       " (87, 0),\n",
       " (89, 0),\n",
       " (90, 0),\n",
       " (92, 0),\n",
       " (94, 0),\n",
       " (95, 0),\n",
       " (96, 0),\n",
       " (97, 0),\n",
       " (98, 0),\n",
       " (101, 0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndt.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    Leaf: 0.3255962432467314\n",
      "|    |    50 => 0.05155393053016444\n",
      "|    |    |    |    Leaf: -0.6608422913626236\n",
      "|    |    |    22 => -0.8902413728078447\n",
      "|    |    |    |    |    Leaf: 0.05384408139108615\n",
      "|    |    |    |    92 => -0.9429501292237006\n",
      "|    |    |    |    |    |    Leaf: -0.057635824798046104\n",
      "|    |    |    |    |    88 => 0.7475247524752471\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.4325149174296529\n",
      "|    |    |    |    |    |    |    56 => -0.14587892049598838\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.2616077698718714\n",
      "|    |    |    |    |    |    |    |    38 => -0.1392757660167131\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.030285363472945415\n",
      "|    |    |    |    |    |    25 => -0.9074087591240876\n",
      "|    |    |    |    |    |    |    Leaf: -0.5452045598088449\n",
      "|    100 => -0.7765507086324315\n",
      "|    |    |    Leaf: -0.18100697987011938\n",
      "|    |    69 => 0.4337884698742954\n",
      "|    |    |    |    |    |    Leaf: -0.2503080653496045\n",
      "|    |    |    |    |    68 => -0.536170928667564\n",
      "|    |    |    |    |    |    |    Leaf: -0.3853378560418351\n",
      "|    |    |    |    |    |    48 => 0.003795866722901553\n",
      "|    |    |    |    |    |    |    Leaf: -0.4725466924854769\n",
      "|    |    |    |    89 => -0.21390374331550815\n",
      "|    |    |    |    |    |    |    Leaf: -0.3904369448750734\n",
      "|    |    |    |    |    |    97 => 0.829581034739824\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.45364444621112743\n",
      "|    |    |    |    |    |    |    13 => 0.44483712354025795\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.5784583924067965\n",
      "|    |    |    |    |    |    |    |    |    32 => -0.4817786693413574\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.641631001753433\n",
      "|    |    |    |    |    |    |    |    88 => -0.009900990099010076\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7758476765910085\n",
      "|    |    |    |    |    22 => -0.9467188383933622\n",
      "|    |    |    |    |    |    Leaf: -0.776450998838406\n",
      "|    |    |    49 => -0.9917961471461851\n",
      "|    |    |    |    |    |    Leaf: -0.5944520264590225\n",
      "|    |    |    |    |    69 => 0.31014304291287376\n",
      "|    |    |    |    |    |    |    Leaf: -0.6901538339229638\n",
      "|    |    |    |    |    |    10 => -0.9926421947285131\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8530693908918656\n",
      "|    |    |    |    |    |    |    |    |    16 => -0.16261525565800494\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7758369503013142\n",
      "|    |    |    |    |    |    |    |    9 => -0.5388378008217571\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8082177260139656\n",
      "|    |    |    |    |    |    |    |    |    |    64 => -0.07482993197278887\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8703055919933926\n",
      "|    |    |    |    |    |    |    |    |    |    |    89 => -0.45454545454545464\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8476412573482971\n",
      "|    |    |    |    |    |    |    |    |    75 => -0.4314643715985318\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9165831172827238\n",
      "|    |    |    |    |    |    |    24 => -0.9511891866434323\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6809149651300941\n",
      "|    |    |    |    89 => -0.732620320855615\n",
      "|    |    |    |    |    Leaf: -0.40712768259773074\n",
      "50 => -0.563436928702011\n",
      "|    |    |    |    Leaf: -0.5123424891680248\n",
      "|    |    |    5 => -0.13008300935168643\n",
      "|    |    |    |    |    Leaf: -0.7194154045924925\n",
      "|    |    |    |    79 => 0.6228840293442088\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8885532191172768\n",
      "|    |    |    |    |    |    |    |    |    |    16 => -0.16722548197820608\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9262476094570542\n",
      "|    |    |    |    |    |    |    |    |    |    |    2 => -0.9416571842350263\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9603909669497777\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    25 => -0.8006204379562043\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9872379236174602\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    17 => -0.8613351254480286\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9722964474458426\n",
      "|    |    |    |    |    |    |    |    |    0 => -0.9919274497477559\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9206317658438341\n",
      "|    |    |    |    |    |    |    |    |    |    2 => -0.8699700010344471\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9535494869994214\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    26 => 0.4154772361294101\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9947882851249708\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    86 => -0.2503113325031133\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9902479097931657\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    45 => 0.932092004381161\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9819165271723206\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    67 => 0.7703757051974554\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9606957197687411\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    91 => -0.9979899927297609\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9847676591008467\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    69 => -0.28045080190723887\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9775336810754389\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    50 => -0.9429616087751371\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.972750681277135\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    4 => -0.9540309942538743\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9795188438826393\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9963628327648365\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9904216336223333\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    68 => -0.985699865410498\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9909512704659185\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    6 => -0.7892412685668406\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9621841502033893\n",
      "|    |    |    |    |    |    |    |    |    |    |    88 => -0.2029702970297032\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9725693018215975\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    23 => -0.9379645833333333\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9565886725192949\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    34 => -0.496040016673614\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9849319606206357\n",
      "|    |    |    |    |    |    |    |    69 => -0.8446033810143043\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8991371225739394\n",
      "|    |    |    |    |    |    |    43 => 0.7036669970267594\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8450501222691865\n",
      "|    |    |    |    |    |    |    |    |    |    |    45 => 0.8065717415115005\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8513733752037207\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    55 => 0.08022727272727281\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9362448269313157\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    31 => -0.5841392649903289\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8954624219162916\n",
      "|    |    |    |    |    |    |    |    |    |    40 => 0.09820126397666507\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9276013303123054\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    5 => -0.8989177261742145\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9629059664039987\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    88 => 0.1485148514851483\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9383118460512427\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    22 => -0.9022628700735433\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.957751984205854\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    32 => -0.8124373119358075\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9144105178210993\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    89 => -0.6042780748663104\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8738823889676536\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    16 => -0.3872590108968985\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9374673715731872\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    34 => -0.6586077532305128\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8650077513216998\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    2 => -0.9178649012103031\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9871473811132753\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    56 => -0.6163384390955507\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9401465463555538\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    26 => -0.5564705021226761\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9564159301806875\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    48 => 0.3317165752846898\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9618528971392984\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    5 => -0.9696332878007776\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9295976821118929\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    50 => -0.9191956124314442\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9707908409263898\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    24 => -0.8026939198417558\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9872922491199709\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    3 => 0.9050025786487881\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9691581313793819\n",
      "|    |    |    |    |    |    |    |    |    |    |    5 => -0.980666176316066\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9485641283848543\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    25 => -0.8703576642335766\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9734094577411878\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    22 => -0.8914199509711485\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9518755548488823\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    73 => 0.24613839005399996\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9824162040165538\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    2 => -0.997827661115134\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9971114732811279\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    93 => -0.9486881434739289\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9683127735598221\n",
      "|    |    |    |    |    |    |    |    |    66 => -0.9150943396226416\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7961556977735378\n",
      "|    |    |    |    |    |    |    |    88 => -0.50990099009901\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.769774703749343\n",
      "|    |    |    |    |    |    10 => -0.9965274458509341\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.908525570528194\n",
      "|    |    |    |    |    |    |    |    97 => 0.828984642910392\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9002928277086564\n",
      "|    |    |    |    |    |    |    |    |    71 => -0.9831646712826807\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9503032069713312\n",
      "|    |    |    |    |    |    |    |    |    |    |    36 => -0.3993006993006994\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9903291772115559\n",
      "|    |    |    |    |    |    |    |    |    |    13 => 0.6003380454824826\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9797008122325129\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    48 => 0.2743568114719529\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9594027286419067\n",
      "|    |    |    |    |    |    |    |    |    |    |    32 => -0.8495486459378134\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9329043238094398\n",
      "|    |    |    |    |    |    |    2 => -0.9559325540498604\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9755167179740857\n",
      "|    |    |    |    |    |    |    |    |    49 => -0.9990294887566651\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    86 => 0.20672478206724787\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    73 => 0.8771819665955044\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9902081594254747\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    61 => -0.9246021393164623\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9936063741922945\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    36 => -0.7298368298368298\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9926977102315974\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    95 => 0.0004495729057395481\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9947117288612697\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    31 => -0.1516441005802709\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9987491884299929\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    21 => -0.8453406544857838\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9893292346287538\n",
      "|    |    |    |    |    |    |    |    |    |    |    52 => -0.8974957225073884\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9849243049942658\n",
      "|    |    |    |    |    |    |    |    |    |    7 => -0.6067397349910028\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9866262096257724\n",
      "|    |    |    |    |    |    |    |    31 => -0.33668600902643464\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9220009451754095\n",
      "|    |    |    |    |    |    |    |    |    31 => -0.34609929078014195\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.985767601683649\n",
      "|    |    |    |    |    |    |    |    |    |    |    69 => -0.024707412223667125\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9986160983100204\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    15 => -0.19548239817373558\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9886755619156143\n",
      "|    |    |    |    |    |    |    |    |    |    14 => -0.34915773353751917\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.962970324142165\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    14 => -0.49157733537519144\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9978902860407014\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    31 => -0.47414571244358483\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9939204020970527\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    14 => -0.6263399693721287\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9840843944473153\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    46 => 0.5666320974850919\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9788027483698669\n",
      "|    |    |    |    |    |    |    |    |    |    |    65 => -0.13240418118466907\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9412719234319583\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    5 => -0.9628034044341705\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9807290217434511\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    99 => -0.9132155432282751\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9642991221478322\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    8 => -0.7894201424211597\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.927468829086669\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    72 => 0.9150008126117343\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9954083908609489\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    7 => -0.5599541959757893\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9737942020408132\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    94 => 0.7374696145387197\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9807721792855155\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    46 => 0.5747990666320973\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9665714130077926\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    78 => -0.8949343339587242\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9574406063256085\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    100 => -0.9869317136020614\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9750090069285522\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    64 => -0.3707482993197279\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9841604249125018\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    58 => -0.978689818468824\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9942626969299466\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    26 => -0.8970319133362612\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9591675389664021\n",
      "|    |    |    |    |    3 => 0.7890665291387314\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8902565959846239\n",
      "|    |    |    |    |    |    |    |    22 => -0.7931972468414106\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9281225017998084\n",
      "|    |    |    |    |    |    |    |    |    8 => -0.7236351305527298\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.949945453662113\n",
      "|    |    |    |    |    |    |    |    |    |    |    74 => -0.8962145901228378\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9566919744007631\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    76 => 0.7083333333333334\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9723612258228204\n",
      "|    |    |    |    |    |    |    |    |    |    23 => -0.9834125\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9175179870413801\n",
      "|    |    |    |    |    |    |    44 => 0.58817985998923\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.7996228131776886\n",
      "|    |    |    |    |    |    |    |    |    |    55 => 0.2920454545454546\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8856381921532774\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    82 => -0.08640483383685799\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.860418301461881\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    65 => -0.2508710801393729\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8317013599042458\n",
      "|    |    |    |    |    |    |    |    |    |    |    24 => -0.7600244901803797\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8589244728291514\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    25 => -0.8453138686131387\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8933000755256987\n",
      "|    |    |    |    |    |    |    |    |    2 => -0.9587255611875453\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9219715004586017\n",
      "|    |    |    |    |    |    |    |    99 => -0.9552893606724574\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9351501144242726\n",
      "|    |    |    |    |    |    10 => -0.9972203725361772\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9422868828203328\n",
      "|    |    |    |    |    |    |    |    14 => -0.4196018376722819\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9835799433525713\n",
      "|    |    |    |    |    |    |    |    |    |    62 => -0.7413742258920673\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9702437580880957\n",
      "|    |    |    |    |    |    |    |    |    67 => -0.23742647941423606\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9951416217266676\n",
      "|    |    |    |    |    |    |    44 => 0.4159935379644588\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9121406456343054\n",
      "|    |    44 => 0.3916262789445343\n",
      "|    |    |    |    |    |    Leaf: -0.7014820998205344\n",
      "|    |    |    |    |    49 => -0.9900351999878686\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9358870736220977\n",
      "|    |    |    |    |    |    |    6 => -0.4682858289843436\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7798006298224925\n",
      "|    |    |    |    |    |    |    |    |    36 => 0.05944055944055926\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8723306523918679\n",
      "|    |    |    |    |    |    |    |    |    |    18 => -0.3008323424494649\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8424719428654714\n",
      "|    |    |    |    |    |    |    |    63 => -0.8452185343411107\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.749431348906644\n",
      "|    |    |    |    |    |    86 => -0.676214196762142\n",
      "|    |    |    |    |    |    |    Leaf: -0.7585430165229029\n",
      "|    |    |    |    62 => -0.7853140666470068\n",
      "|    |    |    |    |    |    |    Leaf: -0.7633925613811927\n",
      "|    |    |    |    |    |    37 => 0.14566592579928206\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9202254287518826\n",
      "|    |    |    |    |    |    |    |    |    72 => 0.8742077035592395\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8931863453070275\n",
      "|    |    |    |    |    |    |    |    |    |    94 => 0.4463479569394606\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8553969957555442\n",
      "|    |    |    |    |    |    |    |    25 => -0.904956204379562\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9269423575501406\n",
      "|    |    |    |    |    |    |    51 => -0.9990748524540896\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9001576764585073\n",
      "|    |    |    |    |    |    |    |    |    36 => -0.23310023310023326\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9217251463279739\n",
      "|    |    |    |    |    |    |    |    2 => -0.9462087514223647\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9546094968045121\n",
      "|    |    |    |    |    44 => 0.2034194938072159\n",
      "|    |    |    |    |    |    |    Leaf: -0.7242502270923784\n",
      "|    |    |    |    |    |    45 => 0.40700985761226716\n",
      "|    |    |    |    |    |    |    Leaf: -0.8574689223282723\n",
      "|    |    |    11 => 0.9884\n",
      "|    |    |    |    |    Leaf: -0.6433067005869804\n",
      "|    |    |    |    74 => -0.6542993231386312\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.966128270019831\n",
      "|    |    |    |    |    |    |    |    88 => 0.4207920792079206\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9973632256098368\n",
      "|    |    |    |    |    |    |    |    |    77 => -0.1842546063651591\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9845533015624839\n",
      "|    |    |    |    |    |    |    16 => -0.24643755238893533\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9633177718005004\n",
      "|    |    |    |    |    |    32 => -0.679037111334002\n",
      "|    |    |    |    |    |    |    Leaf: -0.9179110740107679\n",
      "|    |    |    |    |    3 => 0.9429602888086642\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8800216124221372\n",
      "|    |    |    |    |    |    |    |    |    26 => -0.5821256038647343\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8734524961022556\n",
      "|    |    |    |    |    |    |    |    |    |    67 => 0.3399351818509181\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9341159739060919\n",
      "|    |    |    |    |    |    |    |    |    |    |    49 => -0.9975187515282709\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.977836961658562\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    98 => -0.9895486004090897\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9559418702400774\n",
      "|    |    |    |    |    |    |    |    27 => -0.9974662723226535\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8693096244473595\n",
      "|    |    |    |    |    |    |    37 => -0.39374252008890404\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.782194485298989\n",
      "|    |    |    |    |    |    |    |    21 => -0.7590173246236864\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8239986219872534\n",
      "|    |    |    |    |    |    |    |    |    66 => -0.2610062893081763\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9195805894537858\n",
      "|    |    |    |    |    |    |    |    |    |    6 => -0.6449217181854676\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8840540663890031\n",
      "|    |    |    |    |    |    0 => -0.9991957671726136\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8397265174702866\n",
      "|    |    |    |    |    |    |    21 => -0.7365174035154154\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9538881012427142\n",
      "|    |    |    |    |    |    |    |    |    |    26 => -0.7918130581174059\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9696589860122872\n",
      "|    |    |    |    |    |    |    |    |    18 => -0.5357907253269916\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9350302323629828\n",
      "|    |    |    |    |    |    |    |    85 => -0.7948717948717949\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9857450764752907\n",
      "|    3 => 0.6296028880866428\n",
      "|    |    |    |    |    Leaf: -0.4513883920092928\n",
      "|    |    |    |    21 => -0.6508725425226419\n",
      "|    |    |    |    |    |    |    Leaf: -0.8506516851947546\n",
      "|    |    |    |    |    |    6 => -0.5776796467282216\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7609697519331684\n",
      "|    |    |    |    |    |    |    0 => -0.9956279327113805\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6682681648138578\n",
      "|    |    |    |    |    35 => -0.5544646165246387\n",
      "|    |    |    |    |    |    |    Leaf: -0.7866818601405398\n",
      "|    |    |    |    |    |    87 => 0.12855377008652658\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.30385622733676954\n",
      "|    |    |    |    |    |    |    16 => -0.2627829002514668\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.5339853375125202\n",
      "|    |    |    |    |    |    |    |    23 => -0.9531270833333334\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6570554203552308\n",
      "|    |    |    50 => -0.6826325411334553\n",
      "|    |    |    |    |    |    Leaf: -0.7026127769459646\n",
      "|    |    |    |    |    49 => -0.9772252097877575\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7684997475115534\n",
      "|    |    |    |    |    |    |    |    36 => -0.4165501165501167\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8335735531479593\n",
      "|    |    |    |    |    |    |    92 => -0.9986599023643151\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.838542839854602\n",
      "|    |    |    |    |    |    |    |    |    |    21 => -0.09823598093975827\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8837787582868475\n",
      "|    |    |    |    |    |    |    |    |    |    |    51 => -0.9913390442510506\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.90980111566032\n",
      "|    |    |    |    |    |    |    |    |    53 => -0.7086397058823529\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9675636999642248\n",
      "|    |    |    |    |    |    |    |    65 => -0.22648083623693388\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8220817709230477\n",
      "|    |    |    |    |    |    14 => -0.8070444104134762\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.856786050270965\n",
      "|    |    |    |    |    |    |    48 => 0.2604386334879797\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.722564517055116\n",
      "|    |    |    |    |    |    |    |    4 => -0.87933135991642\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8180552058995434\n",
      "|    |    |    |    46 => 0.22206378014000516\n",
      "|    |    |    |    |    Leaf: -0.5751180365085043\n",
      "|    |    49 => -0.9971661829906532\n",
      "|    |    |    |    |    Leaf: -0.5115695653518129\n",
      "|    |    |    |    15 => 0.3121470623573229\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8203177379390759\n",
      "|    |    |    |    |    |    |    71 => -0.9909107750735243\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7936955916842231\n",
      "|    |    |    |    |    |    |    |    47 => 0.5406766325727774\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9076083675996225\n",
      "|    |    |    |    |    |    |    |    |    |    98 => -0.9961332623497436\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.879448618233052\n",
      "|    |    |    |    |    |    |    |    |    74 => -0.9611431436450237\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9444113191380353\n",
      "|    |    |    |    |    |    78 => -0.8911819887429644\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7349688401284378\n",
      "|    |    |    |    |    |    |    72 => 0.8051357061595971\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8535684788417627\n",
      "|    |    |    |    |    38 => -0.1604456824512534\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8848898056010722\n",
      "|    |    |    |    |    |    |    |    88 => 0.08910891089108897\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9364504492036922\n",
      "|    |    |    |    |    |    |    3 => 0.38978855079938124\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9011835303921005\n",
      "|    |    |    |    |    |    |    |    64 => 0.078231292517007\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8310226591818197\n",
      "|    |    |    |    |    |    20 => -0.8031688624817016\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9107692579489693\n",
      "|    |    |    |    |    |    |    72 => 0.8087111977896961\n",
      "|    |    |    |    |    |    |    |    73 => 0.32387291221901304\n",
      "|    |    |    46 => 0.01853772361939321\n",
      "|    |    |    |    Leaf: -0.5693533498518195\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=100, minElems_perLeaf=10, post_pruning=True)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.08351466046771418\n",
      "Root Mean Square Error: 0.14487350561966586\n",
      "R^2 score: 0.4967093827674589\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 0.09301023400398062),\n",
       " (100, 0.04225699101356177),\n",
       " (22, 0.032335379198476066),\n",
       " (49, 0.018735550923822605),\n",
       " (69, 0.017787728444085875),\n",
       " (89, 0.017751073618493422),\n",
       " (92, 0.015611295288987082),\n",
       " (88, 0.014290584536063019),\n",
       " (25, 0.014069481462683437),\n",
       " (38, 0.008768162740742342),\n",
       " (56, 0.008291475808037135),\n",
       " (21, 0.007583932249600146),\n",
       " (46, 0.007373777289016005),\n",
       " (16, 0.006238065656563634),\n",
       " (35, 0.005733342994400259),\n",
       " (3, 0.005299331528075273),\n",
       " (87, 0.0050225125442872765),\n",
       " (13, 0.0044425053344488725),\n",
       " (6, 0.004144145194785893),\n",
       " (48, 0.004134381613539004),\n",
       " (45, 0.004059408210134817),\n",
       " (23, 0.00404867666289739),\n",
       " (72, 0.003969714931068744),\n",
       " (97, 0.0034806334640175526),\n",
       " (10, 0.003035329126075655),\n",
       " (15, 0.002991282366054264),\n",
       " (44, 0.002936549537881607),\n",
       " (68, 0.002790997713146166),\n",
       " (0, 0.0026220071137632076),\n",
       " (14, 0.00221127209256381),\n",
       " (78, 0.0018894350033366542),\n",
       " (71, 0.0017574171318755494),\n",
       " (24, 0.0017317143904138194),\n",
       " (32, 0.0016811444818233146),\n",
       " (4, 0.001644894400523776),\n",
       " (86, 0.001621374957818214),\n",
       " (62, 0.0015245246679651556),\n",
       " (2, 0.0015153055970080387),\n",
       " (74, 0.0014883866525565305),\n",
       " (37, 0.001440788485342752),\n",
       " (11, 0.0013548761853017484),\n",
       " (36, 0.0013488442222441806),\n",
       " (64, 0.0012620932693898405),\n",
       " (65, 0.0011097380730255065),\n",
       " (55, 0.0010187486469382212),\n",
       " (66, 0.0009653329165916998),\n",
       " (20, 0.0009418494169147608),\n",
       " (9, 0.0008359546439738849),\n",
       " (26, 0.0008067156421322154),\n",
       " (34, 0.0007996979600544426),\n",
       " (5, 0.0007921246339681184),\n",
       " (47, 0.0007769247147077398),\n",
       " (99, 0.0005804980787753602),\n",
       " (63, 0.0005493191896707875),\n",
       " (27, 0.0005447720056761546),\n",
       " (51, 0.000534802364148433),\n",
       " (75, 0.0005200162663584173),\n",
       " (53, 0.00047663118431048564),\n",
       " (67, 0.000439078417995424),\n",
       " (31, 0.0004148341478692949),\n",
       " (8, 0.00033765659868917303),\n",
       " (94, 0.000334903173107975),\n",
       " (18, 0.0002959954979237175),\n",
       " (79, 0.00029513841096089847),\n",
       " (98, 0.0002679194631379109),\n",
       " (43, 0.0002536339813367784),\n",
       " (40, 0.00022798378117999478),\n",
       " (85, 0.00015832094986767943),\n",
       " (82, 0.00012063078836547405),\n",
       " (73, 9.565943977560452e-05),\n",
       " (7, 9.036289795877526e-05),\n",
       " (93, 5.6883728140044106e-05),\n",
       " (76, 5.230127718720406e-05),\n",
       " (17, 4.7555725246899226e-05),\n",
       " (77, 2.4410700609741058e-05),\n",
       " (58, 2.361624130252081e-05),\n",
       " (91, 1.8986410103670374e-05),\n",
       " (52, 8.192418782510303e-06),\n",
       " (61, 1.1521822684067518e-06),\n",
       " (95, 4.3155372176816033e-07),\n",
       " (1, 0.0),\n",
       " (12, 0),\n",
       " (19, 0),\n",
       " (28, 0),\n",
       " (29, 0),\n",
       " (30, 0),\n",
       " (33, 0),\n",
       " (39, 0),\n",
       " (41, 0),\n",
       " (42, 0),\n",
       " (54, 0),\n",
       " (57, 0),\n",
       " (59, 0),\n",
       " (60, 0),\n",
       " (70, 0),\n",
       " (80, 0),\n",
       " (81, 0),\n",
       " (83, 0),\n",
       " (84, 0),\n",
       " (90, 0),\n",
       " (96, 0),\n",
       " (101, 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndt.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50, 100,  49,   3,  74,  75,   5,   9,  93,  99,  44,  46,   0,\n",
       "        26,  51,  38,  11,  13,  12,  73,  84,  24,   1,  76,  34,   2,\n",
       "        41,  69,  32,  40,  91,  96,   6,  89,  66,  27,  94,  88,  64,\n",
       "        42,  77,  68,  54,  47,  14,  98,  37,  15,  17,  62,   7,  86,\n",
       "        78,  52,  28,   4,  79,  18,  87,  60,  53,  65,  36,  67,  97,\n",
       "        55,   8,  22,  23,  48,  63,  19,  83,  82,  72, 101,  16,  25,\n",
       "        30,  45,  71,  85,  90,  43,  10,  58,  29,  80,  31,  56,  92,\n",
       "        59,  95,  57,  61,  33,  39,  35,  70,  20,  21,  81])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.08166815905930165\n",
      "Root Mean Square Error: 0.14807020948045876\n",
      "R^2 score: 0.4742536368300284\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Random Forest class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalRandomForest_regressor: # post-pruning (kinda) with cross-validation or greedy \n",
    "    def __init__(self, n_trees, trees=None, bootstrap_subSpace=None, outOfBag_error=None, feat_importances=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = trees\n",
    "        self.bootSamplesIdxs_SubSp = bootstrap_subSpace\n",
    "        self.oob_error = outOfBag_error\n",
    "        self.feature_importances = feat_importances\n",
    "        \n",
    "    def fit(self, X, y, depth, minElems_perLeaf, post_pruning=True, verbose=False):\n",
    "        n,d = X.shape\n",
    "        \n",
    "        self.trees = []\n",
    "        self.bootSamplesIdxs_SubSp = []\n",
    "        self.feature_importances = {k:0 for k in range(d)}\n",
    "        \n",
    "        n_learn = int(n/3) # Bootstrap amount to be taken aside\n",
    "        # random subspace method amount\n",
    "        d_learn = int(d/self.n_trees) if int(d/self.n_trees) >= int(np.sqrt(d)) else int(np.sqrt(d))\n",
    "                \n",
    "        # Fitting the forest -----------------------------------\n",
    "        for i in range(self.n_trees):\n",
    "            if verbose: print(\"\\tFitting #{} tree\".format(i+1))\n",
    "            \n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            subspace_idxs = np.sort(np.random.permutation(d)[:d_learn])\n",
    "            self.bootSamplesIdxs_SubSp.append((bootstrap_idxs, subspace_idxs))\n",
    "                        \n",
    "            dt = NumericalDecisionTree_regressor()\n",
    "            \n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs][:,subspace_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=depth, minElems_perLeaf=minElems_perLeaf, post_pruning=post_pruning))\n",
    "            \n",
    "            for k,v in dt.feature_importances.items():\n",
    "                self.feature_importances[subspace_idxs[k]] += v\n",
    "        \n",
    "        # Out-Of-Bag Estimate for the forest -------------------\n",
    "        oob_errors = []\n",
    "        for sampleIdx in range(n):\n",
    "            # finds all the tree that not have current sample in their bootstrapp set (not trained on it)\n",
    "            # and the relative subspace on wich they have been trained\n",
    "            missingTreesIdx_subspc = [(idx,subspace_idxs) for idx,(bootstrap_idxs,subspace_idxs)\n",
    "                                      in enumerate(self.bootSamplesIdxs_SubSp)\n",
    "                                      if sampleIdx not in bootstrap_idxs]\n",
    "\n",
    "            if len(missingTreesIdx_subspc) == 0: continue\n",
    "            \n",
    "            regr_results = np.empty(len(missingTreesIdx_subspc)) # regression estimate of the selected trees\n",
    "            for i, (missingTree_idx, tree_subSpace) in enumerate(missingTreesIdx_subspc):\n",
    "                # reshape in order to correctly use the decision_tree.predict(...): it needs a matrix (num,dim)\n",
    "                # while numpy matrix indexing returns (dim,)\n",
    "                regr_results[i] = self.trees[missingTree_idx].predict(X[sampleIdx,tree_subSpace].reshape(1,-1))\n",
    "            \n",
    "            # done at this level of granularity because a sample might end up in \n",
    "            # being part of no bootstrap set of any tree (so we cannot predict wich value in y will be used)\n",
    "            oob_errors.append(np.square(y[sampleIdx]-np.mean(regr_results)))\n",
    "            #oob_errors.append(r2_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            #oob_errors.append(explained_variance_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            \n",
    "        self.oob_error = np.sqrt(np.mean(oob_errors))\n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"Trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n,_ = X.shape\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, (tree,(_,subspace_idxs)) in enumerate(zip(self.trees, self.bootSamplesIdxs_SubSp)):\n",
    "            results[row] = tree.predict(X[:,subspace_idxs])\n",
    "            \n",
    "        return np.mean(results,axis=0)\n",
    "    \n",
    "    def sort_featureImportances(self, columns=None, num=0):\n",
    "        if self.feature_importances is None:\n",
    "            raise Exception(\"Need to first fit the model!\")\n",
    "            \n",
    "        d = len(self.feature_importances)\n",
    "        n_printFeat = d if not num else num\n",
    "        \n",
    "        if columns is None:\n",
    "            return sorted(self.feature_importances.items(), key=lambda kv: kv[1], reverse=True)[:n_printFeat]\n",
    "\n",
    "        if len(columns) != d:\n",
    "            raise Exception(\"Argument list lenght differs from feature total amount\")\n",
    "        return sorted(zip(columns, map(lambda kv: round(kv[1], 4), self.feature_importances.items())),\n",
    "                      key=lambda kv: kv[1], reverse=True)[:n_printFeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 trees\n",
      "Training 10 trees\n",
      "Training 20 trees\n",
      "Training 30 trees\n",
      "Training 50 trees\n",
      "Training 70 trees\n",
      "Training 100 trees\n",
      "Training 150 trees\n",
      "Training 200 trees\n"
     ]
    }
   ],
   "source": [
    "n_trees = [1,10,20,30,50,70,100,150,200]\n",
    "oob_errors = np.empty(len(n_trees))\n",
    "for i, num_t in enumerate(n_trees):\n",
    "    print(\"Training {} trees\".format(num_t))\n",
    "    nrf = NumericalRandomForest_regressor(num_t)\n",
    "    # no train and test, cause it's a forest\n",
    "    nrf.fit(data, values, depth=100, minElems_perLeaf=5);\n",
    "    oob_errors[i] = nrf.oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Out-Of-Bag errors per estimator amount (trees number)\")\n",
    "plt.errorbar(n_trees, oob_errors, fmt='--o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  49,   3, 100,  51,  92,   2,  46,  93,  99,  43,  40,  44,\n",
       "        41,  71,  69,  68,  62,  38,  74,  14,   4,  98,   6,  78,  10,\n",
       "        91,  75,  76,  25,  17,  88,  90,  67,  59,  24,   7,  45,  23,\n",
       "        39,  94,  29,  36,  31,   0,  18,  96,  22,  60,  21,   9,  30,\n",
       "        89,  48,  35,  32,  15,  34,  95,  11,  27,  55,  73,  65,  16,\n",
       "        82,  79,   8,   1,  97,  37,   5,  77,  19,  86,  47,  63,  33,\n",
       "        28, 101,  26,  87,  42,  66,  72,  58,  64,  83,  61,  52,  54,\n",
       "        20,  84,  13,  53,  57,  12,  85,  80,  56,  81,  70])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=100)\n",
    "# oob error not working, need to perform evaluation on test split\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06966533520208767\n",
      "Root Mean Square Error: 0.10247850538665117\n",
      "R^2 score: 0.7481705824258656\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors - Models definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regulariser: 1.3\n",
      "Residual variance: 0.01160705745374177\n",
      "Root Mean Square Error: 0.1078997770895623\n",
      "R^2 score: 0.7208214864982986\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[1,1.3,1.5,1.7,2]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)[0]\n",
    "print(\"Best regulariser: {}\".format(win_regulariser))\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.05947410227750999\n",
      "Root Mean Square Error: 0.11147876491710357\n",
      "R^2 score: 0.7019938735042794\n"
     ]
    }
   ],
   "source": [
    "rf = NumericalRandomForest_regressor(100)\n",
    "rf.fit(trainVal_data, trainVal_values, depth=100, minElems_perLeaf=10);\n",
    "\n",
    "pred = rf.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09828329626705119"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(39, 0.9110206377142744),\n",
       " (3, 0.8252768375368457),\n",
       " (2, 0.662166731582441),\n",
       " (46, 0.6128644300474198),\n",
       " (78, 0.5602364139160805),\n",
       " (0, 0.5485355111671377),\n",
       " (10, 0.4686934707265575),\n",
       " (40, 0.4595699058467275),\n",
       " (71, 0.4416502319456311),\n",
       " (75, 0.4393184427521904),\n",
       " (43, 0.4206398177634407),\n",
       " (30, 0.41447690858082525),\n",
       " (77, 0.40553293647082517),\n",
       " (20, 0.4009259089703996),\n",
       " (100, 0.40014559045407705),\n",
       " (69, 0.39855313728010494),\n",
       " (72, 0.3950750535725271),\n",
       " (41, 0.393545922778647),\n",
       " (50, 0.3673734817613748),\n",
       " (68, 0.3536992396841796),\n",
       " (91, 0.35290727972074504),\n",
       " (61, 0.32047967517444964),\n",
       " (57, 0.31667377426619914),\n",
       " (96, 0.3143594624038817),\n",
       " (98, 0.3136646218556004),\n",
       " (73, 0.31312708288026336),\n",
       " (63, 0.31005664288077606),\n",
       " (49, 0.306815629702226),\n",
       " (14, 0.3034790637021964),\n",
       " (74, 0.29866606564159953),\n",
       " (22, 0.2969694694156041),\n",
       " (28, 0.29553952246099596),\n",
       " (16, 0.29174341565385614),\n",
       " (51, 0.27679048673410833),\n",
       " (99, 0.2727251542368779),\n",
       " (38, 0.27194681865284287),\n",
       " (90, 0.26544074939746065),\n",
       " (87, 0.2647063766989451),\n",
       " (67, 0.262123255727568),\n",
       " (97, 0.242324569411442),\n",
       " (23, 0.23620401758607484),\n",
       " (80, 0.23523422499016083),\n",
       " (25, 0.23206938505223415),\n",
       " (76, 0.2291604593929183),\n",
       " (19, 0.22059557599523802),\n",
       " (1, 0.2132947004148664),\n",
       " (47, 0.21295949429426803),\n",
       " (31, 0.20969982000888363),\n",
       " (44, 0.2093845140007553),\n",
       " (12, 0.2006438067487989),\n",
       " (34, 0.1992525521983447),\n",
       " (60, 0.19661799119731949),\n",
       " (15, 0.19133473426908146),\n",
       " (62, 0.18729243199939594),\n",
       " (56, 0.18724461035000028),\n",
       " (42, 0.18702722584583398),\n",
       " (92, 0.1865796673831944),\n",
       " (86, 0.1812741261490188),\n",
       " (27, 0.1780147442813353),\n",
       " (21, 0.176992256154489),\n",
       " (53, 0.17681312567346097),\n",
       " (81, 0.17529879855669123),\n",
       " (55, 0.17409372844529883),\n",
       " (66, 0.17186013075424128),\n",
       " (95, 0.16503511001695026),\n",
       " (5, 0.1633721381368805),\n",
       " (93, 0.15677111140876895),\n",
       " (37, 0.15366191776058716),\n",
       " (58, 0.15228869136173867),\n",
       " (85, 0.15090198006769512),\n",
       " (64, 0.13004011121770276),\n",
       " (24, 0.12907319891176583),\n",
       " (8, 0.12877250627607695),\n",
       " (32, 0.1287715374776676),\n",
       " (54, 0.12343842264651354),\n",
       " (36, 0.11851881689142965),\n",
       " (45, 0.11402190472115833),\n",
       " (89, 0.11395897881320745),\n",
       " (79, 0.11025164485684325),\n",
       " (4, 0.10945653746109985),\n",
       " (35, 0.10940610060024222),\n",
       " (48, 0.10623895254863339),\n",
       " (82, 0.10409016002162179),\n",
       " (59, 0.10399029249669231),\n",
       " (26, 0.10298093296566223),\n",
       " (94, 0.09400314139648527),\n",
       " (52, 0.09071501836026699),\n",
       " (101, 0.0901333759633208),\n",
       " (7, 0.07771295915598411),\n",
       " (17, 0.07517684661705318),\n",
       " (9, 0.07281237077355976),\n",
       " (83, 0.07194760930971442),\n",
       " (84, 0.06715529171548991),\n",
       " (88, 0.06705223140050925),\n",
       " (33, 0.06671114744531653),\n",
       " (6, 0.06635714441491254),\n",
       " (29, 0.05914408905156887),\n",
       " (13, 0.0591190190931739),\n",
       " (65, 0.05503870313269536),\n",
       " (11, 0.046587159683575886),\n",
       " (18, 0.0359899103772003),\n",
       " (70, 0.005898553104290814)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM - Not Working (use the sklearn model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_SupportVector_regression:\n",
    "    def __init__(self, weight=None, alpha=None, bias=None):\n",
    "        self.x = alpha\n",
    "        self.w = weight\n",
    "        self.bias = bias\n",
    "        self.Nabla = None\n",
    "                \n",
    "    def SMO2_ab(self, n, H, f, a, LB, UB, maxiter, eps, alpha_s):\n",
    "        \"\"\"\n",
    "        % min_{x} .5 x H x + f' x \n",
    "        %         LB <= x <= UB\n",
    "        %         a' x = b\n",
    "        % n         grandezza problema length(x)\n",
    "        % maxiter   max num it\n",
    "        % eps       precisione\n",
    "        % alpha_s   punto di inizio valido per x\n",
    "        % Nabla     ....\n",
    "        % err       flag di ok\n",
    "        % x         valore della soluzione ottima\n",
    "        % bias      ....\n",
    "        \"\"\"\n",
    "        self.x = alpha_s\n",
    "        self.Nabla = f\n",
    "        for i in range(n):\n",
    "            if (self.x[i] != 0.0):\n",
    "                for j in range(n):\n",
    "                    self.Nabla[j] += H[j,i] * self.x[i]\n",
    "        iter_ = 0\n",
    "        while True:\n",
    "            minF_up = float(\"inf\");\n",
    "            maxF_low = float(\"-inf\");\n",
    "            for i in range(n): \n",
    "                F_i = self.Nabla[i]/a[i]\n",
    "                if (LB[i] < self.x[i]) and (self.x[i] < UB[i]) :\n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == LB[i])) or ((a[i] < 0) and (self.x[i] == UB[i]))) : \n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == UB[i])) or ((a[i] < 0) and (self.x[i] == LB[i]))) : \n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "            if ((maxF_low - minF_up) <= eps):\n",
    "                err = 0.0\n",
    "                break\n",
    "\n",
    "            iter_ += 1\n",
    "            if (iter_ >= maxiter):\n",
    "                err = 1.0\n",
    "                break\n",
    "\n",
    "            if (a[u] > 0):\n",
    "                tau_lb = (LB[u]-self.x[u])*a[u] \n",
    "                tau_ub = (UB[u]-self.x[u])*a[u] \n",
    "            else:\n",
    "                tau_ub = (LB[u]-self.x[u])*a[u] \n",
    "                tau_lb = (UB[u]-self.x[u])*a[u]\n",
    "\n",
    "            if (a[v] > 0):\n",
    "                tau_lb = max(tau_lb,(self.x[v]-UB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-LB[v])*a[v]) \n",
    "            else:\n",
    "                tau_lb = max(tau_lb,(self.x[v]-LB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-UB[v])*a[v])\n",
    "\n",
    "            tau = (self.Nabla[v]/a[v]-self.Nabla[u]/a[u])/(H[u,u]/(a[u]*a[u])\n",
    "                                                           +H[v,v]/(a[v]*a[v])\n",
    "                                                           -2*H[v,u]/(a[u]*a[v]))\n",
    "            tau = min(max(tau,tau_lb),tau_ub)\n",
    "            self.x[u] += tau/a[u]\n",
    "            self.x[v] -= tau/a[v]\n",
    "\n",
    "            for i in range(n):\n",
    "                self.Nabla[i] += H[u,i]*tau/a[u] - H[v,i]*tau/a[v]\n",
    "\n",
    "        tsv = 0\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for k in range(n):\n",
    "            if ((self.x[k] > LB[k]) and (self.x[k] < UB[k])):\n",
    "                self.bias -= self.Nabla[k]/a[k]\n",
    "                tsv += 1\n",
    "\n",
    "        if (tsv > 0):\n",
    "            self.bias /= tsv\n",
    "        else:    \n",
    "            self.bias = -(maxF_low + minF_up)/2.0\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def fit(self, X, y, C):\n",
    "        n = X.shape[0]\n",
    "        cov = np.matmul(X, X.T)\n",
    "        Q = np.matmul(np.matmul(np.diag(y.flatten()), cov),\n",
    "                      np.diag(y.flatten()))\n",
    "        \n",
    "        if self.SMO2_ab(n,Q,-np.ones(n),y.flatten(),\n",
    "                   np.zeros(n),C*np.ones(n),10000000,.0001,np.zeros(n)):\n",
    "            print(\"Problem in SMO\")\n",
    "            \n",
    "        self.w = np.matmul(np.matmul(X.T, np.diag(y.flatten())),\n",
    "                           self.x)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.matmul(X, self.w) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 6.399482291880672\n",
      "Root Mean Square Error: 9.058969844025647\n",
      "R^2 score: -1966.8795992349653\n"
     ]
    }
   ],
   "source": [
    "lsvr = linear_SupportVector_regression()\n",
    "lsvr.fit(trainVal_data, trainVal_values, C=1.0);\n",
    "\n",
    "pred = lsvr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   8,   17,   90,  104,  125,  156,  239,  241,  244,  254,  267,\n",
       "         300,  375,  389,  464,  514,  569,  580,  595,  615,  628,  642,\n",
       "         707,  743,  831,  856,  857,  918,  987, 1003, 1013, 1076, 1095,\n",
       "        1108, 1118, 1194, 1208, 1319, 1445, 1468]),)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lsvr.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel=\"linear\", tol=.0001, C=1)\n",
    "svr.fit(trainVal_data, trainVal_values.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(svr.dual_coef_)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.07308253361834248\n",
      "Root Mean Square Error: 0.10832676960510007\n",
      "R^2 score: 0.7186075244570947\n"
     ]
    }
   ],
   "source": [
    "pred = svr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Features Elimination\n",
    "<img src=\"img/Algo1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFitting #1 tree\n",
      "\tFitting #2 tree\n",
      "\tFitting #3 tree\n",
      "\tFitting #4 tree\n",
      "\tFitting #5 tree\n",
      "\tFitting #6 tree\n",
      "\tFitting #7 tree\n",
      "\tFitting #8 tree\n",
      "\tFitting #9 tree\n",
      "\tFitting #10 tree\n",
      "\tFitting #11 tree\n",
      "\tFitting #12 tree\n",
      "\tFitting #13 tree\n",
      "\tFitting #14 tree\n",
      "\tFitting #15 tree\n",
      "\tFitting #16 tree\n",
      "\tFitting #17 tree\n",
      "\tFitting #18 tree\n",
      "\tFitting #19 tree\n",
      "\tFitting #20 tree\n",
      "\tFitting #21 tree\n",
      "\tFitting #22 tree\n",
      "\tFitting #23 tree\n",
      "\tFitting #24 tree\n",
      "\tFitting #25 tree\n",
      "\tFitting #26 tree\n",
      "\tFitting #27 tree\n",
      "\tFitting #28 tree\n",
      "\tFitting #29 tree\n",
      "\tFitting #30 tree\n",
      "\tFitting #31 tree\n",
      "\tFitting #32 tree\n",
      "\tFitting #33 tree\n",
      "\tFitting #34 tree\n",
      "\tFitting #35 tree\n",
      "\tFitting #36 tree\n",
      "\tFitting #37 tree\n",
      "\tFitting #38 tree\n",
      "\tFitting #39 tree\n",
      "\tFitting #40 tree\n",
      "\tFitting #41 tree\n",
      "\tFitting #42 tree\n",
      "\tFitting #43 tree\n",
      "\tFitting #44 tree\n",
      "\tFitting #45 tree\n",
      "\tFitting #46 tree\n",
      "\tFitting #47 tree\n",
      "\tFitting #48 tree\n",
      "\tFitting #49 tree\n",
      "\tFitting #50 tree\n",
      "\tFitting #51 tree\n",
      "\tFitting #52 tree\n",
      "\tFitting #53 tree\n",
      "\tFitting #54 tree\n",
      "\tFitting #55 tree\n",
      "\tFitting #56 tree\n",
      "\tFitting #57 tree\n",
      "\tFitting #58 tree\n",
      "\tFitting #59 tree\n",
      "\tFitting #60 tree\n",
      "\tFitting #61 tree\n",
      "\tFitting #62 tree\n",
      "\tFitting #63 tree\n",
      "\tFitting #64 tree\n",
      "\tFitting #65 tree\n",
      "\tFitting #66 tree\n",
      "\tFitting #67 tree\n",
      "\tFitting #68 tree\n",
      "\tFitting #69 tree\n",
      "\tFitting #70 tree\n",
      "\tFitting #71 tree\n",
      "\tFitting #72 tree\n",
      "\tFitting #73 tree\n",
      "\tFitting #74 tree\n",
      "\tFitting #75 tree\n",
      "\tFitting #76 tree\n",
      "\tFitting #77 tree\n",
      "\tFitting #78 tree\n",
      "\tFitting #79 tree\n",
      "\tFitting #80 tree\n",
      "\tFitting #81 tree\n",
      "\tFitting #82 tree\n",
      "\tFitting #83 tree\n",
      "\tFitting #84 tree\n",
      "\tFitting #85 tree\n",
      "\tFitting #86 tree\n",
      "\tFitting #87 tree\n",
      "\tFitting #88 tree\n",
      "\tFitting #89 tree\n",
      "\tFitting #90 tree\n",
      "\tFitting #91 tree\n",
      "\tFitting #92 tree\n",
      "\tFitting #93 tree\n",
      "\tFitting #94 tree\n",
      "\tFitting #95 tree\n",
      "\tFitting #96 tree\n",
      "\tFitting #97 tree\n",
      "\tFitting #98 tree\n",
      "\tFitting #99 tree\n",
      "\tFitting #100 tree\n"
     ]
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(100)\n",
    "nrf.fit(trainVal_data, trainVal_values, depth=200, minElems_perLeaf=5, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(15, 1.2667524423089707),\n",
       " (49, 1.2172819021391184),\n",
       " (28, 1.2166564500118702),\n",
       " (78, 1.1326225732468072),\n",
       " (0, 0.868117099177149),\n",
       " (50, 0.8532842851096637),\n",
       " (39, 0.80421147423288),\n",
       " (44, 0.7668728101124096),\n",
       " (38, 0.7177620499480079),\n",
       " (42, 0.7155394471381687),\n",
       " (2, 0.6919078701983675),\n",
       " (100, 0.6827767846690378),\n",
       " (51, 0.666603083891823),\n",
       " (46, 0.6626017133146643),\n",
       " (4, 0.6401371923312802),\n",
       " (40, 0.6238594648811658),\n",
       " (43, 0.6159912525316843),\n",
       " (3, 0.6104394820962873),\n",
       " (61, 0.5788099002788748),\n",
       " (10, 0.541403292081756),\n",
       " (14, 0.5301586271627494),\n",
       " (45, 0.5242100397772046),\n",
       " (32, 0.5180017189917707),\n",
       " (48, 0.5088652391621283),\n",
       " (62, 0.49947803723429635),\n",
       " (17, 0.47169686580005404),\n",
       " (30, 0.468331705716618),\n",
       " (16, 0.4573196515383959),\n",
       " (91, 0.44981002174248763),\n",
       " (12, 0.4443974996992389),\n",
       " (6, 0.4369008703644815),\n",
       " (41, 0.43003828801409444),\n",
       " (67, 0.4214263078132912),\n",
       " (99, 0.41464557786989387),\n",
       " (56, 0.41281149611187984),\n",
       " (89, 0.40857486928099845),\n",
       " (68, 0.4061050166410038),\n",
       " (26, 0.39532022478906886),\n",
       " (75, 0.3934874938252527),\n",
       " (88, 0.3928629294652691),\n",
       " (9, 0.39272420828025434),\n",
       " (37, 0.39174735983848663),\n",
       " (21, 0.3916535644071859),\n",
       " (1, 0.38356667243982395),\n",
       " (34, 0.37538091736958384),\n",
       " (83, 0.37023346649729566),\n",
       " (84, 0.3663376081016735),\n",
       " (65, 0.3650326719720504),\n",
       " (54, 0.35511605723806605),\n",
       " (66, 0.3461371883526292),\n",
       " (25, 0.3419270237420563),\n",
       " (101, 0.34190387405489864),\n",
       " (27, 0.339965820787043),\n",
       " (59, 0.33254144475528113),\n",
       " (7, 0.3305950365947685),\n",
       " (19, 0.32950437007628486),\n",
       " (86, 0.3252958192604068),\n",
       " (73, 0.3109653019099663),\n",
       " (60, 0.30382040034254537),\n",
       " (87, 0.2984299265285572),\n",
       " (92, 0.29542326649325806),\n",
       " (55, 0.2908505386648569),\n",
       " (69, 0.29074651960686493),\n",
       " (13, 0.2880619632782213),\n",
       " (85, 0.28674146086242214),\n",
       " (57, 0.2857551030513772),\n",
       " (72, 0.28482142107895436),\n",
       " (77, 0.2833523924426647),\n",
       " (74, 0.2828206365001925),\n",
       " (71, 0.28139653379921914),\n",
       " (90, 0.28112945599392547),\n",
       " (23, 0.27653175747965675),\n",
       " (79, 0.27258455309872415),\n",
       " (64, 0.2709653990343691),\n",
       " (95, 0.26833718473150936),\n",
       " (96, 0.267064691633597),\n",
       " (58, 0.2578341496215303),\n",
       " (47, 0.25749098016533445),\n",
       " (98, 0.25506601212646507),\n",
       " (22, 0.25462732587790654),\n",
       " (97, 0.24086152764420798),\n",
       " (5, 0.23740272746356508),\n",
       " (36, 0.2345014045040557),\n",
       " (33, 0.22001086273152337),\n",
       " (35, 0.2140524878205829),\n",
       " (18, 0.21136781558156714),\n",
       " (82, 0.1973569371367987),\n",
       " (53, 0.17212494294041636),\n",
       " (31, 0.16660848601442282),\n",
       " (24, 0.1665654664893012),\n",
       " (52, 0.16522128193941008),\n",
       " (94, 0.15651117353971172),\n",
       " (63, 0.1542003547221443),\n",
       " (80, 0.14992898030537216),\n",
       " (93, 0.12371642893549471),\n",
       " (8, 0.11980831581596985),\n",
       " (81, 0.08651409147160727),\n",
       " (76, 0.08640923478632749),\n",
       " (11, 0.07594173921846795),\n",
       " (29, 0.05658913623485796),\n",
       " (20, 0.055380785695062607),\n",
       " (70, 0.038798524239218694)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Features sorted by their score:\\n\")\n",
    "nrf.sort_featureImportances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pctWInvInc', 1.2668),\n",
       " ('NumKidsBornNeverMar', 1.2173),\n",
       " ('PctPopUnderPov', 1.2167),\n",
       " ('PctWOFullPlumb', 1.1326),\n",
       " ('population', 0.8681),\n",
       " ('PctKidsBornNeverMar', 0.8533),\n",
       " ('MalePctNevMarr', 0.8042),\n",
       " ('PctKids2Par', 0.7669),\n",
       " ('MalePctDivorce', 0.7178),\n",
       " ('PersPerFam', 0.7155),\n",
       " ('racepctblack', 0.6919),\n",
       " ('PctUsePubTrans', 0.6828),\n",
       " ('NumImmig', 0.6666),\n",
       " ('PctTeen2Par', 0.6626),\n",
       " ('racePctAsian', 0.6401),\n",
       " ('FemalePctDiv', 0.6239),\n",
       " ('PctFam2Par', 0.616),\n",
       " ('racePctWhite', 0.6104),\n",
       " ('PctNotSpeakEnglWell', 0.5788),\n",
       " ('numbUrban', 0.5414),\n",
       " ('pctWFarmSelf', 0.5302),\n",
       " ('PctYoungKids2Par', 0.5242),\n",
       " ('PctUnemployed', 0.518),\n",
       " ('PctWorkMom', 0.5089),\n",
       " ('PctLargHouseFam', 0.4995),\n",
       " ('pctWPubAsst', 0.4717),\n",
       " ('PctNotHSGrad', 0.4683),\n",
       " ('pctWSocSec', 0.4573),\n",
       " ('NumInShelters', 0.4498),\n",
       " ('medIncome', 0.4444),\n",
       " ('agePct12t21', 0.4369),\n",
       " ('TotalPctDiv', 0.43),\n",
       " ('PctPersOwnOccup', 0.4214),\n",
       " ('PopDens', 0.4146),\n",
       " ('PctRecentImmig', 0.4128),\n",
       " ('MedOwnCostPctInc', 0.4086),\n",
       " ('PctPersDenseHous', 0.4061),\n",
       " ('HispPerCap', 0.3953),\n",
       " ('PctVacMore6Mos', 0.3935),\n",
       " ('MedRentPctHousInc', 0.3929),\n",
       " ('agePct65up', 0.3927),\n",
       " ('whitePerCap', 0.3917),\n",
       " ('PctOccupMgmtProf', 0.3917),\n",
       " ('householdsize', 0.3836),\n",
       " ('PctEmplManu', 0.3754),\n",
       " ('RentLowQ', 0.3702),\n",
       " ('RentMedian', 0.3663),\n",
       " ('PersPerOwnOccHous', 0.365),\n",
       " ('PctImmigRec8', 0.3551),\n",
       " ('PersPerRentOccHous', 0.3461),\n",
       " ('OtherPerCap', 0.3419),\n",
       " ('LemasPctOfficDrugUn', 0.3419),\n",
       " ('NumUnderPov', 0.34),\n",
       " ('PctRecImmig10', 0.3325),\n",
       " ('agePct12t29', 0.3306),\n",
       " ('medFamInc', 0.3295),\n",
       " ('RentQrange', 0.3253),\n",
       " ('PctHousOwnOcc', 0.311),\n",
       " ('PctSpeakEnglOnly', 0.3038),\n",
       " ('MedRent', 0.2984),\n",
       " ('NumStreet', 0.2954),\n",
       " ('PctImmigRec10', 0.2909),\n",
       " ('PctHousLess3BR', 0.2907),\n",
       " ('pctWWage', 0.2881),\n",
       " ('RentHighQ', 0.2867),\n",
       " ('PctRecImmig5', 0.2858),\n",
       " ('PctHousOccup', 0.2848),\n",
       " ('PctHousNoPhone', 0.2834),\n",
       " ('PctVacantBoarded', 0.2828),\n",
       " ('HousVacant', 0.2814),\n",
       " ('MedOwnCostPctIncNoMtg', 0.2811),\n",
       " ('indianPerCap', 0.2765),\n",
       " ('OwnOccLowQuart', 0.2726),\n",
       " ('PersPerOccupHous', 0.271),\n",
       " ('PctSameHouse85', 0.2683),\n",
       " ('PctSameCity85', 0.2671),\n",
       " ('PctRecImmig8', 0.2578),\n",
       " ('PctWorkMomYoungKids', 0.2575),\n",
       " ('LandArea', 0.2551),\n",
       " ('blackPerCap', 0.2546),\n",
       " ('PctSameState85', 0.2409),\n",
       " ('racePctHisp', 0.2374),\n",
       " ('PctOccupManu', 0.2345),\n",
       " ('PctEmploy', 0.22),\n",
       " ('PctEmplProfServ', 0.2141),\n",
       " ('pctWRetire', 0.2114),\n",
       " ('OwnOccQrange', 0.1974),\n",
       " ('PctImmigRec5', 0.1721),\n",
       " ('AsianPerCap', 0.1666),\n",
       " ('PctBSorMore', 0.1666),\n",
       " ('PctImmigRecent', 0.1652),\n",
       " ('PctBornSameState', 0.1565),\n",
       " ('PctLargHouseOccup', 0.1542),\n",
       " ('OwnOccMedVal', 0.1499),\n",
       " ('PctForeignBorn', 0.1237),\n",
       " ('agePct16t24', 0.1198),\n",
       " ('OwnOccHiQuart', 0.0865),\n",
       " ('MedYrHousBuilt', 0.0864),\n",
       " ('pctUrban', 0.0759),\n",
       " ('PctLess9thGrade', 0.0566),\n",
       " ('perCapInc', 0.0554),\n",
       " ('MedNumBR', 0.0388)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Features sorted by their score:\\n\")\n",
    "nrf.sort_featureImportances(columns=pred_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe(estimator, parameters_dict, data, vals, n_features=None, verbose=False):\n",
    "    _,d = data.shape\n",
    "    nf = n_features if n_features else int(d/2)\n",
    "    train_feat = list(range(d))    \n",
    "    rmse = []\n",
    "    r2 = []\n",
    "\n",
    "    tr_data, ts_data, tr_vals, ts_vals = trainTest_split(in_matrix=data, out_vect=vals, train_amount=0.7)\n",
    "    \n",
    "    while d > nf:\n",
    "        if verbose:\n",
    "            update = round((data.shape[1]-d)*100/(data.shape[1]-nf), 2) # just print completion rate\n",
    "            print(\"\\t[\"+'#'*(int(update/5))+' '*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "            \n",
    "        estimator.fit(X=tr_data[:,train_feat], y=tr_vals, **parameters_dict)\n",
    "        pred = estimator.predict(ts_data)\n",
    "        rem = Regression_evaluationMetric(ts_vals, pred)\n",
    "        rmse.append(rem.rootMeanSquareError())\n",
    "        r2.append(rem.rSquared())\n",
    "            \n",
    "        rank = estimator.sort_featureImportances()\n",
    "        toDiscard_idx = rank[-1][0]\n",
    "        train_feat.remove(train_feat[toDiscard_idx])\n",
    "\n",
    "        d -= 1\n",
    "    \n",
    "    return (rmse, r2, train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[                    ] 0.0%\n",
      "\t[                   ] 1.96%\n",
      "\t[                   ] 3.92%\n",
      "\t[#                  ] 5.88%\n",
      "\t[#                  ] 7.84%\n",
      "\t[#                  ] 9.8%\n",
      "\t[##                 ] 11.76%\n",
      "\t[##                 ] 13.73%\n",
      "\t[###                ] 15.69%\n",
      "\t[###                ] 17.65%\n",
      "\t[###                ] 19.61%\n",
      "\t[####               ] 21.57%\n",
      "\t[####               ] 23.53%\n",
      "\t[#####              ] 25.49%\n",
      "\t[#####              ] 27.45%\n",
      "\t[#####              ] 29.41%\n",
      "\t[######             ] 31.37%\n",
      "\t[######             ] 33.33%\n",
      "\t[#######            ] 35.29%\n",
      "\t[#######            ] 37.25%\n",
      "\t[#######            ] 39.22%\n",
      "\t[########           ] 41.18%\n",
      "\t[########           ] 43.14%\n",
      "\t[#########          ] 45.1%\n",
      "\t[#########          ] 47.06%\n",
      "\t[#########          ] 49.02%\n",
      "\t[##########         ] 50.98%\n",
      "\t[##########         ] 52.94%\n",
      "\t[##########         ] 54.9%\n",
      "\t[###########        ] 56.86%\n",
      "\t[###########        ] 58.82%\n",
      "\t[############       ] 60.78%\n",
      "\t[############       ] 62.75%\n",
      "\t[############       ] 64.71%\n",
      "\t[#############      ] 66.67%\n",
      "\t[#############      ] 68.63%\n",
      "\t[##############     ] 70.59%\n",
      "\t[##############     ] 72.55%\n",
      "\t[##############     ] 74.51%\n",
      "\t[###############    ] 76.47%\n",
      "\t[###############    ] 78.43%\n",
      "\t[################   ] 80.39%\n",
      "\t[################   ] 82.35%\n",
      "\t[################   ] 84.31%\n",
      "\t[#################  ] 86.27%\n",
      "\t[#################  ] 88.24%\n",
      "\t[################## ] 90.2%\n",
      "\t[################## ] 92.16%\n",
      "\t[################## ] 94.12%\n",
      "\t[###################] 96.08%\n",
      "\t[###################] 98.04%\n"
     ]
    }
   ],
   "source": [
    "p_dict = {\"depth\":200, \"minElems_perLeaf\":5, \"post_pruning\":True, \"verbose\":False}\n",
    "rootMSE, r2, surv_feature = rfe(NumericalRandomForest_regressor(100),\n",
    "                                p_dict, trainVal_data, trainVal_values, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "x = range(trainVal_data.shape[1],len(surv_feature),-1)\n",
    "plt.plot(x, rootMSE, label=\"RMSE\")\n",
    "#plt.plot(x, r2, label=\"R2\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(trainVal_data.shape[1], len(surv_feature))\n",
    "#range(trainVal_data.shape[1],len(surv_feature),-1)\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Root Mean Square Error\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15950524950718215,\n",
       " 0.17113526202722198,\n",
       " 0.31529633306745614,\n",
       " 0.309950081678997,\n",
       " 0.3545869863238208,\n",
       " 0.40335021015983974,\n",
       " 0.3743350930716753,\n",
       " 0.3618972835676513,\n",
       " 0.453821694961102,\n",
       " 0.3617319645207951,\n",
       " 0.39832492034570766,\n",
       " 0.4168825685150099,\n",
       " 0.45982945268396214,\n",
       " 0.43804417938353263,\n",
       " 0.44192025399237156,\n",
       " 0.4535934167529332,\n",
       " 0.44784436734379296,\n",
       " 0.4218520024826061,\n",
       " 0.4329530733864868,\n",
       " 0.3823140356736881,\n",
       " 0.3899390965536767,\n",
       " 0.3877585713425083,\n",
       " 0.37387087260073626,\n",
       " 0.38131333143736595,\n",
       " 0.4271516323149062,\n",
       " 0.3960745391358284,\n",
       " 0.3355870069439423,\n",
       " 0.41705623799445174,\n",
       " 0.4088092397084685,\n",
       " 0.39809280390637763,\n",
       " 0.46078522012151074,\n",
       " 0.4618297192716776,\n",
       " 0.42619600357023574,\n",
       " 0.4151428881855723,\n",
       " 0.4456670423017163,\n",
       " 0.4347062214254675,\n",
       " 0.4470225862166969,\n",
       " 0.4074430650970377,\n",
       " 0.40951092726528265,\n",
       " 0.4236516491027642,\n",
       " 0.3943130179699116,\n",
       " 0.43552630608930976,\n",
       " 0.3707150524265042,\n",
       " 0.45320788489613484,\n",
       " 0.4076676186576423,\n",
       " 0.37625987004343076,\n",
       " 0.38569277320142376,\n",
       " 0.42266675780646357,\n",
       " 0.43206671635611466,\n",
       " 0.38259435674907494,\n",
       " 0.4084374766708566]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['population', 'householdsize', 'racePctWhite', 'racePctAsian',\n",
       "       'agePct12t21', 'agePct16t24', 'agePct65up', 'numbUrban', 'pctWWage',\n",
       "       'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'medFamInc', 'whitePerCap',\n",
       "       'blackPerCap', 'indianPerCap', 'NumUnderPov', 'PctPopUnderPov',\n",
       "       'PctLess9thGrade', 'PctEmplProfServ', 'PctOccupManu', 'MalePctDivorce',\n",
       "       'MalePctNevMarr', 'TotalPctDiv', 'PctFam2Par', 'PctKids2Par',\n",
       "       'PctWorkMomYoungKids', 'NumKidsBornNeverMar', 'PctKidsBornNeverMar',\n",
       "       'NumImmig', 'PctImmigRec8', 'PctImmigRec10', 'PctRecImmig5',\n",
       "       'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup',\n",
       "       'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup',\n",
       "       'PctPersDenseHous', 'PctHousLess3BR', 'HousVacant', 'PctVacantBoarded',\n",
       "       'MedYrHousBuilt', 'PctWOFullPlumb', 'OwnOccHiQuart', 'RentMedian',\n",
       "       'MedRentPctHousInc', 'PctSameHouse85', 'PopDens', 'PctUsePubTrans'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_features.columns[surv_feature]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
