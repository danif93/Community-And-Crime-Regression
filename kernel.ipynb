{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    nanRows, nanCols = np.where(df.isna())\n",
    "    for j in nanCols:\n",
    "        available = df.iloc[~nanRows, j]\n",
    "        value = value_withStrategy(available, strategy)\n",
    "        df.iloc[nanRows,j] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,values = drop_sample(pred_features, regr_values[\"robbPerPop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data.shape[0]\n",
    "\n",
    "trVl_Amount = int(n*0.7)\n",
    "indexes = np.random.permutation(n)\n",
    "idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "trainVal_data = data[idxTrVl]\n",
    "test_data = data[idxTs]\n",
    "trainVal_values = values[idxTrVl]\n",
    "test_values = values[idxTs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def kFold_crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor):\n",
    "    nVal = train_data.shape[0]\n",
    "    \n",
    "    # Validation indexes adjustment\n",
    "    elemPerFold, remainder = np.divmod(nVal,k)\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values()))\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList):\n",
    "        print(\"#{} fold:\".format(row+1))\n",
    "        for col, params in enumerate(params_product):\n",
    "            update = col*100/len(params_product)\n",
    "            print(\"\\t[\"+\"#\"*(int(update/5))+\" \"*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)}\n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            #val_results[row,col] = r2_score(trainVal_values[valIdx],pred)\n",
    "            #val_results[row,col] = explained_variance_score(trainVal_values[valIdx],pred)\n",
    "            val_results[row,col] = np.mean(np.square(train_values[valIdx]-pred))\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "\n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance\n",
    "            residual -= np.matmul(data_matrix, self.weights)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "np.where(mp.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 2.162310179537521e+31\n",
      "R^2 score: -5.9320785418722686e+32\n",
      "Explained Variance Score: -4.0727187383408635e+30\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 34, 38, 50, 67, 76, 92, 93, 94])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07198140921010004\n",
      "R^2 score: 0.6735424365120475\n",
      "Explained Variance Score: 0.6774951217473056\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression:\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        self.weights = np.zeros((data_matrix.shape[1],1))\n",
    "        n = float(data_matrix.shape[0])\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            dist = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            coord_descent = (step/n)*np.matmul(data_matrix.T, dist)\n",
    "            self.weights -= coord_descent\n",
    "\n",
    "            upper = self.weights > softTresh\n",
    "            lower = self.weights < -softTresh\n",
    "\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  27,  49,  51,  71,  91,  92,  98, 101])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=10)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.8)\n",
    "np.where(lr.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.7113568063332496\n",
      "R^2 score: -18.515352082219774\n",
      "Explained Variance Score: 0.012067232341596346\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  76,  93,  94, 100])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.061549729003930895\n",
      "R^2 score: 0.6735594634906952\n",
      "Explained Variance Score: 0.6753372729897191\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor:\n",
    "    class Node:\n",
    "        def __init__(self, value, isLeaf=False, feature=None, left=None, right=None):\n",
    "            self.value = value\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.value))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            print(\"|    \"*level+\"{} => {}\".format(self.feature, self.value))\n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElem_perLeaf):\n",
    "        self.root = self.learn(X, y, depth, minElem_perLeaf)\n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElem_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElem_perLeaf: #or other condition\n",
    "            return self.Node(value=np.mean(y), isLeaf=True)\n",
    "            \n",
    "        best_costDescent = 0 #split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "\n",
    "                if sorted_x[i2]<sorted_x[i2+1]:\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "\n",
    "        return self.Node(value=best_cut, feature=best_feature,\n",
    "                        left=self.learn(X[left_idxs],y[left_idxs],depth-1,minElem_perLeaf),\n",
    "                        right=self.learn(X[~left_idxs],y[~left_idxs],depth-1,minElem_perLeaf))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.value:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.value\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    None => 0.5100181968349873\n",
      "|    |    100 => -0.33296521258972944\n",
      "|    |    |    |    |    None => -0.19629075087463083\n",
      "|    |    |    |    4 => -0.979453247431656\n",
      "|    |    |    |    |    None => -0.6485360822920946\n",
      "|    |    |    21 => -0.7930354381646628\n",
      "|    |    |    |    None => 0.18936986833794878\n",
      "|    50 => -0.04936014625228524\n",
      "|    |    |    |    |    None => -0.28642691796750974\n",
      "|    |    |    |    1 => -0.35326086956521746\n",
      "|    |    |    |    |    None => 0.055184993794525926\n",
      "|    |    |    89 => 0.12834224598930477\n",
      "|    |    |    |    |    None => -0.3865909741146588\n",
      "|    |    |    |    38 => 0.08746518105849588\n",
      "|    |    |    |    |    None => -0.6006946008695025\n",
      "|    |    50 => -0.47020109689213896\n",
      "|    |    |    |    |    None => 0.011381855282161402\n",
      "|    |    |    |    51 => -0.7704501056454165\n",
      "|    |    |    |    |    None => -0.6379556784124296\n",
      "|    |    |    50 => -0.6840950639853748\n",
      "|    |    |    |    |    None => -0.6800242624466498\n",
      "|    |    |    |    2 => -0.7997310437571119\n",
      "|    |    |    |    |    None => -0.8239465048385032\n",
      "49 => -0.9916786243003126\n",
      "|    |    |    |    None => -0.1379705817834371\n",
      "|    |    |    41 => 0.16752577319587636\n",
      "|    |    |    |    |    None => -0.7288106248316132\n",
      "|    |    |    |    69 => 0.44321629822280006\n",
      "|    |    |    |    |    None => 0.050204714393608185\n",
      "|    |    69 => 0.4418075422626786\n",
      "|    |    |    |    |    None => -0.8145271089371612\n",
      "|    |    |    |    40 => 0.1074380165289257\n",
      "|    |    |    |    |    None => -0.8879903446124421\n",
      "|    |    |    3 => 0.4612686952037133\n",
      "|    |    |    |    |    None => -0.6888140593831567\n",
      "|    |    |    |    11 => 0.9693999999999999\n",
      "|    |    |    |    |    None => -0.806787120473172\n",
      "|    50 => -0.7612431444241317\n",
      "|    |    |    |    None => -0.5658853511061644\n",
      "|    |    |    93 => 0.08801062769843915\n",
      "|    |    |    |    |    None => -0.5057586504897392\n",
      "|    |    |    |    48 => 0.49831294812315463\n",
      "|    |    |    |    |    None => -0.8819676736877072\n",
      "|    |    93 => -0.5813683161740286\n",
      "|    |    |    |    |    None => -0.895766585840919\n",
      "|    |    |    |    3 => 0.7296544610624034\n",
      "|    |    |    |    |    None => -0.7299209851024455\n",
      "|    |    |    69 => 0.21488946684005206\n",
      "|    |    |    |    |    None => -0.9321333235714425\n",
      "|    |    |    |    40 => -0.09042294603791928\n",
      "|    |    |    |    |    None => -0.9661969599253651\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElem_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.06835981935260961\n",
      "R^2 score: 0.5773903356378863\n",
      "Explained Variance Score: 0.5782774284717946\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49,  50, 100,  69,  89,  93,  41,   3,  20,  99,   2,  38,  75,\n",
       "        48,  40,   8,  11,  67,   1,  24,  18,  52,  10,  44,  63,  19,\n",
       "        36,  29,  60,  96,  56,  30,  73,  79,  77,  65,  37,  31,   6,\n",
       "         9,  14,  12,  94,  35,  51,  55,  26,  95,  43,  34,  64,  88,\n",
       "        47,  13,  16,  98,  72,  78,  54,   0,  62,  39,  80,  25,  74,\n",
       "         7,  57,   5,  91,  17,  97,  87, 101,  23,  21,  68,  61,  15,\n",
       "        86,  46,  45,  82,  81,   4,  92,  85,  22,  90,  27,  53,  58,\n",
       "        76,  71,  33,  59,  42,  84,  83,  28,  32,  66,  70])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07996859444280134\n",
      "R^2 score: 0.4160187815952242\n",
      "Explained Variance Score: 0.4172435215571928\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests\n",
    "\n",
    "class NumericalRandomForest_regressor:\n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.boot_samplesIdxs = []\n",
    "        self.oob_error = None\n",
    "        \n",
    "    def fit(self, X, y, depths, minElems_perLeaf):\n",
    "        n, d = X.shape\n",
    "        n_learn = int(n/3)\n",
    "        \n",
    "        val_folds = 3\n",
    "        params_dict = {\"depth\":depths, \"minElem_perLeaf\":minElems_perLeaf}\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            print(\"Fitting #{} tree\".format(i+1))\n",
    "            \n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            self.boot_samplesIdxs.append(bootstrap_idxs)\n",
    "                        \n",
    "            dt = NumericalDecisionTree_regressor()            \n",
    "            win_params = kFold_crossValidation_selectionGrid(val_folds, params_dict, \n",
    "                                                             X[~bootstrap_idxs], y[~bootstrap_idxs], dt)\n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=win_params[0], minElem_perLeaf=win_params[1]))\n",
    "        \n",
    "        \n",
    "        oob_errors = []\n",
    "        for sampleIdx in range(n):\n",
    "            missingBoot_TreesIdx = [idx for idx,bootstrap_idxs in enumerate(self.boot_samplesIdxs) \n",
    "                                    if sampleIdx not in bootstrap_idxs]\n",
    "\n",
    "            if len(missingBoot_TreesIdx)==0: continue\n",
    "            \n",
    "            regr_results = np.empty(len(missingBoot_TreesIdx))\n",
    "            for i, missing_tree in enumerate(missingBoot_TreesIdx):\n",
    "                regr_results[i] = self.trees[missing_tree].predict(X[sampleIdx].reshape(1,-1))\n",
    "                \n",
    "            oob_errors.append(np.mean(np.square(np.mean(regr_results)-y[sampleIdx])))\n",
    "            #oob_errors.append(r2_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            #oob_errors.append(explained_variance_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            \n",
    "        self.oob_error = np.mean(oob_errors)\n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, tree in enumerate(self.trees):\n",
    "            results[row] = tree.predict(X)\n",
    "            \n",
    "        return np.mean(results,axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "Fitting #2 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n",
      "Fitting #3 tree\n",
      "#1 fold:\n",
      "\t[                    ] 0.0%\n",
      "#2 fold:\n",
      "\t[                    ] 0.0%\n",
      "#3 fold:\n",
      "\t[                    ] 0.0%\n"
     ]
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(3)\n",
    "# no train and test, cause it's a forest\n",
    "nrf.fit(data,values,depths=[10],minElems_perLeaf=[20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01237872915832776"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrf.oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07093161098240276\n",
      "R^2 score: 0.7753652339689243\n",
      "Explained Variance Score: 0.7777124433711212\n"
     ]
    }
   ],
   "source": [
    "pred = nrf.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/.conda/envs/bcb/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 50,  49, 100,   3,  51,  99,  91,  93,   2,  92,  78,  41,  68,\n",
       "        69,  40,  38,  10,  44,  29,  17,  87,  57,  66,  94,  46,   4,\n",
       "        15,  71,  14,  80,  98,  75,   1,  81,   6,  89,  43,  24,  67,\n",
       "        65,  11,  20,  34, 101,  74,  72,  55,  58,  76,  16,   0,  95,\n",
       "         8,  88,  62,  54,  23,  12,  47,  61,  39,  35,  82,  73,  21,\n",
       "        96,  59,  18,  86,  36,  90,   7,  30,  45,  64,  31,  22,  60,\n",
       "        28,  84,  26,  33,  37,   5,  32,  27,  25,  97,  13,  48,  63,\n",
       "         9,  79,  77,  56,  19,  52,  53,  42,  85,  83,  70])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[2,2.05,2.1,2.2,3]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 \n",
    "params_dict = {\"depth\":[10,15,20,30],\"minElem_perLeaf\":[5,10,20,30]}\n",
    "\n",
    "dt = NumericalDecisionTree_regressor()\n",
    "win_params = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, dt)\n",
    "dt.fit(trainVal_data, trainVal_values, depth=win_params[0], minElem_perLeaf=win_params[1])\n",
    "pred = dt.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
