{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cross validation purposes: create the cartesian product between the chosen values sets\n",
    "from itertools import product \n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    # for each column that contain at least 1 NaN value...\n",
    "    for nanCol in np.unique(np.where(pred_features.isna())[1]):\n",
    "        nanRows = np.where(pred_features.iloc[:,nanCol].isna())[0] #find NaN rows for the current column\n",
    "        available = df.iloc[~nanRows, nanCol]\n",
    "        value = value_withStrategy(available, strategy) #compute the filling value\n",
    "        df.iloc[nanRows, nanCol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- TBD <br>\n",
    "A thourough study from scratch of outliers detection is needed here, but for now it feels like it exceeds the course final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values rows on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_naSample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = \"robbPerPop\"\n",
    "data,values = drop_naSample(pred_features, regr_values[dep_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTest_split(in_matrix, out_vect, train_amount=0.7):\n",
    "    n,_ = in_matrix.shape\n",
    "\n",
    "    trVl_Amount = int(n*train_amount) #training-validation amount\n",
    "    indexes = np.random.permutation(n)\n",
    "    idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "    idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "    return in_matrix[idxTrVl], in_matrix[idxTs], out_vect[idxTrVl], out_vect[idxTs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal_data, test_data, trainVal_values, test_values = trainTest_split(data, values, train_amount=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_evaluationMetric:\n",
    "    def __init__(self, true, predicted):\n",
    "        self.true = true.flatten()\n",
    "        self.predicted = predicted.flatten()\n",
    "        self.residuals = self.true-self.predicted\n",
    "    \n",
    "    def meanSquareError(self):\n",
    "        return np.mean(np.square(self.residuals))\n",
    "    \n",
    "    def rootMeanSquareError(self):\n",
    "        return np.sqrt(np.mean(np.square(self.residuals)))\n",
    "    \n",
    "    def meanAbsoluteError(self):\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def rSquared(self):\n",
    "        ss_residual = np.sum(np.square(self.residuals))\n",
    "        ss_total = np.sum(np.square(self.true-np.mean(self.true)))        \n",
    "        return 1 - ss_residual/ss_total\n",
    "    \n",
    "    def adjusted_rSquared(self, p):\n",
    "        n = self.true.shape[0]\n",
    "        return 1-(1-self.rSquared)*((n-1)/(n-p-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def kFold_crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor, verbose=False):\n",
    "    nVal,_ = train_data.shape\n",
    "    \n",
    "    # Validation indexes adjustment -------------------------------\n",
    "    elemPerFold, remainder = np.divmod(nVal,k) #the remainder will be distributed across the firsts folds\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    # in each fold put as many samples as the division quotient +1 if the remainder is still positive\n",
    "    # then decrease the division remainder by 1\n",
    "    for i in range(k): \n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation --------------------------------------------\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values())) # build all the hyp-par combination\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList): # for each fold\n",
    "        if verbose: print(\"#{} fold:\".format(row+1))\n",
    "        for col, params in enumerate(params_product):\n",
    "            \n",
    "            if verbose:\n",
    "                update = col*100/len(params_product) # just print completion rate\n",
    "                print(\"\\t[\"+\"#\"*(int(update/5))+\" \"*(int((100-update)/5))+\"] {}%\".format(update))\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)} # {argument_name:argument_value, ... }\n",
    "            \n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            rem = Regression_evaluationMetric(trainVal_values[valIdx], pred)\n",
    "            #val_results[row,col] = rem.rSquared()\n",
    "            val_results[row,col] = rem.rootMeanSquareError()\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            # project each column on the current residuals\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            # find the most correlated variable\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "            \n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance # update the solution vector: canonical basis over the found column\n",
    "            residual -= np.matmul(data_matrix, self.weights) # update the residual\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 92])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "np.where(mp.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 1.3161066161343946e+29\n",
      "Root Mean Square Error: 4576011566576380.0\n",
      "R^2 score: -7.51760865856762e+32\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 34, 38, 50, 76, 77, 92, 94, 96])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.057784752532006364\n",
      "Root Mean Square Error: 0.10298488563047162\n",
      "R^2 score: 0.6192389909724942\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression: # Iterative Soft Thresholding Algorithm (Proximal Gradient)\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        self.weights = np.zeros((data_matrix.shape[1],1))\n",
    "        n = data_matrix.shape[0]\n",
    "        # convergence step-size: n/(2*||X^t*X||_2)\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # gradient step of the lasso formulation\n",
    "            dist = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            coord_descent = (step/n)*np.matmul(data_matrix.T, dist)\n",
    "            self.weights -= coord_descent\n",
    "\n",
    "            # soft thresholding operator\n",
    "            upper = self.weights > softTresh  # elem to be reduced\n",
    "            lower = self.weights < -softTresh # elem to be increased\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  27,  49,  51,  71,  91,  92,  98, 101])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=10)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.8)\n",
    "np.where(lr.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.02762789280250509\n",
      "Root Mean Square Error: 0.8615803238857713\n",
      "R^2 score: -25.649988847827537\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   3,  11,  38,  44,  50,  74,  76,  93,  94, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.049602139168549705\n",
      "Root Mean Square Error: 0.10064093434112195\n",
      "R^2 score: 0.6363741017053486\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor: # Least Square Regression Tree with fixed parameter (no pruning)\n",
    "    class Node:\n",
    "        def __init__(self, isLeaf=False, feature=None, cut=None, average=None, left=None, right=None):\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature\n",
    "            self.cut = cut\n",
    "            self.avg = average\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            if self.cut:\n",
    "                print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"Leaf => {}\\n\".format(self.avg))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            if self.cut:\n",
    "                print(\"|    \"*level+\"{} => {}\".format(self.feature, self.cut))\n",
    "            else:\n",
    "                print(\"|    \"*level+\"Leaf: {}\".format(self.avg))                \n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElem_perLeaf, pruning=False):\n",
    "        if not pruning:\n",
    "            self.root = self.learn(X, y, depth, minElem_perLeaf)\n",
    "        else:\n",
    "            # train dataset, pruning dataset\n",
    "            X_trn, X_val, y_trn, y_val = trainTest_split(X, y, train_amount=0.7)\n",
    "            self.root = self.learn(X_trn, y_trn, depth, minElem_perLeaf)\n",
    "            self.prune(X_val, y_val)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElem_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElem_perLeaf: # leaf # or fraction error of the root node??? \n",
    "            return self.Node(isLeaf=True, average=np.mean(y))\n",
    "            \n",
    "        best_costDescent = 0 # split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "                \n",
    "                if sorted_x[i2]<sorted_x[i2+1]: # for a different value\n",
    "                    # try to maximise this value: it is directly correlated \n",
    "                    # to the possible split information gain\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "\n",
    "        return self.Node(feature=best_feature, cut=best_cut, average=np.mean(y),\n",
    "                         left = self.learn(X[left_idxs], y[left_idxs], depth-1, minElem_perLeaf),\n",
    "                         right = self.learn(X[~left_idxs], y[~left_idxs], depth-1, minElem_perLeaf))\n",
    "    \n",
    "    def prune(self, X, y):\n",
    "        # for statistics purposes check errors on different dataset portions and average them\n",
    "        # in order to decide whether to prune or not (same code of k-fold cross-validation)\n",
    "        n,_ = X.shape\n",
    "        folds = 5\n",
    "        elemPerFold, remainder = np.divmod(n, folds)\n",
    "        foldsIdxsList = []\n",
    "        start = 0\n",
    "        for i in range(folds): \n",
    "            end = start+elemPerFold+int(remainder>0)\n",
    "            foldsIdxsList.append(np.arange(start,end)) \n",
    "            remainder -= 1\n",
    "            start = end\n",
    "        \n",
    "        # recursive: start checking if the root receive a possible positive pruning from its sons\n",
    "        self.test_pruning(self.root, X, y, foldsIdxsList)\n",
    "        return self\n",
    "    \n",
    "    def test_pruning(self, node, X, y, foldIdxs):\n",
    "        if node.isLeaf: # leaf: start point of new pruning\n",
    "            return True\n",
    "        \n",
    "        # check sons response: if they both are positive to be pruned it means that the current node\n",
    "        # might need to become a leaf and not split deeper anymore\n",
    "        if self.test_pruning(node.left, X, y, foldIdxs) and self.test_pruning(node.right, X, y, foldIdxs):\n",
    "            \n",
    "            folds = len(foldIdxs)\n",
    "            results = np.empty(folds)\n",
    "            \n",
    "            # not pruned errors\n",
    "            for i, idxs in enumerate(foldIdxs):\n",
    "                pred = self.predict(X[idxs])\n",
    "                results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "            \n",
    "            not_prunErr = np.mean(results)\n",
    "            \n",
    "            # pruned errors\n",
    "            node.isLeaf = True\n",
    "            for i, idxs in enumerate(foldIdxs):\n",
    "                pred = self.predict(X[idxs])\n",
    "                results[i] = Regression_evaluationMetric(true=y[idxs], predicted=pred).rootMeanSquareError()\n",
    "            \n",
    "            # if pruning improves the prediction RMSE then keep current node as leaf\n",
    "            node.isLeaf = np.mean(results) <= not_prunErr\n",
    "            return node.isLeaf\n",
    "        \n",
    "        # else one of the sons performs a good predictive split: it must not be pruned\n",
    "        return False\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.cut:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.avg\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    |    Leaf: -0.02019552504390083\n",
      "|    |    |    |    42 => -0.17872340425531918\n",
      "|    |    |    |    |    Leaf: -0.3695978146131185\n",
      "|    |    |    2 => -0.5286024619840696\n",
      "|    |    |    |    |    Leaf: -0.6520880966297086\n",
      "|    |    |    |    3 => 0.35317173800928303\n",
      "|    |    |    |    |    Leaf: -0.41632397433009594\n",
      "|    |    51 => -0.9955245327332757\n",
      "|    |    |    |    |    Leaf: -0.45089029447259765\n",
      "|    |    |    |    58 => -0.9755327545382794\n",
      "|    |    |    |    |    Leaf: -0.732453525195108\n",
      "|    |    |    74 => -0.6731010278265229\n",
      "|    |    |    |    |    Leaf: -0.6230602283608756\n",
      "|    |    |    |    10 => -0.992874626974923\n",
      "|    |    |    |    |    Leaf: -0.793282857981379\n",
      "|    43 => -0.4063429137760157\n",
      "|    |    |    Leaf: 0.5643506924661275\n",
      "|    |    100 => -0.33296521258972944\n",
      "|    |    |    Leaf: -0.08904033288233844\n",
      "50 => -0.5638025594149909\n",
      "|    |    |    Leaf: -0.26444300165332085\n",
      "|    |    69 => 0.5410706545296922\n",
      "|    |    |    |    |    Leaf: -0.6353215878357985\n",
      "|    |    |    |    50 => -0.6530164533820841\n",
      "|    |    |    |    |    Leaf: -0.7517545370628013\n",
      "|    |    |    49 => -0.9938679611871324\n",
      "|    |    |    |    |    Leaf: -0.9218973533080941\n",
      "|    |    |    |    3 => 0.7817431665807116\n",
      "|    |    |    |    |    Leaf: -0.8248072170463756\n",
      "|    50 => -0.7692870201096892\n",
      "|    |    |    |    |    Leaf: -0.9101398533814575\n",
      "|    |    |    |    44 => 0.4935379644588045\n",
      "|    |    |    |    |    Leaf: -0.826671063374003\n",
      "|    |    |    36 => -0.9508158508158508\n",
      "|    |    |    |    Leaf: -0.3834143799163476\n",
      "|    |    51 => -0.9957746634397725\n",
      "|    |    |    |    |    Leaf: -0.5963953777683555\n",
      "|    |    |    |    41 => 0.32886597938144346\n",
      "|    |    |    |    |    Leaf: -0.9171614274368975\n",
      "|    |    |    2 => -0.8949001758560049\n",
      "|    |    |    |    |    Leaf: -0.9354619655231812\n",
      "|    |    |    |    40 => -0.08070004861448699\n",
      "|    |    |    |    |    Leaf: -0.969905856374414\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElem_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.05715938965530367\n",
      "Root Mean Square Error: 0.10766353387041577\n",
      "R^2 score: 0.5838568518087555\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    |    |    Leaf: 0.5063225168166138\n",
      "|    |    |    |    38 => 0.28077994428969355\n",
      "|    |    |    |    |    Leaf: 0.08044002184797978\n",
      "|    |    |    100 => -0.5212589729431253\n",
      "|    |    |    |    |    |    Leaf: 0.054104036176620345\n",
      "|    |    |    |    |    88 => 0.38118811881188097\n",
      "|    |    |    |    |    |    Leaf: -0.26555895642034694\n",
      "|    |    |    |    96 => 0.6098484848484849\n",
      "|    |    |    |    |    Leaf: -0.40729551748353676\n",
      "|    |    50 => -0.14040219378427793\n",
      "|    |    |    |    Leaf: -0.6968754738759112\n",
      "|    |    |    8 => -0.49423533401152936\n",
      "|    |    |    |    |    Leaf: -0.1376979237057943\n",
      "|    |    |    |    4 => -0.6630680828835103\n",
      "|    |    |    |    |    |    Leaf: -0.27286860736795154\n",
      "|    |    |    |    |    71 => -0.8497267443206817\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.2557267345199134\n",
      "|    |    |    |    |    |    |    31 => -0.7490651192778852\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.4761723045938175\n",
      "|    |    |    |    |    |    68 => -0.797442799461642\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.40177021637450155\n",
      "|    |    |    |    |    |    |    30 => -0.1943213296398893\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6035486036579172\n",
      "|    |    |    |    |    |    |    |    30 => -0.418421052631579\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.4928007667404257\n",
      "|    100 => -0.9383397754463464\n",
      "|    |    |    |    Leaf: -0.8659087596560268\n",
      "|    |    |    28 => -0.07252440725244075\n",
      "|    |    |    |    |    Leaf: -0.5407213072276474\n",
      "|    |    |    |    91 => -0.998161057178292\n",
      "|    |    |    |    |    Leaf: -0.6890299290823996\n",
      "|    |    0 => -0.9971393051324441\n",
      "|    |    |    |    |    Leaf: -0.7560402155942164\n",
      "|    |    |    |    20 => -0.862309480754327\n",
      "|    |    |    |    |    Leaf: -0.8442429163898337\n",
      "|    |    |    6 => -0.6543556804496186\n",
      "|    |    |    |    Leaf: -0.926119524938939\n",
      "50 => -0.4643510054844607\n",
      "|    |    |    |    Leaf: -0.284692133402234\n",
      "|    |    |    4 => -0.5693888211736027\n",
      "|    |    |    |    |    Leaf: -0.5366644141458309\n",
      "|    |    |    |    27 => -0.9665828107986333\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7298918348328056\n",
      "|    |    |    |    |    |    |    |    33 => 0.37159565580618215\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.6676913428115877\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    74 => -0.7247430433692655\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.603073807599387\n",
      "|    |    |    |    |    |    |    |    |    |    |    99 => -0.8990205767086764\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.552086231797644\n",
      "|    |    |    |    |    |    |    |    |    |    64 => -0.4659863945578231\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.698789380468436\n",
      "|    |    |    |    |    |    |    |    |    18 => -0.6368608799048752\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7612445692900437\n",
      "|    |    |    |    |    |    |    100 => -0.981409902448003\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7879362050765637\n",
      "|    |    |    |    |    |    92 => -0.9999042787403083\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.6750751944455486\n",
      "|    |    |    |    |    |    |    18 => -0.2858501783590963\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7404742660536276\n",
      "|    |    |    |    |    |    |    |    62 => -0.6994986729578295\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7723275606966031\n",
      "|    |    |    |    |    |    |    |    |    77 => -0.09212730318257956\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8692513239080795\n",
      "|    |    |    |    |    18 => -0.7067776456599286\n",
      "|    |    |    |    |    |    Leaf: -0.3372465361971265\n",
      "|    |    50 => -0.6402193784277879\n",
      "|    |    |    |    |    Leaf: -0.47785904519616806\n",
      "|    |    |    |    38 => 0.2518105849582173\n",
      "|    |    |    |    |    Leaf: -0.7163423576482898\n",
      "|    |    |    91 => -0.9734850104776975\n",
      "|    |    |    |    |    Leaf: -0.665015449011951\n",
      "|    |    |    |    2 => -0.5650149994827764\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7454877198354826\n",
      "|    |    |    |    |    |    |    |    97 => 0.5442075443566422\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.6421701345181887\n",
      "|    |    |    |    |    |    |    30 => -0.42257617728531854\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8144638337904626\n",
      "|    |    |    |    |    |    |    |    |    25 => -0.8917153284671533\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8830102511781567\n",
      "|    |    |    |    |    |    |    |    3 => 0.5789582258896339\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.738732316607262\n",
      "|    |    |    |    |    |    82 => -0.7477341389728096\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7349357148220288\n",
      "|    |    |    |    |    |    |    |    |    4 => -0.8788089848511231\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8464028125593496\n",
      "|    |    |    |    |    |    |    |    |    |    88 => 0.10891089108910867\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.876854622810023\n",
      "|    |    |    |    |    |    |    |    98 => -0.980246014178038\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7634742704703352\n",
      "|    |    |    |    |    |    |    49 => -0.995041294116086\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8360208998599904\n",
      "|    |    |    |    |    |    |    |    59 => -0.7729083665338645\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8733950789044799\n",
      "|    |    |    |    |    |    |    |    |    27 => -0.9854063351134654\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.923578200505762\n",
      "|    |    |    |    |    40 => -0.02236266407389398\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9237729282329195\n",
      "|    |    |    |    |    |    |    |    15 => -0.0024029796948217275\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8536214793320172\n",
      "|    |    |    |    |    |    |    |    |    |    |    82 => -0.7311178247734139\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8787761009011557\n",
      "|    |    |    |    |    |    |    |    |    |    2 => -0.9554153305058446\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8971282567697085\n",
      "|    |    |    |    |    |    |    |    |    51 => -0.9978117163911469\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9215533854799268\n",
      "|    |    |    |    |    |    |    2 => -0.9684493638150409\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9411641557684408\n",
      "|    |    |    |    |    |    35 => -0.6865505742867728\n",
      "|    |    |    |    |    |    |    Leaf: -0.8326633188023657\n",
      "|    49 => -0.9962392689320775\n",
      "|    |    |    |    Leaf: -0.40801691304533455\n",
      "|    |    |    93 => 0.026071072733311274\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9330272555021134\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    90 => -0.6766917293233081\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9485473203590096\n",
      "|    |    |    |    |    |    |    |    |    |    |    10 => -0.997699166576079\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.884750433941514\n",
      "|    |    |    |    |    |    |    |    |    |    57 => -0.9458103361766181\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9767142346066702\n",
      "|    |    |    |    |    |    |    |    |    |    |    24 => -0.9163471954033815\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9088568235922849\n",
      "|    |    |    |    |    |    |    |    |    46 => 0.4533316048742546\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9270222508817467\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    5 => -0.9735210675633077\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9553794761490432\n",
      "|    |    |    |    |    |    |    |    |    |    |    13 => 0.2705900430239703\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8922698784963761\n",
      "|    |    |    |    |    |    |    |    |    |    40 => -0.05979581915410792\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8678933335688912\n",
      "|    |    |    |    |    |    |    |    98 => -0.9943119728767967\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9389116349326232\n",
      "|    |    |    |    |    |    |    |    |    |    22 => -0.8764095794833113\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9010013937867021\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    74 => -0.8443218851842567\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.931881119900359\n",
      "|    |    |    |    |    |    |    |    |    |    |    74 => -0.8964652795186764\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8531987373264156\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    2 => -0.9795179476569773\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8977223039313114\n",
      "|    |    |    |    |    |    |    |    |    59 => -0.987434875881091\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9620008568412591\n",
      "|    |    |    |    |    |    |    49 => -0.9991166831261835\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9660200606855613\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    98 => -0.9967216789486957\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9314123179448933\n",
      "|    |    |    |    |    |    |    |    |    |    |    88 => 0.15841584158415822\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9046000990323976\n",
      "|    |    |    |    |    |    |    |    |    |    37 => -0.5773636519063087\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9881632238431539\n",
      "|    |    |    |    |    |    |    |    |    40 => 0.07292173067574137\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9639351293629096\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    65 => -0.1881533101045298\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9726340801985751\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    98 => -0.9876712712600522\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9763949066528865\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    99 => -0.8752416898274307\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9934642543591676\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    53 => -0.7468487394957983\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9838458922411699\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    64 => -0.45578231292517013\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9523393091386095\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    74 => -0.879167711205816\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9737745722296078\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    3 => 0.8322846828261992\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9498880364643373\n",
      "|    |    |    |    |    |    |    |    |    |    |    49 => -0.9998445665586846\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9402684474831392\n",
      "|    |    |    |    |    |    |    |    |    |    78 => -0.9924953095684803\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9243020497939607\n",
      "|    |    |    |    |    |    |    |    32 => -0.9281176863925109\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8666043910906176\n",
      "|    |    |    |    |    |    88 => -0.4752475247524754\n",
      "|    |    |    |    |    |    |    Leaf: -0.769774703749343\n",
      "|    |    |    |    |    82 => -0.8806646525679758\n",
      "|    |    |    |    |    |    Leaf: -0.7927857499348535\n",
      "|    |    |    |    40 => -0.13271754982984924\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9275104723290118\n",
      "|    |    |    |    |    |    |    4 => -0.9764931220616403\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8388851346874959\n",
      "|    |    |    |    |    |    47 => 0.2566483084185681\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9703790355298881\n",
      "|    |    |    |    |    |    |    |    |    87 => 0.15574783683559945\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9470397508584362\n",
      "|    |    |    |    |    |    |    |    |    |    29 => -0.8128396055544376\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9158458215738495\n",
      "|    |    |    |    |    |    |    |    52 => -0.8424327267071084\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8670703537341053\n",
      "|    |    |    |    |    |    |    99 => -0.9356601891908394\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9543287203978065\n",
      "|    |    |    |    |    |    |    |    40 => -0.3048128342245989\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.971017565245812\n",
      "|    |    |    |    |    |    |    |    |    20 => -0.4840265220012055\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9899078233140323\n",
      "|    |    |    |    |    2 => -0.9336919416571843\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8434630520332314\n",
      "|    |    |    |    |    |    |    |    93 => -0.15891730322152103\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9617168625476452\n",
      "|    |    |    |    |    |    |    |    |    |    |    16 => -0.23595976529756912\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9369382500121459\n",
      "|    |    |    |    |    |    |    |    |    |    94 => 0.41358953582590585\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9364082451096006\n",
      "|    |    |    |    |    |    |    |    |    |    |    1 => -0.1467391304347826\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9865621673667149\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    25 => -0.7973284671532846\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.97006257493067\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    18 => -0.5129607609988109\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9789835389310685\n",
      "|    |    |    |    |    |    |    |    |    63 => -0.8908971409792967\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.9235578345766364\n",
      "|    |    |    |    |    |    |    66 => -0.9150943396226416\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7351565501980893\n",
      "|    |    |    |    |    |    99 => -0.8513633002336052\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.9163828932084288\n",
      "|    |    |    |    |    |    |    |    61 => -0.8849465170884425\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9776037595014421\n",
      "|    |    |    |    |    |    |    |    |    |    |    97 => 0.2400477113463544\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9868779619544814\n",
      "|    |    |    |    |    |    |    |    |    |    93 => -0.8905679176353372\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9959690182689744\n",
      "|    |    |    |    |    |    |    |    |    |    |    22 => -0.8322128983594193\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9878275540715418\n",
      "|    |    |    |    |    |    |    |    |    53 => -0.5305934873949579\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9802735119744304\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    82 => -0.5670694864048338\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9723178439400565\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    72 => 0.9042743377214367\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.964776126560123\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    16 => -0.7581726739312658\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9541589926373486\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    71 => -0.9970648171734247\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9489325554039153\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    65 => -0.22648083623693388\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.934188849580192\n",
      "|    |    |    |    |    |    |    |    |    |    |    94 => -0.33661303391596253\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9113257630966419\n",
      "|    |    |    |    |    |    |    |    |    |    8 => -0.7773821634452356\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.937733257365964\n",
      "|    |    |    |    |    |    |    |    |    |    |    92 => -0.9999042787403083\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9932093121861377\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    86 => -0.4881693648816936\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9818075817201309\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    83 => -0.6086474501108647\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9688511702066578\n",
      "|    |    |    |    |    |    |    0 => -0.9965296690255765\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9669882660700326\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    30 => -0.7023545706371191\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9831988445893126\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    95 => 0.45047205155102654\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9874869768471284\n",
      "|    |    |    |    |    |    |    |    |    |    |    13 => 0.3463429625076827\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9939038394438482\n",
      "|    |    |    |    |    |    |    |    |    |    0 => -0.9998380867764622\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9366114136555763\n",
      "|    |    |    |    |    |    |    |    |    46 => 0.642727508426238\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9782256319204286\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    18 => 0.08466111771700358\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9575721260606846\n",
      "|    |    |    |    |    |    |    |    |    |    |    85 => -0.38217338217338215\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.927541704760769\n",
      "|    |    |    |    |    |    |    |    |    |    22 => -0.914986799924571\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9795029437355629\n",
      "|    |    |    |    |    |    |    |    18 => -0.28489892984542214\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9503032069713312\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    61 => -0.9219932168014611\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9779900741859641\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    41 => -0.2237113402061855\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9856482243889327\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    36 => -0.366899766899767\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.984244132035999\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    47 => 0.43036978756884336\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9877716091096064\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    24 => -0.5354306974991758\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9931563116958833\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    3 => 0.9297576070139248\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9974736432978671\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    58 => -0.9842146803472771\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9991714256690208\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9994509446009257\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9998315227268593\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.999838360278529\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9998425995605642\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9998628387135065\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    0 => -0.9999195903923648\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    7 => -0.5576639947652544\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9922354281777106\n",
      "|    |    |    |    |    |    |    |    |    |    |    14 => -0.656967840735069\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9914810545330877\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    39 => -0.4919429810969941\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9662510544889207\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    88 => -0.24752475247524763\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9861374273267582\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    25 => -0.7709562043795621\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9694562591370637\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    30 => -0.2878116343490305\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9779517960541134\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    74 => -0.8345449987465531\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9988984731442099\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    69 => -0.21348071087993076\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9901684090577838\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    29 => -0.6794123566109881\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9848736600813558\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    61 => -0.9676493608139838\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9972104075296029\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    56 => -0.9555069292487235\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -1.0\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    48 => 0.6609025727541119\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9859919704257264\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    15 => 0.05010212663702973\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9893866518265293\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    6 => -0.7175832998795664\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9977580792622331\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    39 => -0.6158971180663154\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.983157621985781\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    53 => -0.9149159663865547\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.965456930476607\n",
      "|    |    |    |    |    |    |    |    |    |    6 => -0.7854275391409072\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9491239460631677\n",
      "|    |    |    |    |    |    |    |    |    36 => -0.942890442890443\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.958247980460486\n",
      "|    |    3 => 0.7875193398659103\n",
      "|    |    |    |    |    |    |    Leaf: -0.6966340271980849\n",
      "|    |    |    |    |    |    30 => -0.243213296398892\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7906312800060067\n",
      "|    |    |    |    |    |    |    26 => -0.720648514126775\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9248983053093242\n",
      "|    |    |    |    |    6 => -0.7541148133279808\n",
      "|    |    |    |    |    |    Leaf: -0.5777791658858559\n",
      "|    |    |    |    40 => 0.0811861934856587\n",
      "|    |    |    |    |    |    Leaf: -0.8301422621492582\n",
      "|    |    |    |    |    49 => -0.9982447394309999\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.9424345971496533\n",
      "|    |    |    |    |    |    |    32 => -0.7512537612838516\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8614920521348155\n",
      "|    |    |    |    |    |    67 => -0.48421558036250145\n",
      "|    |    |    |    |    |    |    Leaf: -0.8092247353288018\n",
      "|    |    |    69 => 0.1480277416558301\n",
      "|    |    |    |    |    Leaf: -0.4579728195819145\n",
      "|    |    |    |    100 => -0.326891220320265\n",
      "|    |    |    |    |    |    |    Leaf: -0.373763873982501\n",
      "|    |    |    |    |    |    45 => 0.9603504928806135\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7063331169146648\n",
      "|    |    |    |    |    |    |    |    68 => -0.6367765814266488\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.860756915705174\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    52 => -0.6277803701975424\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8899783434107876\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    36 => -0.8060606060606061\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8102008276909896\n",
      "|    |    |    |    |    |    |    |    |    |    |    25 => -0.886065693430657\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9466579510304914\n",
      "|    |    |    |    |    |    |    |    |    |    29 => -0.8243107265043268\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9556768677888048\n",
      "|    |    |    |    |    |    |    |    |    |    |    90 => -0.7368421052631579\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.898085357289555\n",
      "|    |    |    |    |    |    |    |    |    88 => -0.014851485148515087\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9206435437305572\n",
      "|    |    |    |    |    |    |    |    |    |    47 => 0.23949645948072384\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9644700417182563\n",
      "|    |    |    |    |    |    |    44 => 0.4664781906300485\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.7231629809242403\n",
      "|    |    |    |    |    |    |    |    73 => 0.32148687680522425\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8244204175555291\n",
      "|    |    |    |    |    |    |    |    |    |    |    75 => -0.3926085305657511\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8599314821439877\n",
      "|    |    |    |    |    |    |    |    |    |    51 => -0.9995899968841684\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8861973473254627\n",
      "|    |    |    |    |    |    |    |    |    90 => -0.7293233082706765\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.7539010569181098\n",
      "|    |    |    |    |    10 => -0.995820589618609\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8523830345430694\n",
      "|    |    |    |    |    |    |    |    |    |    |    94 => 0.7131612455145271\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9162022989453393\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    33 => 0.10893901420217211\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8885655858983362\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    50 => -0.8193784277879341\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9458953328651624\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    86 => -0.7235367372353674\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8363079858488691\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    40 => -0.020418084589207575\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9719156291084582\n",
      "|    |    |    |    |    |    |    |    |    |    18 => -0.6558858501783591\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.7983419679965373\n",
      "|    |    |    |    |    |    |    |    |    41 => -0.12938144329896906\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.6725276375473139\n",
      "|    |    |    |    |    |    |    |    41 => -0.13608247422680408\n",
      "|    |    |    |    |    |    |    |    |    |    Leaf: -0.8229960293799384\n",
      "|    |    |    |    |    |    |    |    |    16 => -0.0889913383626712\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9147222111804534\n",
      "|    |    |    |    |    |    |    |    |    |    |    87 => -0.4548825710754017\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9318634530702743\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    36 => -0.4004662004662006\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9561703612425083\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    35 => -0.5146350500185253\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9599129810469265\n",
      "|    |    |    |    |    |    |    |    |    |    14 => -0.9004594180704442\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.8838405921921445\n",
      "|    |    |    |    |    |    |    43 => 0.09953277644060599\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.7439634649953846\n",
      "|    |    |    |    |    |    49 => -0.9989460854466911\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9775076014691233\n",
      "|    |    |    |    |    |    |    |    |    |    54 => -0.09689394876871682\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9412639733584202\n",
      "|    |    |    |    |    |    |    |    |    40 => -0.23578026251823042\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9762060924063548\n",
      "|    |    |    |    |    |    |    |    |    |    |    55 => -0.010681818181818126\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9942052797321709\n",
      "|    |    |    |    |    |    |    |    |    |    14 => -0.7733537519142419\n",
      "|    |    |    |    |    |    |    |    |    |    |    Leaf: -0.9519638889993065\n",
      "|    |    |    |    |    |    |    |    75 => -0.7410454372864195\n",
      "|    |    |    |    |    |    |    |    |    Leaf: -0.8676489424193841\n",
      "|    |    |    |    |    |    |    98 => -0.9982627700411891\n",
      "|    |    |    |    |    |    |    |    Leaf: -0.8982277014633813\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=100, minElem_perLeaf=10, pruning=True)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.05979815613844872\n",
      "Root Mean Square Error: 0.12097742246932931\n",
      "R^2 score: 0.4745708761706281\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  43,  51,   2, 100,  69,  74,  49,  42,   3,   8,  36,  65,\n",
       "        10,  85,  58,  32,  66,  71,  40,  82,  41,  48,  88,   0,  37,\n",
       "        46,  92,  44,  47,  86,  99,  28,  24,  39,  22,   9,  52,  84,\n",
       "        54,  79,  14,  21,  94,  81,  76,  93,  45,  91,  15,  75,  18,\n",
       "        16,  35,  95,  11,  90,   7,  67,  60,  96,  83,  62,  26,  38,\n",
       "        61,  98,   5,   1,  33,  29,  77,  31,  73,  53,  27,  19,  59,\n",
       "        55,  23,  30,  17,  78,  72,  89,  57,  64,   4,  56,  25,  68,\n",
       "        63,  87,  34,   6,  80,  97, 101,  20,  13,  12,  70])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06408277326879126\n",
      "Root Mean Square Error: 0.11786492244595967\n",
      "R^2 score: 0.5012594997718978\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalRandomForest_regressor: # post-pruning (kinda) with cross-validation or greedy \n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.boot_samplesIdxs = []\n",
    "        self.oob_error = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElems_perLeaf):\n",
    "        n = X.shape[0]\n",
    "        n_learn = int(n/3) # Bootstrap amount to be taken aside\n",
    "        \n",
    "        #val_folds = 3\n",
    "        #params_dict = {\"depth\":depths, \"minElem_perLeaf\":minElems_perLeaf}\n",
    "        \n",
    "        # Fitting the forest -----------------------------------\n",
    "        for i in range(self.n_trees):\n",
    "            print(\"Fitting #{} tree\".format(i+1))\n",
    "            \n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            self.boot_samplesIdxs.append(bootstrap_idxs)\n",
    "                        \n",
    "            dt = NumericalDecisionTree_regressor()\n",
    "            \n",
    "            # find the best hyp-par for the current setting (bootstrapping)\n",
    "            #win_params = kFold_crossValidation_selectionGrid(val_folds, params_dict, \n",
    "            #                                                 X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "            #                                                 dt, verbose=True)\n",
    "            #self.trees.append(dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "            #                         depth=win_params[0], minElem_perLeaf=win_params[1]))\n",
    "            \n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=depth, minElem_perLeaf=minElems_perLeaf, pruning=True))\n",
    "        \n",
    "        # Out-Of-Bag Estimate for the forest -------------------\n",
    "        oob_errors = []\n",
    "        for sampleIdx in range(n):\n",
    "            missingBoot_TreesIdx = [idx for idx,bootstrap_idxs in enumerate(self.boot_samplesIdxs) \n",
    "                                    if sampleIdx not in bootstrap_idxs]\n",
    "\n",
    "            if len(missingBoot_TreesIdx) == 0: continue\n",
    "            \n",
    "            regr_results = np.empty(len(missingBoot_TreesIdx)) # regression estimate of the selected trees\n",
    "            for i, missing_tree in enumerate(missingBoot_TreesIdx):\n",
    "                # reshape in order to correctly use the decision_tree.predict(...): it needs a matrix (num,dim)\n",
    "                # while numpy matrix indexing returns (dim,)\n",
    "                regr_results[i] = self.trees[missing_tree].predict(X[sampleIdx].reshape(1,-1))\n",
    "            \n",
    "            # done at this level of granularity because a sample might end up in \n",
    "            # being part of no bootstrap set of any tree (so we cannot predict wich value in y will be used)\n",
    "            oob_errors.append(np.square(y[sampleIdx]-np.mean(regr_results)))\n",
    "            #oob_errors.append(r2_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            #oob_errors.append(explained_variance_score(np.mean(regr_results),y[sampleIdx]))\n",
    "            \n",
    "        self.oob_error = np.sqrt(np.mean(oob_errors))\n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"Trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, tree in enumerate(self.trees):\n",
    "            results[row] = tree.predict(X)\n",
    "            \n",
    "        return np.mean(results,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "Fitting #2 tree\n",
      "Fitting #3 tree\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11144806836915046"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(3)\n",
    "# no train and test, cause it's a forest\n",
    "nrf.fit(data, values, depth=100, minElems_perLeaf=5);\n",
    "nrf.oob_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/.conda/envs/bcb/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 50,  49,   3, 100,  51,  69,  91,  43,   2,  92,  38,  71,  41,\n",
       "        99,  44,  93,  15,  10,   6,  40,  36,  88,  46,  61,  24,  89,\n",
       "         4,  27,  63,  42,  75,  23,  96,  74,   8,  35,  26,  77,  68,\n",
       "        86,  48,  67,  82,  65,  28,  22,  58,  34,  14,  52,  11,  73,\n",
       "        78,   7,  32,   0,  95,  45,  25,  21,  20,  98,  47,  54,  64,\n",
       "        90,  30,   5,  13,  60,  55,  94,  80,  17,  29,  59,  97,  18,\n",
       "       101,  53,  33,  12,  56,   1,  16,  85,   9,  72,  19,  79,  83,\n",
       "        39,  62,  37,  66,  76,  87,  31,  81,  57,  84,  70])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.05576825819482322\n",
      "Root Mean Square Error: 0.09610909195758337\n",
      "R^2 score: 0.6683847951893649\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.009269216592095427\n",
      "Root Mean Square Error: 0.09711437801726346\n",
      "R^2 score: 0.6614112274436692\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[2,2.05,2.1,2.2,3]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = kFold_crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting #1 tree\n",
      "Fitting #2 tree\n",
      "Fitting #3 tree\n",
      "Fitting #4 tree\n",
      "Fitting #5 tree\n",
      "Residual variance: 0.053474362312960155\n",
      "Root Mean Square Error: 0.10121891146292093\n",
      "R^2 score: 0.6321855288668612\n"
     ]
    }
   ],
   "source": [
    "rf = NumericalRandomForest_regressor(5)\n",
    "rf.fit(trainVal_data, trainVal_values, depth=100, minElems_perLeaf=10);\n",
    "\n",
    "pred = rf.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11119805045257683"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.oob_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_SupportVector_regression:\n",
    "    def __init__(self, weight=None, alpha=None, bias=None):\n",
    "        self.x = alpha\n",
    "        self.w = weight\n",
    "        self.bias = bias\n",
    "        self.Nabla = None\n",
    "                \n",
    "    def SMO2_ab(self, n, H, f, a, LB, UB, maxiter, eps, alpha_s):\n",
    "        \"\"\"\n",
    "        % min_{x} .5 x H x + f' x \n",
    "        %         LB <= x <= UB\n",
    "        %         a' x = b\n",
    "        % n         grandezza problema length(x)\n",
    "        % maxiter   max num it\n",
    "        % eps       precisione\n",
    "        % alpha_s   punto di inizio valido per x\n",
    "        % Nabla     ....\n",
    "        % err       flag di ok\n",
    "        % x         valore della soluzione ottima\n",
    "        % bias      ....\n",
    "        \"\"\"\n",
    "        self.x = alpha_s\n",
    "        self.Nabla = f\n",
    "        for i in range(n):\n",
    "            if (self.x[i] != 0.0):\n",
    "                for j in range(n):\n",
    "                    self.Nabla[j] += H[j,i] * self.x[i]\n",
    "        iter_ = 0\n",
    "        while True:\n",
    "            minF_up = float(\"inf\");\n",
    "            maxF_low = float(\"-inf\");\n",
    "            for i in range(n): \n",
    "                F_i = self.Nabla[i]/a[i]\n",
    "                if (LB[i] < self.x[i]) and (self.x[i] < UB[i]) :\n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == LB[i])) or ((a[i] < 0) and (self.x[i] == UB[i]))) : \n",
    "                    if (minF_up > F_i):\n",
    "                        minF_up = F_i\n",
    "                        u = i\n",
    "                elif (((a[i] > 0) and (self.x[i] == UB[i])) or ((a[i] < 0) and (self.x[i] == LB[i]))) : \n",
    "                    if (maxF_low < F_i):\n",
    "                        maxF_low = F_i\n",
    "                        v = i\n",
    "            if ((maxF_low - minF_up) <= eps):\n",
    "                err = 0.0\n",
    "                break\n",
    "\n",
    "            iter_ += 1\n",
    "            if (iter_ >= maxiter):\n",
    "                err = 1.0\n",
    "                break\n",
    "\n",
    "            if (a[u] > 0):\n",
    "                tau_lb = (LB[u]-self.x[u])*a[u] \n",
    "                tau_ub = (UB[u]-self.x[u])*a[u] \n",
    "            else:\n",
    "                tau_ub = (LB[u]-self.x[u])*a[u] \n",
    "                tau_lb = (UB[u]-self.x[u])*a[u]\n",
    "\n",
    "            if (a[v] > 0):\n",
    "                tau_lb = max(tau_lb,(self.x[v]-UB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-LB[v])*a[v]) \n",
    "            else:\n",
    "                tau_lb = max(tau_lb,(self.x[v]-LB[v])*a[v]) \n",
    "                tau_ub = min(tau_ub,(self.x[v]-UB[v])*a[v])\n",
    "\n",
    "            tau = (self.Nabla[v]/a[v]-self.Nabla[u]/a[u])/(H[u,u]/(a[u]*a[u])\n",
    "                                                           +H[v,v]/(a[v]*a[v])\n",
    "                                                           -2*H[v,u]/(a[u]*a[v]))\n",
    "            tau = min(max(tau,tau_lb),tau_ub)\n",
    "            self.x[u] += tau/a[u]\n",
    "            self.x[v] -= tau/a[v]\n",
    "\n",
    "            for i in range(n):\n",
    "                self.Nabla[i] += H[u,i]*tau/a[u] - H[v,i]*tau/a[v]\n",
    "\n",
    "        tsv = 0\n",
    "        self.bias = 0.0\n",
    "\n",
    "        for k in range(n):\n",
    "            if ((self.x[k] > LB[k]) and (self.x[k] < UB[k])):\n",
    "                self.bias -= self.Nabla[k]/a[k]\n",
    "                tsv += 1\n",
    "\n",
    "        if (tsv > 0):\n",
    "            self.bias /= tsv\n",
    "        else:    \n",
    "            self.bias = -(maxF_low + minF_up)/2.0\n",
    "\n",
    "        return err\n",
    "    \n",
    "    def fit(self, X, y, C):\n",
    "        n = X.shape[0]\n",
    "        cov = np.matmul(X, X.T)\n",
    "        Q = np.matmul(np.matmul(np.diag(y.flatten()), cov),\n",
    "                      np.diag(y.flatten()))\n",
    "        \n",
    "        if self.SMO2_ab(n,Q,-np.ones(n),y.flatten(),\n",
    "                   np.zeros(n),C*np.ones(n),10000000,.0001,np.zeros(n)):\n",
    "            print(\"Problem in SMO\")\n",
    "            \n",
    "        self.w = np.matmul(np.matmul(X.T, np.diag(y.flatten())),\n",
    "                           self.x)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.matmul(X, self.w) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 5.6294441029468105\n",
      "Root Mean Square Error: 8.921663229705644\n",
      "R^2 score: -2856.5717143966876\n"
     ]
    }
   ],
   "source": [
    "lsvr = linear_SupportVector_regression()\n",
    "lsvr.fit(trainVal_data, trainVal_values, C=1.0);\n",
    "\n",
    "pred = lsvr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  10,   43,   97,  110,  112,  114,  121,  122,  129,  244,  246,\n",
       "         250,  258,  267,  299,  319,  373,  382,  387,  421,  459,  499,\n",
       "         546,  547,  564,  579,  594,  618,  631,  699,  842,  908,  921,\n",
       "         986, 1002, 1013, 1031, 1076, 1103, 1119, 1121, 1186, 1205, 1260,\n",
       "        1302, 1420, 1437, 1444, 1469, 1489]),)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lsvr.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel=\"linear\", tol=.0001, C=1)\n",
    "svr.fit(trainVal_data, trainVal_values.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(svr.dual_coef_)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual variance: 0.06464753396419612\n",
      "Root Mean Square Error: 0.1109680542840853\n",
      "R^2 score: 0.5579194281626387\n"
     ]
    }
   ],
   "source": [
    "pred = svr.predict(test_data)\n",
    "rem = Regression_evaluationMetric(test_values, pred)\n",
    "\n",
    "print(\"Residual variance: {}\".format(np.var(test_values-pred)))\n",
    "\n",
    "print(\"Root Mean Square Error: {}\".format(rem.rootMeanSquareError()))\n",
    "print(\"R^2 score: {}\".format(rem.rSquared()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
