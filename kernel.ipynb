{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    nanRows, nanCols = np.where(df.isna())\n",
    "    for j in nanCols:\n",
    "        available = df.iloc[~nanRows, j]\n",
    "        value = value_withStrategy(available, strategy)\n",
    "        df.iloc[nanRows,j] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,values = drop_sample(pred_features, regr_values[\"robbPerPop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data.shape[0]\n",
    "\n",
    "trVl_Amount = int(n*0.7)\n",
    "indexes = np.random.permutation(n)\n",
    "idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "trainVal_data = data[idxTrVl]\n",
    "test_data = data[idxTs]\n",
    "trainVal_values = values[idxTrVl]\n",
    "test_values = values[idxTs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "\n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance\n",
    "            residual -= np.matmul(data_matrix, self.weights)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "np.where(mp.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 2.163497390032889e+31\n",
      "R^2 score: -4.8861325862267744e+32\n",
      "Explained Variance Score: -3.8568662080651845e+29\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 38, 48, 50, 74, 76, 77, 92, 94])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07356269296755365\n",
      "R^2 score: 0.668290263545612\n",
      "Explained Variance Score: 0.6693114540463263\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression:\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        self.weights = np.zeros((data_matrix.shape[1],1))\n",
    "        n = float(data_matrix.shape[0])\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            dist = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            coord_descent = (step/n)*np.matmul(data_matrix.T, dist)\n",
    "            self.weights -= coord_descent\n",
    "\n",
    "            upper = self.weights > softTresh\n",
    "            lower = self.weights < -softTresh\n",
    "\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  27,  49,  51,  71,  91,  92,  98, 101])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=10)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.8)\n",
    "np.where(lr.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.7144181261141582\n",
      "R^2 score: -15.13471641925296\n",
      "Explained Variance Score: 0.005766194848579209\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  69,  74,  76,  94, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.06650733267764729\n",
      "R^2 score: 0.6407210418367291\n",
      "Explained Variance Score: 0.641124679881149\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regression:\n",
    "    class Node:\n",
    "        def __init__(self, value, isLeaf=False, feature=None, left=None, right=None):\n",
    "            self.value = value\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.value))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            print('\\t'*level+\"{} => {}\".format(self.feature, self.value))\n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElem_perLeaf):        \n",
    "        self.root = self.learn(X, y, depth, minElem_perLeaf)\n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElem_perLeaf):      \n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElem_perLeaf: #or other condition\n",
    "            return self.Node(value=np.mean(y), isLeaf=True)\n",
    "            \n",
    "        best_costDescent = 0 #split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx,i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "\n",
    "                if sorted_x[i2]<sorted_x[i2+1]:\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "\n",
    "        return self.Node(value=best_cut, feature=best_feature,\n",
    "                        left=self.learn(X[left_idxs],y[left_idxs],depth-1,minElem_perLeaf),\n",
    "                        right=self.learn(X[~left_idxs],y[~left_idxs],depth-1,minElem_perLeaf))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        y = np.zeros(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.value:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.value\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tNone => 0.5287888357411749\n",
      "\t\t100 => -0.3436407141542426\n",
      "\t\t\tNone => -0.1669125969718251\n",
      "\t50 => 0.026325411334552018\n",
      "\t\t\tNone => -0.5177077695660993\n",
      "\t\t100 => -0.9379716547027425\n",
      "\t\t\tNone => -0.768165576743706\n",
      "50 => -0.6329067641681901\n",
      "\t\t\tNone => -0.8224806053982927\n",
      "\t\t73 => -0.7151827200803716\n",
      "\t\t\tNone => -0.019698515544602224\n",
      "\t50 => -0.7736745886654479\n",
      "\t\t\tNone => -0.8384565441845723\n",
      "\t\t51 => -0.9924216637196692\n",
      "\t\t\tNone => -0.9490712734109973\n"
     ]
    }
   ],
   "source": [
    "dt = NumericalDecisionTree_regression()\n",
    "dt.fit(trainVal_data, trainVal_values, depth=3, minElem_perLeaf=1)\n",
    "dt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07585356691562414\n",
      "R^2 score: 0.531303436542665\n",
      "Explained Variance Score: 0.5313166120306662\n"
     ]
    }
   ],
   "source": [
    "pred = dt.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50, 100,   3,  66,  92,   2,  51,  71,  52,  88,  10,  26,  46,\n",
       "        99,  37,  58,   6,  86,   7,  34,   0,  93,  40,  42,  56,  18,\n",
       "        27,  87,  43,  35,  55,  73,  38,  39,  75,  83,  15,  48,  96,\n",
       "        17,  69,  61,  25,  64,  91,  76,  97,  23,  44,  13,  19,  89,\n",
       "        22,  45,  36,  95,   8,  32,  74,  90,  57,   9,  21,  94,  84,\n",
       "         5,  41,  78,  68,  82,  29,  16,  49,  65,  14, 101,  24,  77,\n",
       "        47,  11,  80,  28,  98,  31,  33,  60,   4,  72,  54,  67,  70,\n",
       "         1,  59,  79,  12,  20,  30,  63,  62,  53,  85,  81])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.08746174867533872\n",
      "R^2 score: 0.6186693471494426\n",
      "Explained Variance Score: 0.619272778810366\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest project class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/.conda/envs/bcb/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 50, 100,  49,  51,   3,  46,  92,  99,   2,  44,  74,  40,  14,\n",
       "         4,  78,  41,  59,  32,  63,  93,  71,   6,  17,  96,  48,  69,\n",
       "        15,  13,  37,  31,   1,  36,  54,  61,  30,  73,   0,  72,  66,\n",
       "        27,  98,  58,  35,  25,  24,  10,  65,   7,  67,  88,  89,  18,\n",
       "        20,   8,  91,  23,  77,  90,  39,  80,  97,  28,  38,  21,  60,\n",
       "        47,  22,  43,  82,  42,  26,  62,   5,  94,   9,  52,  87,  33,\n",
       "        57,  68,  16, 101,  76,  84,  56,  45,  95,  75,  53,  34,  86,\n",
       "        29,  19,  11,  55,  79,  64,  83,  85,  12,  81,  70])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07471129166374027\n",
      "R^2 score: 0.7314293001541479\n",
      "Explained Variance Score: 0.731829832097239\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor):\n",
    "    nVal = train_data.shape[0]\n",
    "    \n",
    "    # Validation indexes adjustment\n",
    "    elemPerFold, remainder = np.divmod(nVal,k)\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values()))\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for rowVal, valIdx in enumerate(valIdxList):\n",
    "        for colVal, params in enumerate(params_product):\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)}\n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            #val_results[rowVal,colVal] = r2_score(trainVal_values[valIdx],pred)\n",
    "            #val_results[rowVal,colVal] = explained_variance_score(trainVal_values[valIdx],pred)\n",
    "            val_results[rowVal,colVal] = np.mean(np.square(train_values[valIdx]-pred))\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.01245992452487423\n",
      "R^2 score: 0.7185998766470033\n",
      "Explained Variance Score: 0.7193535535276633\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[2,2.05,2.1,2.2,3]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got multiple values for argument 'depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ab263bc93063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNumericalDecisionTree_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwin_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossValidation_selectionGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainVal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainVal_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainVal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainVal_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_regulariser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminElem_perLeaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Square Error: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_values\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got multiple values for argument 'depth'"
     ]
    }
   ],
   "source": [
    "k = 3 \n",
    "params_dict = {\"depth\":[10,15,20,30],\"minElem_perLeaf\":[5,10,20,30]}\n",
    "\n",
    "dt = NumericalDecisionTree_regression()\n",
    "win_params = crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, dt)\n",
    "dt.fit(trainVal_data, trainVal_values, win_regulariser, depth=win_params[0], minElem_perLeaf=win_params[1])\n",
    "pred = dt.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
