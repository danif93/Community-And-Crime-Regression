{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "#import os\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = pd.read_csv(\"commViolUnnormData.txt\", na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop first non predictive features (communityname, state, countyCode, communityCode, \"fold\")\n",
    "pred_features = cmp[cmp.columns[5:-18]]\n",
    "regr_values = cmp[cmp.columns[-18:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features with a lot of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: 124 features\n",
      "After dropping: 102 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping: {} features\".format(str(pred_features.shape[1])))\n",
    "\n",
    "#drop features that contain at least some threshold (from the total) of NaN values\n",
    "cut_tresh = 0.75\n",
    "to_drop = pred_features.columns[pred_features.count() < pred_features.shape[0]*cut_tresh]\n",
    "\n",
    "pred_features = pred_features.drop(columns=to_drop)\n",
    "\n",
    "print(\"After dropping: {} features\".format(str(pred_features.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing on features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def value_withStrategy(v, strat):\n",
    "    if strat == \"mean\":\n",
    "        return np.mean(v)\n",
    "    if strat == \"median\":\n",
    "        return np.median(v)\n",
    "    if strat == \"most_frequent\":\n",
    "        return Counter(v).most_common(1)[0][0]\n",
    "    print(\"Invalid imputing strategy!\")\n",
    "        \n",
    "def imputing(df, strategy):\n",
    "    nanRows, nanCols = np.where(df.isna())\n",
    "    for j in nanCols:\n",
    "        available = df.iloc[~nanRows, j]\n",
    "        value = value_withStrategy(available, strategy)\n",
    "        df.iloc[nanRows,j] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing(pred_features, \"mean\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dependent Variable and drop possible missing values on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sample(df, vals):\n",
    "    idxRow = np.where(vals.isna())[0]\n",
    "    return df.drop(index=idxRow).values, vals.drop(index=idxRow).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,values = drop_sample(pred_features, regr_values[\"robbPerPop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(matrix, strat):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        mi = np.min(matrix[:,j])\n",
    "        ma = np.max(matrix[:,j])\n",
    "        di = ma-mi\n",
    "        if (di > 1e-6):\n",
    "            if strat==\"0_mean,1_std\":\n",
    "                matrix[:,j] = (matrix[:,j]-np.mean(matrix[:,j]))/np.std(matrix[:,j])\n",
    "            elif strat==\"[0,1]\":\n",
    "                matrix[:,j] = (matrix[:,j]-mi)/di\n",
    "            elif strat==\"[-1,1]\":\n",
    "                matrix[:,j] = 2*((matrix[:,j]-mi)/di)-1\n",
    "            else:\n",
    "                print(\"Invalid normalisation strategy!\")\n",
    "        else:\n",
    "            matrix[:,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"[-1,1]\"\n",
    "normalise(data,strategy)\n",
    "normalise(values,strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data.shape[0]\n",
    "\n",
    "trVl_Amount = int(n*0.7)\n",
    "indexes = np.random.permutation(n)\n",
    "idxTrVl = np.sort(indexes[0:trVl_Amount])\n",
    "idxTs = np.sort(indexes[trVl_Amount:])\n",
    "\n",
    "trainVal_data = data[idxTrVl]\n",
    "test_data = data[idxTs]\n",
    "trainVal_values = values[idxTrVl]\n",
    "test_values = values[idxTs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matching Pursuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matchingPursuit:\n",
    "    def __init__(self, iterations, weights = None, indexes = None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        self.indexes = indexes\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect):\n",
    "        residual = output_vect.copy()\n",
    "        self.weights = np.zeros((data_matrix.shape[1], 1))\n",
    "        self.indexes = []\n",
    "\n",
    "        #data_2norm = np.sqrt(np.sum(np.square(data_matrix), axis=0))\n",
    "        data_2norm = np.linalg.norm(data_matrix, ord=2, axis=0).reshape(1,-1)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            projection = np.matmul(residual.T, data_matrix)\n",
    "            k = np.argmax(np.divide(np.square(projection), data_2norm))\n",
    "            self.indexes.append(k)\n",
    "\n",
    "            distance = projection[0,k]/np.linalg.norm(data_matrix[:,k], ord=2)\n",
    "            self.weights[k,0] += distance\n",
    "            residual -= np.matmul(data_matrix, self.weights)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = matchingPursuit(iterations=10)\n",
    "mp.fit(trainVal_data, trainVal_values)\n",
    "np.where(mp.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 2.163497390032889e+31\n",
      "R^2 score: -4.8861325862267744e+32\n",
      "Explained Variance Score: -3.8568662080651845e+29\n"
     ]
    }
   ],
   "source": [
    "pred = mp.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 38, 48, 50, 74, 76, 77, 92, 94])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import orthogonal_mp\n",
    "omp_coef = orthogonal_mp(trainVal_data, trainVal_values)\n",
    "np.where(omp_coef)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07356269296755365\n",
      "R^2 score: 0.668290263545612\n",
      "Explained Variance Score: 0.6693114540463263\n"
     ]
    }
   ],
   "source": [
    "pred = np.matmul(test_data, omp_coef)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L1 Penalty (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_regression:\n",
    "    def __init__(self, iterations, weights=None):\n",
    "        self.iterations = iterations\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, data_matrix, output_vect, _lambda):\n",
    "        self.weights = np.zeros((data_matrix.shape[1],1))\n",
    "        n = float(data_matrix.shape[0])\n",
    "        step = n/(2*np.linalg.norm(np.matmul(data_matrix.T, data_matrix), ord=2))\n",
    "        softTresh = step*_lambda\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            dist = np.matmul(data_matrix, self.weights) - output_vect\n",
    "            coord_descent = (step/n)*np.matmul(data_matrix.T, dist)\n",
    "            self.weights -= coord_descent\n",
    "\n",
    "            upper = self.weights > softTresh\n",
    "            lower = self.weights < -softTresh\n",
    "\n",
    "            self.weights[upper] -= softTresh\n",
    "            self.weights[lower] += softTresh\n",
    "            self.weights[~upper & ~lower] = 0\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  10,  27,  49,  51,  71,  91,  92,  98, 101])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = lasso_regression(iterations=10)\n",
    "lr.fit(trainVal_data, trainVal_values, 0.8)\n",
    "np.where(lr.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.7144181261141582\n",
      "R^2 score: -15.13471641925296\n",
      "Explained Variance Score: 0.005766194848579209\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  38,  44,  50,  69,  74,  76,  94, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(trainVal_data, trainVal_values)\n",
    "np.where(lasso.coef_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.06650733267764729\n",
      "R^2 score: 0.6407210418367291\n",
      "Explained Variance Score: 0.641124679881149\n"
     ]
    }
   ],
   "source": [
    "pred = lasso.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalDecisionTree_regressor:\n",
    "    class Node:\n",
    "        def __init__(self, value, isLeaf=False, feature=None, left=None, right=None):\n",
    "            self.value = value\n",
    "            self.isLeaf = isLeaf\n",
    "            self.feature = feature\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "\n",
    "        def print_tree(self):\n",
    "            if self.left: self.left.print_tree()\n",
    "            print(\"Feature: {}, cut: {}\\n\".format(self.feature, self.value))\n",
    "            if self.right: self.right.print_tree()\n",
    "\n",
    "        def print_tree_indented(self, level=0):\n",
    "            if self.right: self.right.print_tree_indented(level+1)\n",
    "            print(\"|    \"*level+\"{} => {}\".format(self.feature, self.value))\n",
    "            if self.left: self.left.print_tree_indented(level+1)\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, X, y, depth, minElem_perLeaf):\n",
    "        self.root = self.learn(X, y, depth, minElem_perLeaf)\n",
    "        return self\n",
    "        \n",
    "    def learn(self, X, y, depth, minElem_perLeaf):\n",
    "        n, d = X.shape\n",
    "\n",
    "        if depth==0 or n<=minElem_perLeaf: #or other condition\n",
    "            return self.Node(value=np.mean(y), isLeaf=True)\n",
    "            \n",
    "        best_costDescent = 0 #split that maximise the error descent\n",
    "\n",
    "        for i1 in range(d):\n",
    "            sorted_idx = np.argsort(X[:,i1])\n",
    "            sorted_x, sorted_y = X[sorted_idx, i1], y[sorted_idx]\n",
    "\n",
    "            s_right, s_left = np.sum(sorted_y), 0\n",
    "            n_right, n_left = n, 0\n",
    "\n",
    "            for i2 in range(n-1):\n",
    "                s_left += sorted_y[i2]\n",
    "                s_right -= sorted_y[i2]\n",
    "                n_left += 1\n",
    "                n_right -= 1\n",
    "\n",
    "                if sorted_x[i2]<sorted_x[i2+1]:\n",
    "                    new_costDescent = (s_left**2)/n_left + (s_right**2)/n_right\n",
    "                    if new_costDescent > best_costDescent:\n",
    "                        best_costDescent = new_costDescent\n",
    "                        best_feature = i1\n",
    "                        best_cut = (sorted_x[i2]+sorted_x[i2+1])/2\n",
    "\n",
    "        left_idxs = X[:,best_feature] < best_cut\n",
    "\n",
    "        return self.Node(value=best_cut, feature=best_feature,\n",
    "                        left=self.learn(X[left_idxs],y[left_idxs],depth-1,minElem_perLeaf),\n",
    "                        right=self.learn(X[~left_idxs],y[~left_idxs],depth-1,minElem_perLeaf))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.root is None:\n",
    "            raise Exception(\"Tree not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        y = np.empty(n)\n",
    "        \n",
    "        for i in range(n):\n",
    "            current = self.root\n",
    "            while not current.isLeaf:\n",
    "                if X[i,current.feature] < current.value:\n",
    "                    current = current.left\n",
    "                else:\n",
    "                    current = current.right\n",
    "                \n",
    "            y[i] = current.value\n",
    "        \n",
    "        return y\n",
    "                \n",
    "    def pprint(self):\n",
    "        self.root.print_tree_indented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |    |    None => 0.5287888357411749\n",
      "|    |    100 => -0.3436407141542426\n",
      "|    |    |    |    None => -0.690614054846674\n",
      "|    |    |    52 => -0.26582672266293367\n",
      "|    |    |    |    None => -0.050534495221858634\n",
      "|    50 => 0.026325411334552018\n",
      "|    |    |    |    |    None => -0.7433024311030433\n",
      "|    |    |    |    46 => 0.036038371791547896\n",
      "|    |    |    |    |    None => -0.5527615463776374\n",
      "|    |    |    3 => 0.43599793708096957\n",
      "|    |    |    |    |    None => -0.25346421182791007\n",
      "|    |    |    |    92 => -0.9911936441083564\n",
      "|    |    |    |    |    None => -0.48276052938771075\n",
      "|    |    100 => -0.9379716547027425\n",
      "|    |    |    |    |    None => -0.6839792768083104\n",
      "|    |    |    |    55 => -0.24625000000000002\n",
      "|    |    |    |    |    None => -0.46376312314222246\n",
      "|    |    |    10 => -0.9885510048119757\n",
      "|    |    |    |    |    None => -0.9009580942790387\n",
      "|    |    |    |    7 => -0.13185015540651085\n",
      "|    |    |    |    |    None => -0.7729085638486599\n",
      "50 => -0.6329067641681901\n",
      "|    |    |    |    |    None => -0.6710190558846003\n",
      "|    |    |    |    2 => -0.8976931829936898\n",
      "|    |    |    |    |    None => -0.8191191416668752\n",
      "|    |    |    71 => -0.9716149873792927\n",
      "|    |    |    |    |    None => -0.75408407423602\n",
      "|    |    |    |    99 => -0.7815078731521329\n",
      "|    |    |    |    |    None => -0.8677736294701751\n",
      "|    |    73 => -0.7151827200803716\n",
      "|    |    |    None => -0.019698515544602224\n",
      "|    50 => -0.7736745886654479\n",
      "|    |    |    |    |    None => -0.9252735151292212\n",
      "|    |    |    |    43 => 0.6352824578790883\n",
      "|    |    |    |    |    None => -0.8167167332012107\n",
      "|    |    |    36 => -0.9447552447552447\n",
      "|    |    |    |    None => -0.3834143799163476\n",
      "|    |    51 => -0.9924216637196692\n",
      "|    |    |    |    |    None => -0.7006856202308761\n",
      "|    |    |    |    93 => -0.35918299568249745\n",
      "|    |    |    |    |    None => -0.915199662226089\n",
      "|    |    |    2 => -0.8964518464880522\n",
      "|    |    |    |    |    None => -0.9293083877692535\n",
      "|    |    |    |    40 => -0.07875546912980064\n",
      "|    |    |    |    |    None => -0.9686481297904226\n"
     ]
    }
   ],
   "source": [
    "ndt = NumericalDecisionTree_regressor()\n",
    "ndt.fit(trainVal_data, trainVal_values, depth=5, minElem_perLeaf=10)\n",
    "ndt.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.08187856868820133\n",
      "R^2 score: 0.6277352099996715\n",
      "Explained Variance Score: 0.6277493557352403\n"
     ]
    }
   ],
   "source": [
    "pred = ndt.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree SkLearn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50, 100,   3,  73,  88,  92,   2,  51,  71,  52,   6,  26,  10,\n",
       "        99,  46,  27,  38,  37,   7,  86,  31,  40,  34,  75,  93,  68,\n",
       "        18,  43,  35,  87,  82,  55,  32,  48,  83,  96,  76,  84,  91,\n",
       "        69,  23,   1,  25,  22,  19,  66,  13,  90,  56,  39,  61,   9,\n",
       "         0,  60,  89,  49,  97,  28,  53,  94,  45,  44,  81,  74,  16,\n",
       "        41,  17,   5,  42,  29,  47,  36,  98,  14, 101,   4,  15,  80,\n",
       "         8,  11,  95,  54,  78,  72,  24,  21,  59,  67,  63,  85,  58,\n",
       "        12,  30,  57,  65,  20,  77,  79,  62,  33,  64,  70])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(trainVal_data, trainVal_values)\n",
    "np.flip(np.argsort(dtr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.08546261202136474\n",
      "R^2 score: 0.6130458336476073\n",
      "Explained Variance Score: 0.6143059234577165\n"
     ]
    }
   ],
   "source": [
    "pred = dtr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest project class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests\n",
    "\n",
    "class NumericalRandomForest_regressor:\n",
    "    def __init__(self, n_trees):\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.trees_sampleIdxs = []\n",
    "        \n",
    "    def fit(self, X, y, depths, minElems_perLeaf):\n",
    "        n, d = X.shape\n",
    "        n_learn = int(n/3)\n",
    "        params_product = list(product(depths,minElems_perLeaf))\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            print(\"Fitting #{} tree\".format(i+1))\n",
    "            bootstrap_idxs = np.sort(np.random.permutation(n)[:n_learn])\n",
    "            self.trees_sampleIdxs.append(bootstrap_idxs)\n",
    "            \n",
    "            boot_results = np.empty(len(params_product))\n",
    "            \n",
    "            dt = NumericalDecisionTree_regression()\n",
    "            \n",
    "            for idx, params in enumerate(params_product):\n",
    "                dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs], depth=params[0], minElem_perLeaf=params[1])\n",
    "                pred = dt.predict(X[bootstrap_idxs])\n",
    "                boot_results[idx] = np.mean(np.square(y[bootstrap_idxs]-pred))\n",
    "                #boot_results[idx] = r2_score(trainVal_values[valIdx],pred)\n",
    "                #boot_results[idx] = explained_variance_score(trainVal_values[valIdx],pred)\n",
    "            \n",
    "            win_params = params_product[np.argmin(boot_results)]\n",
    "            \n",
    "            # some doubts here: once found best hyp-par for a bootstrap,\n",
    "            # should i retrain with the whole dataset???\n",
    "            self.trees.append(dt.fit(X[~bootstrap_idxs], y[~bootstrap_idxs],\n",
    "                                     depth=win_params[0], minElem_perLeaf=win_params[1]))\n",
    "            \n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(self.trees)==0:\n",
    "            raise Exception(\"trees not initialised! need to first fit the model\")\n",
    "\n",
    "        n = X.shape[0]\n",
    "        results = np.empty((self.n_trees,n))\n",
    "        for row, tree in enumerate(self.trees):\n",
    "            results[row] = tree.predict(X)\n",
    "            \n",
    "        return np.mean(results,axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 tree\n",
      "Fitting 2 tree\n",
      "Fitting 3 tree\n"
     ]
    }
   ],
   "source": [
    "nrf = NumericalRandomForest_regressor(3)\n",
    "nrf.fit(trainVal_data,trainVal_values,depths=[5,10],minElems_perLeaf=[10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.0716567567601048\n",
      "R^2 score: 0.6762744981069209\n",
      "Explained Variance Score: 0.6766483904037376\n"
     ]
    }
   ],
   "source": [
    "pred = nrf.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SkLearn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/.conda/envs/bcb/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 50,  49, 100,   3,  51,  69,   2,  92,  46,  40,  66,  24,  74,\n",
       "        93,  99,  77,  34,  91,  44,  68,  17,  35,  26,  36,  71,  14,\n",
       "        41,  13,  23,   8, 101,  27,  10,   0,  21,  39,  72,  63,   9,\n",
       "         6,  73,  95,  37,  70,  86,  67,  98,  18,  38,  32,  88,  79,\n",
       "        43,  78,  25,  76,   4,  89,  65,  48,  20,  47,  96,  75,  82,\n",
       "        62,   5,  61,  58,   1,  90,  45,  55,  56,  29,  22,  28,   7,\n",
       "        85,  53,  30,  52,  84,  94,  60,  31,  97,  15,  33,  16,  12,\n",
       "        81,  83,  57,  87,  19,  54,  59,  42,  64,  11,  80])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(trainVal_data, trainVal_values.ravel())\n",
    "np.flip(np.argsort(rfr.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07570589036746808\n",
      "R^2 score: 0.7548044890874106\n",
      "Explained Variance Score: 0.7549114567542822\n"
     ]
    }
   ],
   "source": [
    "pred = rfr.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def crossValidation_selectionGrid(k, parameters_dict, train_data, train_values, predictor):\n",
    "    nVal = train_data.shape[0]\n",
    "    \n",
    "    # Validation indexes adjustment\n",
    "    elemPerFold, remainder = np.divmod(nVal,k)\n",
    "    valIdxList = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        end = start+elemPerFold+int(remainder>0)\n",
    "        valIdxList.append(np.arange(start,end)) \n",
    "        remainder -= 1\n",
    "        start = end\n",
    "    \n",
    "    # Cross validation\n",
    "    params_names = parameters_dict.keys()\n",
    "    params_product = list(product(*parameters_dict.values()))\n",
    "    val_results = np.empty((len(valIdxList),len(params_product)))\n",
    "    \n",
    "    for row, valIdx in enumerate(valIdxList):\n",
    "        for col, params in enumerate(params_product):\n",
    "                     \n",
    "            arg_dict = {k:v for k,v in zip(params_names,params)}\n",
    "            \n",
    "            predictor.fit(train_data[~valIdx], train_values[~valIdx], **arg_dict)\n",
    "            pred = predictor.predict(train_data[valIdx])\n",
    "            \n",
    "            #val_results[row,col] = r2_score(trainVal_values[valIdx],pred)\n",
    "            #val_results[row,col] = explained_variance_score(trainVal_values[valIdx],pred)\n",
    "            val_results[row,col] = np.mean(np.square(train_values[valIdx]-pred))\n",
    "            \n",
    "    selected = np.argmin(val_results.mean(axis=0))\n",
    "    return params_product[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regularised Least Squares\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tikhonov_leastSquares:\n",
    "    def __init__(self, weights = None):\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y, _lambda):\n",
    "        inv = np.linalg.inv(np.matmul(X.T, X) + _lambda*np.eye(X.shape[1]))\n",
    "        self.weights = np.matmul(inv, np.matmul(X.T, y))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"weights not initialised! need to first fit the model\")\n",
    "        return np.matmul(X, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.01245992452487423\n",
      "R^2 score: 0.7185998766470033\n",
      "Explained Variance Score: 0.7193535535276633\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "params_dict = {\"_lambda\":[2,2.05,2.1,2.2,3]}\n",
    "\n",
    "tls = tikhonov_leastSquares()\n",
    "\n",
    "win_regulariser = crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, tls)\n",
    "tls.fit(trainVal_data, trainVal_values, win_regulariser)\n",
    "pred = tls.predict(test_data)\n",
    "\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.07876674161588458\n",
      "R^2 score: 0.6246788578285754\n",
      "Explained Variance Score: 0.6248593232220527\n"
     ]
    }
   ],
   "source": [
    "k = 3 \n",
    "params_dict = {\"depth\":[10,15,20,30],\"minElem_perLeaf\":[5,10,20,30]}\n",
    "\n",
    "dt = NumericalDecisionTree_regressor()\n",
    "win_params = crossValidation_selectionGrid(k, params_dict, trainVal_data, trainVal_values, dt)\n",
    "dt.fit(trainVal_data, trainVal_values, depth=win_params[0], minElem_perLeaf=win_params[1])\n",
    "pred = dt.predict(test_data)\n",
    "print(\"Mean Square Error: {}\".format(np.mean(np.square(test_values-pred))))\n",
    "print(\"R^2 score: {}\".format(r2_score(test_values, pred)))\n",
    "print(\"Explained Variance Score: {}\".format(explained_variance_score(test_values, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
